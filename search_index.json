[["index.html", "Statistics and Data Science Applications Introduction Course goals and context", " Statistics and Data Science Applications Maria Osipenko 2024-04-15 Introduction Course goals and context This course follows the introductory statistics course and contains statistics concepts on intermediate and advanced level. Course goals You will become familiar with basic statistical concepts: Calculate and interpret probabilities, expected values and other key parameters of distributions. Explain the relationship between population and sample. Describe the properties of a statistic or estimator including its probability distribution, expected value, variance, bias, and mean squared error. State and apply the law of large numbers and central limit theorem. Explain the principles of hypothesis testing, conduct hypothesis testing and interpret the results. Explain and apply linear regression model, conduct hypothesis testing. You will apply these skills to analyse real-world data, in particular: Prepare your data for the analysis. Choose an appropriate distributional model/ statistical technique for analyzing your data. Construct an estimator for the unknown parameters. Compute and interpret point estimates. Construct and interpret hypothesis tests and confidence intervals. We will be switching back and forth between theory, its applications, and data analysis. Hopefully you are in this course because you are passionate about statistical data analysis and can‚Äôt wait to learn more about it üòç. If you are taking the course just because it‚Äôs a required course, I hope you become at least a little bit a fan of statistics. So, to give you motivation for that, let‚Äôs look at some examples of using statistics for solving business problems. We will be working with excel/calc for data manipulations under the assumption that you have basic skills in it. If you are not, please look at the tutorial here: https://bookdown.org/bkrauth/BOOK/basic-data-cleaning-with-excel.html#a-quick-tour-of-excel Why Statistics and Data Science? Example 0.1 (Analysing customer behavior) Imaging that you work in a data analysts team for some company running online shops. Your customers order stuff online using either a mobile application (app) or web interface (web). They login, spend some time in the shop and then they (hopefully) buy a lot of your products. Your task is to improve customer experience in order to boost the sales. You may need to answer the following questions: Do the customers order more, if they are using the app or the web? How likely is that a customer, who just logged in, clicks on an advertisement shown in the shop? Does the time spend in the shop by a person has anything to do with the amount of her orders? Is the current discount campaign successful (meaning that it persistently increases the mean sales)? Can we predict the sales in a new market segment? Data analysis using statistical methods: Confirm or clarify assumptions through data Discover new connections, determine influencing factors (further) develop products/advertising, model behavior, predict business development, ‚Ä¶ "],["random-events-and-probability---a-short-review.html", "Chapter 1 Random events and probability - a short review 1.1 Outcomes and Events 1.2 Probabilities 1.3 Sampling as a random process", " Chapter 1 Random events and probability - a short review Probability theory describes random events such as: throwing a certain number when rolling a die, having a positive return in a portfolio of stocks in future, default chance of a borrower customer behavior (the odds of clicking on an advertisement) ‚Ä¶ In this chapter we review how to: model random events using probabilities, calculate and interpret different kinds of probabilities: marginal, joint, and conditional ones determine whether two events are independent or not We use the following simple example to recall the basics of randomness and probability. Example 1.1 (rolling two dice with faces 1,2,3) We conduct the experiment ‚Äúrolling two fair dice with the faces 1, 2 and 3 on two sides of each die‚Äù. Different outcomes are possible: Since we do not know the outcome prior to conducting the action, it is a random process or a random experiment, which outcomes we observe at the end. 1.1 Outcomes and Events Normally when modelling a random process, we are interested in certain outcomes of interest, which can be numbers, or more complex objects. An outcome is a complete description of the end state of a random process, in the sense that everything we are interested in can be defined in terms of a single outcome denoted as \\(\\omega\\). All possible outcomes are gathered in the sample space \\(\\Omega.\\) Example 1.2 (Outcomes and Sample space in rolling the dice twice) Example 1.1 cont. The outcomes of rolling the dice with faces 1,2,3 can be defined as a pair of two numbers \\((a,b)\\) describing the faces shown as a result of the first (\\(\\color{red}a\\)) and of the second (\\(\\color{blue}b\\)) die: \\[(\\color{red}2,\\color{blue}3); (\\color{red}3,\\color{blue}3); (\\color{red}1,\\color{blue}2); \\ldots\\] Sample space (the collection of all possible outcomes): \\[\\begin{align*} \\Omega &amp;= \\{(1;1), (1;2), (1;3), (2;1),\\ldots, (3;3)\\}\\\\ &amp;=\\{(a,b) | a,b\\in \\{1; 2; 3\\}\\}, \\end{align*}\\] where \\(a=\\text {result of the 1st throw}\\) and \\(b=\\text {result of the 2nd throw}\\) The cardinality (the number of elements) of \\(\\Omega\\) is: \\[|\\Omega| = \\text{possibilities}^\\text{number of repetitions} = 3^2=9.\\] Sometimes we are interested in a single outcome, but most of the time we favor a couple of outcomes with a certain property. For example, if we play a board game, where you win if you get the same number twice (outcomes \\((1,1); (2,2); (3;3)\\)) or the sum of the faces is greater than four (outcomes \\((2,3); (3,2); (3;3)\\)). Therefore, we define events which are sets of outcomes that we are interested in. We can think of an event as either: A subset of the sample space A statement that is either true or false These two concepts are equivalent, though the subset concept makes the math clearer. Example 1.3 (Possible events in rolling the dice twice) Example 1.2 cont. Events: \\(A=\\)‚ÄúThe numbers on both dice are the same‚Äù: \\[A=\\{(a,b)\\in \\Omega | a=b\\} =\\{(1;1), (2;2), (3;3)\\}\\] \\(B=\\)‚ÄúThe numbers on both dice are odd‚Äù: \\[B=\\{(a,b)\\in \\Omega | a, b \\text{ odd}\\} =\\{(1;1), (1;3), (3;1), (3;3)\\}\\] \\(A\\cup B=\\)‚ÄúThe numbers on both dice are the same and /or the numbers are odd‚Äù: \\[A\\cup B=\\{(a,b)\\in \\Omega | a=b \\text{ or } a,b \\text{ odd }\\} =\\{(1;1), (1;3), (2;2), (3;1), ( 3;3)\\}\\] \\(A\\cap B=\\)‚ÄúThe numbers on both dice are the same and odd‚Äù: \\[A\\cap B=\\{(a,b)\\in \\Omega | a=b \\text{ or } a,b \\text{ odd }\\} =\\{(1;1), ( 3;3)\\}\\] \\(C=\\)‚ÄúThe sum of both faces is four‚Äù: \\[C=\\{(a,b)\\in\\Omega| a+b=4\\} =\\{(1,3); (2,2); (3,1)\\}\\] Beside the set operations \\(\\cup\\) and \\(\\cap\\) in example 1.3, it is useful to state further possible relations between events, such as: A complement \\(A^c\\) of an event \\(A\\) contains all possible outcome from \\(\\Omega\\) which are not contained in \\(A.\\) The complement of the event ‚ÄúThe numbers on both dice are odd‚Äù is the event ‚ÄúAt least one number is even‚Äù. The complements of the event ‚ÄúThe numbers on both dice are the same‚Äù is the event ‚ÄúThe numbers on both dice are different‚Äù Two events are disjoint (\\(A\\cap B=\\emptyset\\)) if they share no outcomes. The events ‚ÄúThe numbers on both dice are odd‚Äù and ‚ÄúThe numbers on both dice are even‚Äù are disjoint. An event \\(A\\) implies another event (\\(A\\subset B\\)) if all of its outcomes are also in the implied event \\(B.\\) The event ‚ÄúThe numbers on both dice are even‚Äù implies the event ‚ÄúThe sum of both dice is four‚Äù. All events for which a probability can be calculated are summarized in the event space \\(\\mathcal F\\). Formally \\(\\mathcal F\\) is a set of subsets of \\(\\Omega\\), i.e.¬†a subset of the power set \\(\\mathcal P(\\Omega)\\) (the set of all subsets). For a finite and countable set of outcomes, one usually chooses \\(\\mathcal F=\\mathcal P(\\Omega)\\), since this event space then contains all events that can be defined for the random experiment at hand. 1.2 Probabilities Event probabilities quantify the uncertainty associated with the outcome of a random experiment. The probability measure assigns a number between \\(0\\) and \\(1\\) to each event. For \\(\\omega\\in\\Omega\\) the map is called \\(\\omega\\mapsto\\mathbb P(\\omega)\\) probability measure. For an event \\(A\\in \\mathcal F\\), the probability of \\(A\\) is equal to the sum of the probabilities for the outcomes contained in \\(A\\): \\[\\mathbb P(A) = \\sum _{\\omega \\in A } \\mathbb P(\\omega).\\] Any probability measure must satisfy Kolmogorov‚Äôs axioms: \\(\\mathbb P(A)\\geq 0\\), \\(\\mathbb P(\\Omega )=1\\), If \\(A\\cap B=\\emptyset\\), then \\(\\mathbb P(A\\cup B)=\\mathbb P(A)+\\mathbb P(B)\\). Laplace‚Äôs probability model With \\((\\Omega, \\mathcal F,\\mathbb P)\\) there is a Laplace model if the following conditions are met: The sample space \\(\\Omega\\) is finite and all outcomes are equally probable. The power set \\(\\mathcal P(\\Omega)\\) is chosen as the event space, i.e.¬†every subset \\(A\\subseteq \\Omega\\) is an event. The probability of an event \\(A\\in \\mathcal F\\) is then calculated by \\[\\begin{equation*} \\mathbb P(A) = \\frac{|A|}{|\\Omega|} = \\frac{\\text{Number of outcomes favorable for $A$ to occur}} {\\text{Number of all possible outcomes}} \\end{equation*}\\] Example 1.4 (roll the dice twice) Examples 1.1 and 1.2 cont. In this experiment, the sample space is finite, the outcomes are equally likely and we can choose the power set of \\(\\Omega\\) as the event space. Consequently, the Laplace‚Äôs probability model can be applied here. \\[\\begin{align} \\mathbb P(A) &amp;= \\frac{|A|}{|\\Omega|}=\\frac 39 = \\frac 13,\\\\ \\mathbb P(B) &amp;= \\frac{|B|}{|\\Omega|}=\\frac 49. \\end{align}\\] Finite probability spaces The triple \\((\\Omega,\\mathcal F,\\mathbb P)\\) is a finite probability space if the following conditions are met: The result set \\(\\Omega\\) is finite. The power set \\(\\mathcal P(\\Omega)\\) is chosen as the event set. The individual probabilities \\(\\mathbb P(\\omega)\\) of all outcomes \\(\\omega\\in\\Omega\\) are non-negative and add up \\(1\\). The probability of an event \\(A\\subseteq \\Omega\\) is then calculated by \\[\\begin{equation*} \\mathbb P(A) = \\sum _{\\omega\\in A} \\mathbb P(\\omega) = \\text{Sum of all function values} \\mathbb P(\\omega)\\text{ with } \\omega\\in A. \\end{equation*}\\] Example 1.5 (Sum of two dice) Examples 1.1 and 1.4 cont. If we are interested in the resulting sum of two dice instead of the resulting two numbers, the new probability space is: \\(\\Omega = \\{2;3;4;5;6\\}\\) (Why?), \\(\\mathcal F=\\mathcal P(\\Omega)\\): we still choose the power set as the event space, \\(\\mathbb P\\): \\[ \\begin{array}{c|ccccc}\\omega_i&amp;2&amp;3&amp;4&amp;5&amp;6\\\\\\hline \\mathbb P(\\{\\omega_i\\})&amp;\\frac 19&amp;\\frac 29&amp;\\frac 39&amp;\\frac 29&amp;\\frac 19 \\end{array} \\] In this new experiment, the sample space is still finite and the event space is the power set of \\(\\Omega\\). Still, the outcomes are not equally likely. Consequently, the general finite probability space applies here. For the events \\(S_{&gt;4}\\): ‚ÄúThe sum of the numbers is greater than 4‚Äù and \\(S_{even}\\): ‚ÄúThe sum of the numbers is even‚Äù applies: \\[\\begin{align} \\mathbb P(S_{&gt;4}) &amp;= \\mathbb P(\\{5\\}) + \\mathbb P(\\{6\\}) = \\frac 13,\\\\ \\mathbb P(S_{even}) &amp;= \\mathbb P(\\{2\\}) + \\mathbb P(\\{4\\}) + \\mathbb P(\\{6\\})=\\frac 59. \\end{align}\\] Calculating with probabilities The following properties of \\(\\mathbb P\\) hold in general models and facilitate the calculation of probabilities for complex events: \\(\\mathbb P(\\emptyset)=0\\) \\(A\\subseteq B\\) \\(\\Rightarrow\\) \\(\\mathbb P(A)\\leq \\mathbb P(B)\\) and \\(\\mathbb P(B\\setminus A)= \\mathbb P(B)-\\mathbb P( A)\\) \\(\\mathbb P(A^c) = 1-\\mathbb P(A)\\) \\(\\mathbb P(A\\cup B) = \\mathbb P(A) + \\mathbb P(B) - \\mathbb P(A\\cap B)\\) Example 1.6 (Sum of two dice) Example 1.5 cont. \\(\\Omega = \\{2;3;4;5;6\\}\\), \\(\\mathcal F=\\mathcal P(\\Omega)\\): we still choose the power set as the event space, \\(\\mathbb P\\): \\[ \\begin{array}{c|ccccc}\\omega_i&amp;2&amp;3&amp;4&amp;5&amp;6\\\\\\hline \\mathbb P(\\{\\omega_i\\})&amp;\\frac 19&amp;\\frac 29&amp;\\frac 39&amp;\\frac 29&amp;\\frac 19 \\end{array} \\] For \\(S_{&gt;2}\\):‚ÄúThe sum of two dice is greater than 2‚Äù applies:\\[S_{&gt;4}\\subset S_{&gt;2}.\\] Hence with rule 2: \\[\\mathbb P(S_{&gt;2}\\setminus S_{&gt;4})=\\mathbb P(S_{&gt;2}) - \\mathbb P(S_{&gt;4}) =\\frac 89 - \\frac 13 =\\frac 59.\\] \\(S_{&gt;4}^c = S_{\\leq 4}\\): ‚ÄúThe sum of two dice is smaller or equal to 4‚Äù holds (rule 3): \\[\\mathbb P(S_{\\leq 4})=\\mathbb P(S_{&gt;4}^c)=1-\\mathbb P(S_{&gt;4}) = 1 - \\frac 13=\\frac 23.\\] For \\(S_{&gt;4}\\cap S_{even}\\): ‚ÄúThe sum of two dice is greater than 4 and even‚Äù applies: \\[\\mathbb P(S_{&gt;4}\\cap S_{even}) = \\mathbb P(\\{6\\}) = \\frac 19.\\] For \\(S_{&gt;4}\\cup S_{even}\\): ‚ÄúThe sum of two dice is greater than 4 or even‚Äù applies (rule 4): \\[\\mathbb P(S_{&gt;4}\\cup S_{even}) = \\mathbb P(S_{&gt;4}) + \\mathbb P(S_{even}) - \\mathbb P(S_{&gt;4}\\cap S_{even}) = \\frac 13 + \\frac 59 - \\frac 19 = \\frac 79.\\] Joint and conditional probabilities, independence of events Joint probabilities The joint probability of two events \\(A\\) and \\(B\\) is the probability that both happen: \\[\\mathbb P(A\\cap B).\\] Example 1.7 (Joint probabilities for two dice) Examples 1.4 and 1.5 cont. Consider the events: \\(S_4=\\)‚ÄúThe sum of the dice numbers is four‚Äù \\(B=\\)‚ÄúThe dice numbers are both odd‚Äù \\[\\mathbb P(S_4\\cap B) = \\mathbb P(\\{(1,3); (3,1)\\}) = \\frac 29.\\] Joint probabilities are just probabilities, so they obey all of the axioms and rules of probability. Conditional probabilities The conditional probability of an event \\(A\\) given another event \\(B\\) is defined as: \\[\\mathbb P(A|B) = \\frac{\\mathbb P(A\\cap B)}{\\mathbb P(B)} = \\frac{joint~probability}{probability~of~the~given~event}.\\] The conditional probability answers the question: if we already know that \\(B\\) is true, what are the chances that \\(A\\) is true? Example 1.8 (Conditional probabilities for two dice) Example 1.7 cont. Consider the events: \\(S_4=\\)‚ÄúThe sum of the dice numbers is four‚Äù \\(B=\\)‚ÄúThe dice numbers are both odd‚Äù \\[\\mathbb P(S_4|B) = \\frac{\\mathbb P(S_4\\cap B)}{\\mathbb P(B)} = \\frac{\\frac 29}{\\frac 49} = \\frac 12.\\] Independence of events If we assume that certain events are unrelated to each other, we say that the two events are independent. Formally, \\(A\\) and \\(B\\) are independent, if their joint probability is just the two individual probabilities multiplied together: \\[\\mathbb P(A\\cap B) = \\mathbb P(A)\\cdot \\mathbb P(B).\\] This definition is based in the premise, that the conditional probability corresponds to the simple probability, if the two events are unrelated: \\[\\begin{align}\\mathbb P(A|B) &amp;= \\mathbb P(A)\\Leftrightarrow\\\\ \\frac{\\mathbb P(A\\cap B)}{\\mathbb P(B)} &amp;= \\mathbb P(A)\\Leftrightarrow\\\\ \\mathbb P(A\\cap B) &amp;= \\mathbb P(A)\\cdot {\\mathbb P(B)} \\end{align}\\] Example 1.9 (roll the dice twice) Example 1.8 cont. In the experiment with rolling two fair dice, the defined events \\(S_4\\) and \\(B\\) are dependent, since: \\[\\begin{align} \\mathbb P(S_4\\cap B) &amp;=\\frac 29\\not = \\frac 13\\cdot\\frac 49. \\end{align}\\] So, \\(S_4\\) and \\(B\\) are dependent. The same can be deduced from \\(\\mathbb P(S_4|B)=\\frac 12\\not=\\frac 13=\\mathbb P(S_4)\\). Now try to calculate some probabilities yourself! Please round your asnwers to four decimal places! Submit Submit Submit 1.3 Sampling as a random process Now we will look at (somewhat simplified) examples of how sampling can be modeled as a random process. Example 1.10 (Random selection of customers) We first reconstruct the sampling mechanism in a simplified form. Suppose we have a total of \\(5\\) customers (denoted as \\(x_1\\) to \\(x_5\\)) and select \\(3\\) customers randomly and independently (we ‚Äúput‚Äù the drawn customers back into the customer pool before we choose the next one, i.e.¬†one and the same customer can appear multiple times). How can one describe this random selection with an appropriate probability space? The sample space (\\(\\leadsto\\) \\(\\Omega\\)): Outcomes are sample realizations or sample outcomes. \\[\\begin{align} \\Omega &amp;=\\{(x_1,x_1,x_1),(x_1,x_2,x_2), \\ldots, (x_1,x_2,x_3), \\ldots, (x_5, x_2,x_4), \\ldots, (x_5,x_5,x_5)\\}\\\\ |\\Omega|&amp;=5^3 = 125. \\end{align}\\] Event space (since \\(\\Omega\\) is finite \\(\\leadsto\\) the power set): \\[ \\mathcal F = \\mathcal P(\\Omega). \\] Probabilities: every outcome is equally likely (we draw with replacement), i.e.¬†\\[\\mathbb P(\\{(x_i,x_j,x_k)\\} = \\frac 1{|\\Omega|} = \\frac 1{125}.\\] So Laplace‚Äôs probability model applies. For \\(A\\): The combination of certain customers 1, 2 and 3 is selected (in different order), \\(A = \\{(x_1,x_2,x_3),(x_2,x_1,x_3),\\ldots,(x_3,x_2,x_1) \\}\\)) we have \\(|A|=3!\\) (permutations) and \\[\\mathbb P(A) = \\frac{|A|}{|\\Omega|} = \\frac 6{125}.\\] For \\(B\\): Customer 1 is randomly selected at least once, \\(B = \\{(x_1,x_1,x_1),\\ldots, (x_5,x_1,x_2),\\ldots, (x_5,x_5,x_1)\\},\\)) we have \\(\\mathbb P(B) = 1-\\mathbb P(B^c)\\) with \\(|B^c|=4^3 = 64\\) (selecting out of four possibilities \\(x_2,x_3,x_4\\) or \\(x_5\\) three times.): \\[\\mathbb P(B) = 1-\\frac{|B^c|}{|\\Omega|} = \\frac {61}{125}.\\] For operational questions, it is less interesting which customers are specifically selected, but rather what characteristics they have. That‚Äôs why we‚Äôre modifying our modeling by looking at the specific feature ‚Äúuse of app or web interface‚Äù. Example 1.11 (Random selection of customers: app vs web) Example 1.10 cont. Assume that the distribution among the \\(5\\) customers looks like this: \\[ \\begin{array}{c|ccccc}\\hline Customer~i&amp;1&amp;2&amp;3&amp;4&amp;5\\\\\\hline Usage~a_i&amp;App&amp;Web&amp;App&amp;App&amp;Web\\\\\\hline \\end{array} \\] That is, \\(\\mathbb P(App) = 0.6\\) und \\(\\mathbb P(Web)=0.4.\\) What are the different sample outcomes, that we can draw, and what is their probability? Sample space: \\(x_i\\) gets the corresponding values (e.g.¬†\\(x_1=App\\), \\(x_2=Web\\)) \\[\\begin{align} \\Omega &amp;= \\{(App; App; App), (App; Web; Web),\\ldots, (App; Web; App),\\ldots, \\\\ &amp;~~~~(Web; Web; App),\\ldots,(Web; Web; Web) \\}\\\\ |\\Omega|&amp;=5^3 = 125. \\end{align}\\] Event space: \\(\\mathcal F=\\mathcal P(\\Omega).\\) Probability measure: equal probabilities Events: \\(A_3=\\) ‚Äúall three selected customers are app users‚Äù: only customers 1, 3 and 4 may be selected \\(\\leadsto\\) \\(|A_3|=3^3=27\\) and \\[\\mathbb P(A_3)=\\frac{27}{ 125}.\\] \\(A_0=\\) ‚Äúall three selected customers are web users‚Äù: only customers 2 and 5 may be selected \\(\\leadsto\\) \\(|A_0|=2^3=8\\) and \\[\\mathbb P(A_0)=\\frac{8}{125}.\\] \\(A_1=\\) ‚Äúonly one of the selected customers uses the app‚Äù: one place is occupied by customer 1, 3 or 4 (three cases times three possible placements); two other places - by customers 2 or 5 (select out of two possibilities two times) \\(\\leadsto\\) \\(|A_1|=3\\cdot 3\\cdot 2^2=36\\) and \\[\\mathbb P(A_1)=\\frac{36}{125}.\\] \\(A_2=\\) ‚Äútwo of the selected customers use the app‚Äù \\(\\leadsto\\) \\(\\mathbb P(A_2)=1-\\mathbb P(A_2^c) = 1-(P(A_0) + P(A_1) + P(A_3)) = 1-\\frac{71}{125} = \\frac{54}{125}.\\) What do you expect on average (across all sample results)? (Sum over the possible values for the number of app users multiplied by their probability.) \\[ 0\\cdot \\frac{8}{125} + 1\\cdot \\frac{36}{125}+2\\cdot \\frac{54}{125}+3\\cdot \\frac{27}{125} = 1.8 \\] or as a share \\(\\frac{1.8}{3}=0.6\\) \\(\\leadsto\\) on average we have the same share (60%) of app users as in the entire customer base! Sometimes two or more features are of interest: in our example it will be the ‚ÄúNumber of orders‚Äù which we now model in the context of an additional feature ‚ÄúUsage of app or web interface‚Äù. What happens when a sample with respect to the both characteristics is taken? Example 1.12 (Orders and Usage) Here too, let‚Äôs simplify things a bit and assume that we already know the joint probabilities. Now we randomly select user accounts and determine how many orders were made and via which system (app or web) they were made. What different sample results and with what probability can we draw? Sample space: \\[\\begin{align} \\Omega &amp;= \\{(App; 1), (App; 2),\\ldots, (App; 6),\\ldots, \\\\ &amp;~~~~(Web;1),\\ldots,(Web; 6) \\}\\\\ |\\Omega|&amp;=2\\cdot 6=12. \\end{align}\\] Event space: \\(\\mathcal F=\\mathcal P(\\Omega).\\) Probability measure: (assuming we know the joint probabilities): \\[ \\begin{array}{c|cccccc} \\mathbb P((a_i,b_i))&amp;b_1=1&amp;b_2=2&amp;b_3=3&amp;b_4=4&amp;b_5=5&amp;b_6=6\\\\\\hline a_1=App&amp;0.02&amp;0.06&amp;0.12&amp;0.2&amp;0.12&amp;0.08\\\\ a_2=Web&amp;0.08&amp;0.09&amp;0.13&amp;0.05&amp;0.03&amp;0.02\\\\ \\end{array} \\] Events: \\(A_1 =\\) Orders via app \\(\\leadsto\\) \\[\\begin{align}\\mathbb P(A_1)&amp;=\\mathbb P((App;1),\\ldots,(App;6)) =\\\\&amp;~~0.02 + \\ldots + 0.08 = 0.6.\\end{align}\\] \\(B_{\\geq 3}=\\) Min. 3 orders \\(\\leadsto\\) \\[\\begin{align}\\mathbb P(B_{\\geq 3})&amp;=\\mathbb P((App;3),\\ldots,(App;6),(Web;3),\\ldots,(Web;6 )) =\\\\&amp;~~0.12 + \\ldots + 0.08 + 0.13 + \\ldots + 0.02 = 0.75.\\end{align}\\] \\(A_1\\cap B_{\\geq 3}=\\) Min. 3 orders via app \\(\\leadsto\\) \\(A_1\\cap B_{\\geq 3}=\\{(App;3),(App;4),(App;5),(App;6)\\}\\) \\[\\begin{align}\\mathbb P(A_1\\cap B_{\\geq 3})&amp;=\\mathbb P((App;3),(App;4),(App;5),(App;6)) = \\\\&amp;~~0.12 + 0.2 + 0.12 + 0.08 = 0.52.\\end{align}\\] When analyzing customer characteristics, it is often necessary to model probabilities conditioned on certain other characteristics. For example, if it is known that a randomly selected customer uses the app, how many orders per month could you expect from that customer? Example 1.13 (Orders and Usage) Example 1.12 cont. We assume that we already know the joint probabilities. What is the distribution of the number of orders among app users? Result set: \\[\\begin{align} \\Omega &amp;= \\{(App; 1), (App; 2),\\ldots, (App; 6),\\ldots, \\\\ &amp;~~~~(Web;1),\\ldots,(Web; 6) \\}\\\\ |\\Omega|&amp;=12. \\end{align}\\] Event space: \\(\\mathcal F=\\mathcal P(\\Omega).\\) Probability measure: (assuming we know the joint probabilities) \\[\\begin{equation} \\begin{array}{c|cccccc} \\mathbb P((a_i,b_i))&amp;b_1=1&amp;b_2=2&amp;b_3=3&amp;b_4=4&amp;b_5=5&amp;b_6=6\\\\\\hline a_1=App&amp;0,02&amp;0,06&amp;0,12&amp;0,2&amp;0,12&amp;0,08\\\\ a_2=Web&amp;0,08&amp;0,09&amp;0,13&amp;0,05&amp;0,03&amp;0,02\\\\ \\end{array} \\tag{1.1} \\end{equation}\\] Events: \\(A_1 =\\) Use of the app: \\[\\begin{align}\\mathbb P(A_1)&amp;= 0,6.\\end{align}\\] \\(B_{i}=\\) number of orders \\(=i\\) \\(\\leadsto\\) \\[\\begin{align}\\mathbb P(B_{i})&amp;=\\mathbb P((App;i),(Web;i)):\\end{align}\\] \\[ \\begin{array}{c|cccccc} i&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6\\\\\\hline \\mathbb P(B_i) &amp;0,1&amp;0,15&amp;0,25&amp;0,25&amp;0,15&amp;0,1 \\end{array} \\] \\(A_1\\cap B_{i}=\\) \\(i\\) Orders via app \\(\\leadsto\\) \\(A_1\\cap B_{i}=\\{(App;i),(App;i),(App;i), (App;i)\\}\\) \\[\\begin{align}\\mathbb P(A_1\\cap B_{i})&amp;=\\mathbb P((App;i)).\\end{align}\\] \\(\\leadsto\\) first line in (1.1). Conditional distribution of \\(B_i\\) given \\(A_1\\) is (\\(\\mathbb P(B_i|A_1 = \\frac{\\mathbb P(A_1\\cap B_{i})}{\\mathbb P(A_1)}\\)): \\[ \\begin{array}{c|cccccc} i&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6\\\\\\hline \\mathbb P(B_i|A_1) &amp;0,0333&amp;0,1&amp;0,2&amp;0,3333&amp;0,2&amp;0,1333 \\end{array} \\] So what average number of orders do we expect from an app user? \\[ 1\\cdot 0,0333 + 2\\cdot 0,1 + 3\\cdot 0,2 + 4\\cdot 0,3333 + 5\\cdot 0,2 + 6\\cdot 0,1333 = 3,9663 \\] ‚Ä¶and from a web user? Exercise 1.1 (average number of orders) "],["random-variables.html", "Chapter 2 Random variables 2.1 Discrete random variable 2.2 Continuous random variables", " Chapter 2 Random variables Nice to be able to calculate probabilities for various events üòÑ! As business analysts, we mostly work with numbers. This means that we usually transfer the results of a random process to numerical values. For example, we summarize a purchase decision (the interaction of unknown customer preferences and complex consequences of customer interactions with their environment) into a few numerical values, such as the number of products purchased, orders placed or the average invoice amount. We are primarily interested in the resulting numbers, while we can‚Äôt or don‚Äôt want to model the customer‚Äôs complex decision-making process. However, these numbers that we are looking at are not fixed but vary from customer to customer, month to month, etc. If we had to indicate future sales today, we would hardly be able to give a concrete value. We know approximately what values the sales volume can attain, but these are associated with uncertainty and therefore depend on complex events (random processes) that are partly controlled by chance. Probabilistically speaking, this future sales volume is a random variable. In this chapter we will learn: How to define the random variables and what kind of examples exist (\\(\\leadsto\\) discrete vs.¬†continuous random variables), how to derive distribution of random variables and how to calculate the key parameters of this distribution, as the expected value and the variance, but also the covariance and the quantiles. A random variable \\(X\\) is a variable whose values are derived from the result of a random operation. A number \\(x\\in \\mathbb R\\), which the random variable \\(X\\) assumes based on an outcome of an associated random process, is called the realization or the value of \\(X\\). Examples: Sum of two dice, Stock return in the next month, Number of app users in a survey. We differentiate between two types: discrete and continuous random variables, depending on what values they can assume. The distribution of a random variable \\(X\\) indicates which values in \\(\\mathbb R\\) are assumed with which probabilities. This distribution is derived from the original random process. Since random variables have numbers as values, we consider the following events: \\[\\begin{equation*} \\{X=x\\},\\quad \\{X\\not=x\\}, \\quad \\{X\\leq x\\},\\quad \\{X&gt;x\\}, \\quad \\{a\\leq X\\leq b\\}. \\end{equation*}\\] These events are assigned probabilities by the distribution of the random variable. 2.1 Discrete random variable A random variable \\(X\\) is called discrete if it assumes a finite or countably infinite number of values \\(x_1,x_2,\\ldots, x_n, \\ldots\\). Such as dice numbers, number of orders, number of customers per hour, indicator of a loan default or of a product purchase. The probability distribution of \\(X\\) is given by the probabilities \\[\\begin{equation*} \\mathbb P(X=x_i)=p_i, \\quad i=1,2,\\ldots. \\end{equation*}\\] We denote the set of values of \\(X\\) with \\(\\mathcal T=\\{x_1,x_2,\\ldots\\}\\). For the probabilities \\(p_1,p_2,\\ldots\\) applies (according to the axioms): \\[\\begin{gather*} \\phantom{\\quad i=1,2,\\ldots} 0\\leq p_i\\leq 1,\\quad i=1,2,\\ldots,\\\\ p_1+p_2+\\cdots = \\sum_{i\\geq 1} p_i=1. \\end{gather*}\\] The probability \\(\\mathbb P(X\\in A)\\) for \\(A\\subseteq \\mathcal T\\) is given as \\[\\displaystyle \\mathbb P(X\\in A) = \\sum_{i: x_i\\in A} p_i.\\] Example 2.1 (Win when rolling two dice) Example 1.5 cont. In the experiment ‚Äúsum of two fair dice with faces 1, 2 and 3‚Äù we found: \\(\\Omega = \\{2;3;4;5;6\\}\\) \\(\\omega\\in\\Omega,\\) \\(\\omega=\\) the sum of the dice, \\(\\mathcal F=\\mathcal P(\\Omega)\\), \\(\\mathbb P\\): \\[ \\begin{array}{c|ccccc}\\omega_i&amp;2&amp;3&amp;4&amp;5&amp;6\\\\\\hline \\mathbb P(\\{\\omega_i\\})&amp;\\frac 19&amp;\\frac 29&amp;\\frac 39&amp;\\frac 29&amp;\\frac 19 \\end{array} \\] Based on this, we now define a random variable \\(X=\\) The random win (in euros): \\[ X=\\begin{cases} 2\\cdot \\omega, &amp;\\text{ if } \\omega \\text{ odd},\\\\ - 1, &amp;\\text{ if } \\omega \\text{ even},\\\\ \\end{cases} \\] What are the values of \\(X\\) and what are their probabilities? Well, for each result \\(\\omega\\) we can determine the value of \\(X\\), specifically: \\[ \\begin{array}{c|ccccc}\\omega_i&amp;2&amp;3&amp;4&amp;5&amp;6\\\\\\hline x_i&amp;-1&amp;6&amp;-1&amp;10&amp;-1\\\\ \\mathbb P(\\{\\omega_i\\})&amp;\\frac 19&amp;\\frac 29&amp;\\frac 39&amp;\\frac 29&amp;\\frac 19 \\end{array} \\] The probabilities of the \\(X\\) values correspond to the probabilities of the respective outcomes. Only, some values in the above list are doubled. Therefore, to create a distribution of \\(X\\), we write down each value only once and sum the probabilities of the outcomes that give us that value to determine the final probability of that \\(X\\)-value: \\[ \\begin{array}{c|ccc} x_i&amp;-1&amp;6&amp;10\\\\\\hline \\mathbb P(x_i)=p_i&amp;\\frac 59&amp;\\frac 29&amp;\\frac 29 \\end{array} \\] This is the distribution of \\(X\\). Probability that \\(X\\) is positive is calculated as: \\[\\begin{equation*} \\mathbb P(X&gt;0)=\\mathbb P(X\\in \\{6;10\\}) = \\mathbb P(X=6) + \\mathbb P(X=10) = \\frac{4}{9 }. \\end{equation*}\\] 2.1.1 Probability and distribution functions Probability function If you specify the distribution of \\(X\\) like in Example 2.1, then you only have probabilities for numbers that are in the value set \\(\\mathcal T\\). That is, other real numbers \\(x\\not\\in \\mathcal T\\) have the probability of zero, i.e.¬†\\(\\mathbb P(X=x)=0\\) for \\(x\\not \\in \\mathcal T\\). The probability function assigns a value to every \\(x\\in \\mathbb R\\). The probability function \\(f(x)\\) of a discrete random variable \\(X\\) for \\(x\\in \\mathbb R\\) is defined by \\[\\begin{equation*} f(x) = \\begin{cases} \\mathbb P(X=x_i)=p_i, &amp;\\quad x=x_i\\in \\{x_1,x_2,\\ldots, \\},\\\\ 0,&amp;\\quad\\text{else}. \\end{cases} \\end{equation*}\\] Properties of the probability function: \\(f(x)\\geq 0\\), \\(x\\in \\mathbb R\\), \\(\\sum_{i\\geq 1} f(x_i)=1\\) Distribution function The (cumulative) distribution function (cdf) of a discrete random variable is given as \\[\\begin{equation*} F(x) = \\mathbb P(X\\leq x) = \\sum_{i: x_i\\leq x} f(x_i). \\end{equation*}\\] Properties of the distribution function of a discrete random variable: \\(F\\) is a right-sided continuous step function, \\(x\\mapsto F(x)\\) is monotonically increasing, \\(0\\leq F(x)\\leq 1\\), for all \\(x\\in \\mathbb R,\\) \\(\\lim_{x\\rightarrow-\\infty} F(x)=0\\) and \\(\\lim_{x\\rightarrow\\infty} F(x)=1\\). Example 2.2 (Win when rolling two dice) Example 2.1 cont. Probability function: \\[\\begin{align} f(x)&amp;=\\begin{cases}\\frac 59, \\text{if }x=-1,\\\\ \\frac 29, \\text{if }x\\in\\{6;10\\},\\\\ ~0, \\text{else.} \\end{cases} \\end{align}\\] Figure 2.1: Probability function for X Distribution function: \\[\\begin{align} F(x)=\\begin{cases} ~0, \\text{if } x&lt; -1,\\\\ \\frac 59, \\text{if }-1\\leq x&lt;6,\\\\ \\frac 79, \\text{if }~~6\\leq x&lt; 10,\\\\ ~1, \\text{if }~~x\\geq 10. \\end{cases} \\end{align}\\] Calculation example: if \\(6\\leq x&lt; 10\\) then \\[ F(x)=\\mathbb P(X\\leq x) = \\mathbb P(X=-1) + \\mathbb P(X=6) =\\frac 59+\\frac 29 = \\frac 79. \\] Figure 2.2: Distribution function for X Submit Submit Submit Submit Example 2.3 (Costs for customer orders) Example 1.12 cont. In the example with orders via app or web, we had the following common probabilities: \\[ \\begin{array}{c|cccccc} \\mathbb P((a_i,b_i))&amp;b_1=1&amp;b_2=2&amp;b_3=3&amp;b_4=4&amp;b_5=5&amp;b_6=6\\\\\\hline a_1=App&amp;0.02&amp;0.06&amp;0.12&amp;0.2&amp;0.12&amp;0.08 \\\\ a_2=Web&amp;0.08&amp;0.09&amp;0.13&amp;0.05&amp;0.03&amp;0.02 \\\\ \\end{array} \\] We also found that app users place more orders on average. Assume you process the orders placed in the online shop through a service partner. When customers places their orders in the shop, you have to pay a certain fee to the service partner. Furthermore, the fee depends on the number of orders and the interface: app or web. We define the costs that result from all ordering processes on a user account as a random variable \\(K\\). \\(K\\) is calculated as: \\[ K = \\begin{cases} 1.5\\cdot \\text{No. Orders}, &amp;\\text{ if via app and number. Orders }\\leq 4,\\\\ 1\\cdot \\text{No. Orders} + 1, &amp;\\text{ if via web and number. Orders }\\leq 4,\\\\ 6, &amp;\\text{ if via app and number. Orders }&gt;4,\\\\ 5, &amp;\\text{ if via web and display. Orders }&gt;4. \\end{cases} \\] How are the costs distributed? Values of \\(K\\): \\[ \\begin{array}{c|cccccc} (App,b_i)&amp;(App,1)&amp;(App,2)&amp;(App,3)&amp;(App,4)&amp;(App,5)&amp;(App,6)\\\\\\hline k_{1,i}&amp;1.5&amp;3&amp;4.5&amp;6&amp;6&amp;6\\\\ \\mathbb P(App,b_i)&amp;0.02&amp;0.06&amp;0.12&amp;0.2&amp;0.12&amp;0.08\\\\ \\end{array} \\] \\[ \\begin{array}{c|cccccc} (Web,b_i)&amp;(Web,1)&amp;(Web,2)&amp;(Web,3)&amp;(Web,4)&amp;(Web,5)&amp;(Web,6)\\\\\\hline k_{2,i}&amp;2&amp;3&amp;4&amp;5&amp;5&amp;5\\\\ \\mathbb P(Web,b_i)&amp;0.08&amp;0.09&amp;0.13&amp;0.05&amp;0.03&amp;0.02\\\\ \\end{array} \\] Distribution of \\(K\\): \\[ \\begin{array}{c|cccccccc} k_i&amp;1.5&amp;2&amp;3&amp;4&amp;4.5&amp;5&amp;6\\\\\\hline p_i&amp;0.02&amp;0.08&amp;0.15&amp;0.13&amp;0.12&amp;0.1&amp;0.4 \\end{array} \\] 2.1.2 Expected value and variance As in the descriptive statistics, it makes sense to define some meaningful parameters, that essentially describe the distribution of a random variable. For both types of random variables, we define the mean value \\(\\leadsto\\) expected value, a measure for the spread \\(\\leadsto\\) variance and standard deviation. For two discrete random variables, we also define covariance and correlation. Expected value and variance are the most important metrics, as many families of distributions (more on this later) are clearly defined by these two metrics. The two key metrics give us information about the mean position and variability of the values of random variables. Expected value Definition 2.1 (Expected value/ Expectation) The expected value \\(\\mathbb E(X)\\) of a discrete random variable \\(X\\) with value set \\(\\mathcal T=\\{x_1,x_2,\\ldots\\}\\) and probability function \\(f(x)\\) is: \\[\\begin{equation*} \\mathbb E(X) = x_1 f(x_1) + x_2 f(x_2) + \\cdots = \\sum_{i\\geq 1} x_i f(x_i). \\end{equation*}\\] Properties of the expected value \\(\\mathbb E(a X + b) = a \\mathbb E (X) + b\\) (linear transformation) \\(\\mathbb E(X+Y) = \\mathbb E(X) + \\mathbb E(Y)\\) (expected value of a sum = sum of expected values) Variance and standard deviation Definition 2.2 (Variance and standard Deviation) The variance of a random variable \\(X\\) is \\[\\begin{equation*} \\sigma^2 = \\text{Var}(X) = \\mathbb E((X-\\mathbb E(X))^2)\\\\ =\\sum_{i\\geq 1} (x_i-\\mathbb E(X))^2 f(x_i), \\end{equation*}\\] The standard deviation is \\(\\sigma = \\sqrt{\\text{Var}(X)}.\\) Best suited for calculating variance: \\[\\begin{equation*} \\text{Var}(X)=\\mathbb E(X^2) - (\\mathbb E(X))^2. \\end{equation*}\\] For discrete random variables this means: \\[\\begin{align} \\text{Var}(X)&amp;=\\sum_{i\\geq1} x_i^2f(x_i)- (\\mathbb E(X))^2. \\end{align}\\] Properties of Variance \\(\\text{Var}(aX+b)= a^2\\text{Var}(X)\\) (linear transformation) \\(\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y)\\) if \\(X\\) and \\(Y\\) are stochastically independent. This results in the calculation rules for linear transformations (\\(Y=a X + b\\)): \\[\\begin{align} \\mathbb E(Y) &amp;= \\mathbb E (a X + b)=a \\mathbb E(X) + b,\\\\ \\text{Var}(Y)&amp;=\\text{Var}(a X + b) = a^2\\text{Var}(X). \\end{align}\\] Example 2.4 (Costs for customer orders) Example 2.3 cont. In this example, we derived the following probability distribution for the cost \\(K\\): \\[ \\begin{array}{c|ccccccc} k_i&amp;1.5&amp;2&amp;3&amp;4&amp;4.5&amp;5&amp;6\\\\\\hline p_i&amp;0.02&amp;0.08&amp;0.15&amp;0.13&amp;0.12&amp;0.1&amp;0.4 \\end{array} \\] That is, \\(\\mathbb E(K) = \\sum_{i=1}^7 k_i\\cdot p_i = 1.5\\cdot 0.02 + \\ldots + 6\\cdot 0.4 = 4.6.\\) \\(\\text{Var}(K) = \\sum_{i=1}^7 k^2_i\\cdot p_i - \\mathbb E(K)^2= 1.5^2\\cdot 0.02 + \\ldots + 6^2\\cdot 0.4 -4,6^2 = 1.965.\\) Because of rising inflation the service cost is revised. The new cost includes a \\(20\\%\\) increase (an increase by the factor \\({\\color{red}{1.2}}\\)) and an additional fee of \\(\\color{blue}1\\) euro compared to the old costs, such that \\[K_{new} = {\\color{red}{1.2}}\\cdot K+{\\color{blue}{1}}.\\] The new parameters are: \\(\\mathbb E(K_{new}) = \\mathbb E ({\\color{red}{1.2}}\\cdot K+{\\color{blue}{1}})={\\color{red}{1.2}}\\cdot \\mathbb E(K)+{\\color{blue}{1}} = 1.2\\cdot 4.6+1=6.52.\\) \\(\\text{Var}(K_{new}) = \\text{Var}({\\color{red}{1.2}}\\cdot K+{\\color{blue}{1}})= ({\\color{red}{1.2}})^2\\cdot \\text{Var}(K) = 1.44\\cdot 1.965=2.8296.\\) Submit Submit Submit Submit 2.1.3 Covariance and correlation Whenever we are dealing with several random variables as in example 1.12, the question of the association between the variables arises. If we can find a positive association pattern between the number of orders and the usage of app, then we could possibly boost the sales by promoting the mobile app usage. A measure of linear dependence between two random variables is their covariance and their associated correlation. Definition 2.3 (Covariance and Correlation) The covariance of two random variables \\(X\\) and \\(Y\\) is determined by \\[\\begin{equation*} \\sigma_{XY}=\\text{Cov}(X,Y) =\\mathbb E[(X-\\mathbb E(X))\\, (Y-\\mathbb E(Y))] = \\mathbb E[X\\cdot Y] - \\mathbb E(X)\\, \\mathbb E(Y). \\end{equation*}\\] Specifically for discrete random variables: \\[ \\text{Cov}(X,Y) =\\sum_{i,j}x_i\\cdot y_j\\cdot f(x_i,y_j) - \\left(\\sum_ix_i\\cdot f(x_i)\\right)\\cdot\\left( \\sum_jy_j\\cdot f(y_j)\\right). \\] The correlation coefficient is determined by \\[\\begin{equation*} \\rho_{XY}=\\frac{\\sigma_{XY}}{\\sigma_X\\, \\sigma_Y} = \\frac{\\text{Cov}(X,Y)} {\\sqrt{\\text{Var}(X)}\\, \\sqrt{\\text{Var}(Y)}}. \\end{equation*}\\] \\(X\\) and \\(Y\\) are called uncorrelated if \\(\\text{Cov}(X,Y)=0\\). \\(-1\\leq \\rho_{XY}\\leq 1\\) If \\(X, Y\\) are independent, then they are also uncorrelated. Example 2.5 (Orders and Usage) In the example 1.12 we looked at the joint distribution of the app usage and orders. Now we define two random variables: \\(A=\\begin{cases}1, &amp;\\text{ falls } App\\\\0, &amp;\\text{ falls } Web\\end{cases}\\) (an indicator of app usage). \\(B=b_i\\), \\(b_i\\in\\{1; 2; \\ldots 6\\}\\) the number of orders. The joint probabilities are: \\[ \\begin{array}{c|cccccc} \\mathbb P((a_i,b_i))&amp;b_1=1&amp;b_2=2&amp;b_3=3&amp;b_4=4&amp;b_5=5&amp;b_6=6\\\\\\hline a_1=1&amp;0.02&amp;0.06&amp;0.12&amp;0.2&amp;0.12&amp;0.08\\\\ a_2=0&amp;0.08&amp;0.09&amp;0.13&amp;0.05&amp;0.03&amp;0.02\\\\ \\end{array} \\] Are the two quantities correlated? Expected values \\[\\begin{align} \\mathbb E(A)&amp;=1\\cdot0,6 + 0\\cdot0,4 = 0,6,\\\\ \\mathbb E(B)&amp;=1\\cdot0,1 + 2\\cdot0,15 +\\ldots + 6\\cdot0,1=3,5. \\end{align}\\] Variances: \\[\\begin{align} \\text{Var}(A)&amp;=1^2\\cdot0,6 + 0^2\\cdot0,4 - 0,6^2 = 0,24,\\\\ \\text{Var}(B)&amp;=1^2\\cdot0,1 + 2^2\\cdot0,15 +\\ldots + 6^2\\cdot0,1 - 3,5^2=2,05. \\end{align}\\] Covariance: \\[\\begin{align} \\text{Cov}(A,B)&amp;=\\mathbb E(A\\cdot B) - \\mathbb E(A)\\, \\mathbb E(B)\\\\ &amp;=1\\cdot 1\\cdot0.02 + 1\\cdot 2\\cdot0.06 + 1\\cdot 3\\cdot0.12 + 1\\cdot 4\\cdot0.2 \\\\ &amp;+ 1\\cdot 5\\cdot0.12 + 1\\cdot 6\\cdot0.08 \\text{ (f√ºr A=1 und B=1;2;$\\ldots$ 6)} + \\\\ &amp;+ 0\\cdot 1\\cdot0.08 + \\dots + 0\\cdot 6\\cdot0.02\\text{ (f√ºr A=0 und B=1;2;$\\ldots$ 6)} - \\\\ &amp;- 0,6\\cdot3,5 = 0,28. \\end{align}\\] Correlation: \\[\\begin{align} \\rho_{A,B}&amp;=\\frac{\\text{Cov}(A,B)}{\\sqrt{\\text{Var}(A)}\\cdot\\sqrt{\\text{Var}(B)}}\\\\ &amp;= \\frac{0,28}{\\sqrt{0,24}\\cdot\\sqrt{2,05}} = 0,3992. \\end{align}\\] So the two random variables are positively correlated. Yuhu! We have a positive association between the app usage and the number of orders. It means, that by increasing the incentive for the app usage, we could increase our sales. 2.2 Continuous random variables We have seen how to model discrete random variables. However, there are certainly cases where the framework of discrete random variables is not sufficient. For example, when we look at sizes, lengths, volumes, but also when we examine sales, times in spend in our online shop, waiting times or asset returns. These variables take on many different values, which makes modeling difficult for us because we have to take all their probabilities into account ü§î. In addition, due to varying measurement precision, we usually cannot distinguish between the online shopping time of \\(12\\) minutes and \\(12,01\\) minutes. Then why not just look at a range, e.g.¬†\\((11,5, 12.5)\\) minutes directly? In this case, instead of looking at fixed values, it makes sense to look at entire regions/ intervals (\\(\\leadsto\\) continuous random variables) and at the density of these regions as the basis for the probability calculation, with the probabilities represented as areas under this density on the corresponding interval. This implies that the probability of hitting a very specific value (the point probability) becomes zero. A random variable \\(X\\) is said to be continuous if there is a function (density function) \\(x\\mapsto f(x)\\geq 0\\), so that for every interval \\([a,b]\\) we have: \\[\\begin{equation*} \\mathbb P(a\\leq X\\leq b)=\\int_a^b f(x) dx. \\end{equation*}\\] The set of values \\(\\mathcal T\\subseteq\\mathbb R\\) of a continuous random variable is uncountable. It corresponds either to the whole \\(\\mathbb R\\) or to the subsets of \\(\\mathbb R\\) (intervals). 2.2.1 Density function Density properties: \\(f(x)\\geq 0\\) for all \\(x\\in \\mathbb R\\) \\(\\int_{\\mathbb R} f(x)\\, dx=1\\) The point probability is zero: \\(\\mathbb P(X=x)=0\\) for all \\(x\\in \\mathbb R\\) if \\(X\\) represents a continuous random variable. Graphical representation: Figure 2.3: Example: Density function Probabilities are represented by areas under density \\(f(x)\\). Calculation of the probability that \\(X\\) assumes values in set \\(B\\) using the density function: \\[\\begin{equation*} \\mathbb P(X\\in B) = \\int_B f(x)\\, dx = \\int_a^b f(x)dx \\end{equation*}\\] 2.2.2 Distribution function The (cumulative) distribution function of a continuous Random variable is given as \\[\\begin{equation*} F(x) = \\mathbb P(X\\leq x) = \\int_{-\\infty}^x f(t)\\, dt. \\end{equation*}\\] Properties of the distribution function for continuous random variables: \\(F\\) is a continuous function \\(x\\mapsto F(x)\\) is monotonically increasing \\(0\\leq F(x)\\leq 1\\), for all \\(x\\in \\mathbb R\\) \\(\\lim_{x\\rightarrow-\\infty} F(x)=0\\) and \\(\\lim_{x\\rightarrow\\infty} F(x)=1\\). Graphical representation: Figure 2.4: Example: Probability function of a continuous random variable If the distribution function \\(F\\) is given, one can obtain the density \\(f\\) by deriving the former: \\[\\begin{equation*} f(x) = F^\\prime(x),\\quad\\text{ differentiable for $F$ in $x$} \\end{equation*}\\] Calculating the probability that \\(X\\) takes values in \\(B\\) using the distribution function: \\[\\begin{equation*} \\mathbb P(X\\in B) = \\mathbb P(a\\leq X \\leq b) = F(b) - F(a). \\end{equation*}\\] Example 2.6 (Time in the online shop as a continuous random variable) We model the random variable \\(X=\\) time in the online shop in minutes as a continuous random variable with the density: \\[ f(x) = \\begin{cases}0,001\\cdot x , &amp;\\text{ f√ºr } 0\\leq x\\leq 20,\\\\ 0,025 -0,00025\\cdot x, &amp;\\text{ f√ºr } 20&lt; x\\leq 100.\\end{cases} \\] That is, Figure 2.5: Probability density of X=Time in online shop We compute the event probabilities: \\(\\{X&lt;20\\}=\\text{&quot;Time in shop less than 20 minutes&quot;}\\): \\[\\begin{align} \\mathbb P(X&lt;20)&amp;= \\displaystyle \\int_0^{20} (0,001\\cdot x )dx = \\text{Area of the red triangle}\\\\ &amp;= \\frac{1}{2}\\cdot 20\\cdot 0,02 = 0,2. \\end{align}\\] \\(\\{X&lt;60\\}=\\text{&quot;Time in shop less than 60 minutes&quot;}\\): \\[\\begin{align} \\mathbb P(X&lt;60)&amp;= 1-\\mathbb P(X\\geq 60)=1-\\displaystyle \\int^{100}_{60} (0,025 -0,00025\\cdot x)dx \\\\ &amp;= 1- \\text{Area of the transparent triagle} = 1-\\frac{1}{2}\\cdot 40\\cdot 0,01 = 0,8. \\end{align}\\] Figure 2.6: Probability density of X=Time in online shop with probabilities as areas under the density fucntion. Computing the cumulative probability function from the density function: case: \\(x\\in [{0},{20}]\\): \\[\\begin{align} F(x) &amp;= \\int_{{0}}^x f(t)~ dt = \\int_{0}^x (0,001\\cdot t )~ dt=\\Big[0,001\\cdot \\frac{t^2}2 \\Big]_{0}^x \\\\ &amp; = 0,001\\cdot \\frac{x^2}2 - (0,001\\cdot \\frac{0^2}2 )\\\\ &amp;=0,0005x^2. \\end{align}\\] case: \\(x\\in ({20},{100}]\\): \\[\\begin{align} \\displaystyle F(x) &amp;= \\int_{0}^x f(t)~ dt\\\\ &amp;= \\int_{0}^{20} (0,001\\cdot t )~ dt + \\int_{20}^x (0,025 -0,00025\\cdot t)~ dt\\\\ &amp;=0,2 + \\big[0,025\\cdot t -0,00025\\cdot \\frac{t^2}2 \\big]_{20}^x\\\\ &amp;=0,2 + 0,025\\cdot x -0,00025\\cdot \\frac{x^2}2 - (0,025\\cdot 20 -0,00025\\cdot \\frac{20^2}2) \\\\ &amp;= -0.25 + 0.025x -0.000125x^2. \\end{align}\\] That is: \\[ F(x) = \\begin{cases} 0,&amp;x&lt;0,\\\\ 0,0005x^2,&amp;0\\leq x\\leq 20,\\\\ -0.25 + 0.025x -0.000125x^2,&amp;20&lt;x\\leq 100,\\\\ 1,&amp;x&gt;100. \\end{cases} \\] Figure 2.7: Cumulative distribution function of X=Time in online shop Using the cumulative distribution function, we can calculate the event probabilities: \\(\\{X&lt;20\\}=\\text{&quot;Time in shop less than 20 minutes&quot;}\\): \\[\\begin{align}\\mathbb P(X&lt;20)&amp;= \\mathbb P(X\\leq20)=F(20) = 0,0005\\cdot 20^2= 0.2.\\end{align}\\] \\(\\{X&lt;60\\}=\\text{&quot;Zeit im Shop weniger als 60 Minuten&quot;}\\):\\[\\begin{align}\\mathbb P(X&lt;60)&amp;=\\mathbb P(X\\leq60)=F(60) =-0.25 + 0.025\\cdot 60 -0.000125\\cdot 60^2= 0.8.\\end{align}\\] \\(\\{X &gt;30\\}=\\text{&quot;Time in shop less than 30 minutes&quot;}\\):\\[\\begin{align} \\mathbb P(X&gt;30)&amp;=1-\\mathbb P(X\\leq30)= 1-F(30) \\\\&amp;= 1-(-0.25 + 0.025\\cdot 30 -0.000125\\cdot 30^2)\\\\ &amp;= 0.6125.\\end{align}\\] Figure 2.8: Cumulative probability function of X=Time in online shop with probabilities \\(f(x)=\\begin{cases}2x,&amp;\\text{ if }0\\leq x \\leq 1,\\\\0,&amp;\\text{ if } x&lt;0,\\\\1,&amp;\\text{ if }x&gt;1.\\end{cases}\\) \\(f(x)=\\begin{cases}x^2,&amp;\\text{ if }0\\leq x \\leq 1,\\\\0,&amp;\\text{ if } x&lt;0,\\\\1,&amp;\\text{ if }x&gt;1.\\end{cases}\\) \\(f(x)=\\begin{cases}2x,&amp;\\text{ if }0\\leq x \\leq 1,\\\\0,&amp;\\text{ if } x&lt;0,x&gt;1.\\end{cases}\\) \\(f(x)=\\begin{cases}\\frac13 x^3,&amp;\\text{ if }0\\leq x \\leq 1,\\\\0,&amp;\\text{ if } x&lt;0,\\\\1,&amp;\\text{ if }x&gt;1.\\end{cases}\\) Submit Submit Submit Submit Submit Hint: use conditional probability and note that \\(\\mathbb P((0.1 \\leq X \\leq 0.3)(X\\leq 0.5))=\\(\\mathbb P(0.1 \\leq X \\leq 0.3)\\) 2.2.3 Expected value and variance Expected value and variance are the most important metrics, as many families of distributions (more on this later) are clearly defined by these two metrics. The two measures give us information about the mean position and variability of the values of random variables. Expected value Definition 2.4 (Expected Value/ Expectation (continuous rv)) The expected value \\(\\mathbb E(X)\\) of a continuous random variable \\(X\\) with density \\(f(x)\\) is \\[\\begin{equation*} \\mathbb E(X) = \\int_{-\\infty}^\\infty x\\, f(x)\\, dx. \\end{equation*}\\] Properties of the expected value (the same as for discrete rv) \\(\\mathbb E(a X + b) = a \\mathbb E (X) + b\\) (linear transformation) \\(\\mathbb E(X+Y) = \\mathbb E(X) + \\mathbb E(Y)\\) (expected value of a sum = sum of expected values) Variance and standard deviation (the same as for discrete rv) Definition 2.5 (Variance and standard deviation (continuous rv)) The variance of a continuous random variable \\(X\\) is \\[\\begin{equation*} \\sigma^2 = \\text{Var}(X) = \\mathbb E((X-\\mathbb E(X))^2)= = \\int_{-\\infty}^\\infty (x-\\mathbb E(X))^2\\, f(x)\\, dx. \\end{equation*}\\] The standard deviation is \\(\\sigma = \\sqrt{\\text{Var}(X)}.\\) Best suited for calculating variance: \\[\\begin{equation*} \\text{Var}(X)=\\mathbb E(X^2) - (\\mathbb E(X))^2. \\end{equation*}\\] For continuous random variables this means: \\[\\begin{align} \\text{Var}(X)&amp;=\\int_{-\\infty}^{\\infty} x^2f(x)dx- (\\mathbb E(X))^2. \\end{align}\\] Properties of Variance \\(\\text{Var}(aX+b)= a^2\\text{Var}(X)\\) (linear transformation) \\(\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y)\\) if \\(X\\) and \\(Y\\) are stochastically independent. This results in the calculation rules for linear transformations (\\(Y=a X + b\\)): \\[\\begin{align} \\mathbb E(Y) &amp;= \\mathbb E (a X + b)=a \\mathbb E(X) + b,\\\\ \\text{Var}(Y)&amp;=\\text{Var}(a X + b) = a^2\\text{Var}(X). \\end{align}\\] Example 2.7 (Time in the online shop as a continuous random variable) The random variable \\(X=\\) time in the online shop in minutes has the density: \\[ f(x) = \\begin{cases}0,001\\cdot x , &amp;\\text{ f√ºr } 0\\leq x\\leq 20,\\\\ 0,025 -0,00025\\cdot x, &amp;\\text{ f√ºr } 20&lt; x\\leq 100.\\end{cases} \\] We have: Expected value: \\[\\begin{align} \\mathbb E(X) &amp;= \\int_{-\\infty}^{\\infty}x\\cdot f(x)dx = \\int_{0}^{20}x\\cdot f(x)dx + \\int_{20}^{100}x\\cdot f(x)dx\\\\ &amp;=\\int_{0}^{20}x\\cdot(0,001\\cdot x )dx + \\int_{20}^{100}x\\cdot (0,025 -0,00025\\cdot x)dx\\\\ &amp;=\\int_{0}^{20}0,001\\cdot x^2 dx + \\int_{20}^{100}0,025\\cdot x -0,00025\\cdot x^2dx\\\\ &amp;=\\Big[0,001\\cdot \\frac{x^3}3\\Big]_{0}^{20} + \\Big[0,025\\cdot \\frac{x^2}2 -0,00025\\cdot \\frac{x^3}3\\Big]_{20}^{100}\\\\ &amp;=2,6667 + 37,3333 = 40. \\end{align}\\] Variance: \\[\\begin{align} \\text{Var}(X) &amp;= \\mathbb E(X^2) - \\mathbb E(X) = \\int_{-\\infty}^{\\infty}x^2\\cdot f(x)dx - 40^2\\\\ &amp;\\ldots \\text{ (analog zu dem Erwartungswert) }\\\\ &amp;=\\int_{0}^{20}0,001\\cdot x^3 dx + \\int_{20}^{100}\\big(0,025\\cdot x^2 -0,00025\\cdot x^3\\big)dx - 40^2\\\\ &amp;=\\Big[0,001\\cdot \\frac{x^4}4\\Big]_{0}^{20} + \\Big[0,025\\cdot \\frac{x^3}3 -0,00025\\cdot \\frac{x^4}4\\Big]_{20}^{100}- 40^2\\\\ &amp;=40 + 2026,6667 - 40^2= 466,6667. \\end{align}\\] Submit Submit 2.2.4 Quantiles of continuous random variables In the first part of statistics course, we determined empirical quantiles. They helped us to understand which value divides the ordered observations so that at least a proportion \\(p\\) of the observations are less than or equal and at least a proportion \\(1-p\\) - greater or equal. In other words, \\(\\hat q_p\\) has answered the question: which value is fallen below with frequency \\(p\\) or exceeded with frequency \\(1-p\\)? The same logic is used to define quantiles for random variables, except that it is now about the probability of falling below or exceeding this value. Definition 2.6 (Quantile (continuous rv)) The \\(p\\)-quantile of a continuous random variable \\(X\\) with density \\(f(x)&gt;0\\) on an interval in \\(\\mathbb R\\) is the number \\(q_p\\) with \\(0&lt;p&lt; 1\\), for which applies: \\[\\begin{equation*} \\mathbb P(X&lt;q_p)=\\mathbb P(X\\leq q_p)=F(q_p)=p. \\end{equation*}\\] The quantile is (for continuous random variables) clearly determined by the inverse of the distribution function, \\[\\begin{equation*} q_p=F^{(-1)}(p). \\end{equation*}\\] Figure 2.9: Quantil der Standardnormalverteilung f√ºr p=0,9 Special cases: \\(p=0.25\\), then \\(Q_p\\) is called the lower quartile \\(p=0.50\\), then \\(Q_p\\) is called the median \\(p=0.75\\), then \\(Q_p\\) is called the upper quartile Example 2.8 (Time in the online shop as a continuous random variable) Example 2.6 cont. The random variable \\(X=\\) time in the online shop in minutes has the distribution function: \\[ F(x) = \\begin{cases} 0,&amp;x&lt;0,\\\\ 0,0005x^2,&amp;0\\leq x\\leq 20,\\\\ -0.25 + 0.025x -0.000125x^2,&amp;20&lt;x\\leq 100,\\\\ 1,&amp;x&gt;100. \\end{cases} \\] \\(0.1\\) quantile: The distribution function takes the value \\(0.1\\) on the intercept \\(0\\leq x \\leq 20\\), so we set \\[F(q_{0,1})=0.1\\rightarrow 0.0005q_{0.1}^ 2 = 0.1\\] and solve for \\(q_{0.1}\\): \\[q_{0.1} = \\pm\\sqrt{\\frac{0.1}{0.0005}} = \\pm 14, 1421,\\] so \\(q_{0,1} =14.1421\\), since \\(0\\leq q_{0,1} \\leq 20\\) must hold. With a probability of 10%, the time spent in the online shop is shorter than \\(14.1421\\) minutes. \\(0.9\\) quantile: The distribution function takes the value \\(0.9\\) on the intercept \\(20\\leq x \\leq 100\\), so we set \\[F(q_{0.9})=0.9\\rightarrow -0.25 + 0.025 q_{ 0.9} - 0.000125q_{0.1}^2 = 0.9.\\] Then (\\(abc\\) formula for quadratic equations \\(ax^2 + bx + c=0\\): \\(x_{1,2}=\\frac{-b\\pm\\sqrt{b^2 - 4ac}}{2a}\\)) we have \\[q_{0.9} = \\frac{-0.025\\pm\\sqrt{0.025^2 - 4\\cdot(-0.000125)\\cdot(-1.15)}}{2\\cdot ( -0.000125)}.\\] In addition, \\(q_{0.9}\\) should be between \\(20\\) and \\(100\\). So, let‚Äôs take the ‚Äú\\(+\\) version‚Äù and calculate \\(q_{0.9} = 71.7157\\). With a probability of 10%, time spent in the online shop is longer than \\(71.7157\\) minutes. 0.1111 0.25 0.3333 0.5 0.6667 Submit "],["some-important-distribution-models.html", "Chapter 3 Some important distribution models 3.1 Discrete distributions 3.2 Continuous distributions 3.3 Further distributions", " Chapter 3 Some important distribution models We have already modeled some random variables and it was always quite time-consuming to derive the distribution of random variables. We had to look at the original random process and determine for each result what value the random variable takes on for this result and derive the probability of the values based on that. Or with the continuous random variables, we had to laboriously calculate the distribution function, the expected value and the variance based on the density üí©. Luckily üòç there are some pretty universal distribution models with variable parameters for which nice properties are already known and key figures can be determined using the parameters, so you don‚Äôt have to use them every time must be derived again. In this chapter we will learn: Which typical distribution models exist for discrete and continuous random variables? which requirements must be met so that a distribution model can be used in a specific case and how to determine the parameters of these distributions from the facts. Since distributions associate values with probabilities, there are special distribution models for discrete random variables with discrete values and for continuous random variables that have intervals of sets of values. 3.1 Discrete distributions Discrete distributions describe probabilities concerning discrete random variables. Figure 3.1: Distributional models for discrete random variables 3.1.1 Discrete uniform distribution A discrete random variable \\(X\\) is uniformly distributed on the value set \\(\\mathcal T=\\{x_1,x_2,\\ldots, x_n\\}\\) if for all \\(i=1,\\ldots, n\\) the following holds: \\[\\begin{equation*} f(x_i)=\\frac{1}{n}. \\end{equation*}\\] The following applies to such random variables: \\[\\begin{align*} \\mathbb E(X) &amp;= \\frac{1}{n} \\sum_{i=1}^n x_i\\\\ \\text{Var}(X) &amp;= \\frac{1}{n}\\sum_{i=1}^n x_i^2 - (\\mathbb E(X))^2 \\end{align*}\\] Application examples: Rolling a fair game dice Selecting a customer from the pool of \\(n\\) customers Example 3.1 (Product recommendations) To test how many product recommendations should be displayed to the customer during a shop visit in order to achieve the greatest effect, one could use discrete uniform distribution. (We want to recommend our products ü§ë but at the same time we don‚Äôt want to annoy you too much or distract you from shopping ü§Ø) This would mean choosing a number between a minimum number of product recommendations per shop visit (e.g.¬†only one recommendation) and a maximum (e.g.¬†five product recommendations) for each customer with equal probability and then displaying this number of recommendations. You can then analyze what number of product recommendations would be optimal. So in the background there is a uniform distribution and \\(X=\\) ‚ÄòThe number of product recommendations displayed‚Äô is discretely equally distributed between \\(x_{min}=1\\) and \\(x_{max}=5\\). Set of values: \\(\\{x_1=1;x_2=2;\\ldots;x_5=5\\}\\) Parameters: \\(n=5\\) Probability function: \\(f(x_i) = \\frac 15\\) Parameters: Expected value: \\(\\mathbb E(X) = \\frac 15(1 + 2 + \\ldots +5) = 3\\). On average, around 3 product recommendations are displayed. Variance: \\(\\text{Var}(X)=\\frac 15 (1^2 + 2^2 + \\ldots + 5^2) - 3^2 = 2\\). So the standard deviation of the NUMBER of product recommendations is \\(\\sqrt{2}\\). Figure 3.2: Discrete uniform distribution with n=5 3.1.2 Bernoulli distribution Let \\(A\\in\\mathcal F\\) be an event with \\(p=\\mathbb P(A)\\). The random variable \\(X=1_A\\) indicating whether an event \\(A\\) occurs or not (the indicator of \\(A\\)): \\[\\begin{equation*} 1_A= \\begin{cases} 1,&amp;\\text{ if $A$ occurs},\\\\ 0,&amp;\\text{ if $A$ does not occur}. \\end{cases} \\end{equation*}\\] is Bernoulli distributed with parameter \\(p\\) (\\(X\\sim Ber(p)\\)). The indicator function \\(1_A\\) takes the value \\(1\\) if event \\(A\\) occurs and the value \\(0\\) if \\(A\\) does not occur. Examples of \\(A\\): an even number is obtained when rolling the dice; a loan customer does not repay his loan; a patient is cured by a specific therapy; the product of a delivery of goods is defective. A random variable \\(X\\) is Bernoulli distributed with parameter \\(p\\in (0,1)\\) if: Set of values: \\(\\mathcal T=\\{0,1\\}\\) Probability function: \\(f(1)=p\\), \\(f(0)=1-p\\) Parameters: Expected value: \\[\\begin{equation*} \\mathbb E(X)=p. \\end{equation*}\\] Variance: \\[\\begin{equation*} \\text{Var}(X)=p(1-p). \\end{equation*}\\] You can easily check: \\[\\begin{align*} \\mathbb E(X) &amp;= 1\\cdot f(1) + 0\\cdot f(0) = p\\\\[3pt] \\text{Var}(X) &amp;= 1^2\\cdot f(1) + 0^2\\cdot f(0) - (\\mathbb E(X))^2 % = p - p^2 = p(1-p). \\end{align*}\\] Example 3.2 (Indicator of app usage) In example 2.5, we defined the random variable \\(A=\\)‚Äúapp usage indicator‚Äù: \\(A=\\begin{cases}1, &amp;\\text{ if } App\\\\0, &amp;\\text{ if } Web.\\end{cases}\\) \\(A\\) only takes two values \\(\\{0;1\\}\\) and based on the occurrence of the event \\(App=\\)‚Äúa randomly selected customer uses the app‚Äù. The probability \\(\\mathbb P(App)=0.6\\). This means: \\(A\\sim Ber(p=0.6)\\) \\(\\mathbb E(A) = 0.6\\) and \\(\\text{Var}(A) = 0.6\\cdot (1-0.6) = 0.24.\\) We took a little longer when we had to calculate these metrics ‚Äúfrom scratch‚Äù in 2.5 üòè Figure 3.3: Bernoulli distribution with p=0.6 3.1.3 Binomial distribution The binomial distribution results when the sum of hits is formed from \\(n\\) independent repetitions of a Bernoulli experiment with constant probability \\(p\\). I.e., if a Bernoulli random process is repeated \\(n\\) times with constant probability \\(p=\\mathbb P(A)\\), then is the random variable \\[\\begin{equation*} X=\\text{&quot;Number of attempts in which $A$ occurs&#39;&#39;} \\end{equation*}\\] binomially distributed with parameters \\(n\\) and \\(p\\). A random variable is called binomially distributed with the parameters \\(n\\) and \\(p\\), for short \\(X\\sim B(n,p)\\), if the following applies: \\[\\begin{equation*} \\mathbb P(X=k) = f(k) = \\binom{n}{k} p^k\\, (1-p)^{n-k},\\quad\\text{ for } k=0,1,\\ldots, n, \\end{equation*}\\] and \\(f(k)=0\\) otherwise.Remember: Binomial coefficient: \\(\\displaystyle\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\) with \\(n!=1\\cdot 2\\cdot \\ldots \\cdot n\\) and \\(0!=1\\). Parameters: \\[\\begin{equation*} \\mathbb E(X) = np\\quad\\text{ and }\\quad \\text{Var}(X) = n p(1-p) \\end{equation*}\\] Additive property: The sum of independent binomial distributed random variables with the same parameter \\(p\\) is again binomially distributed: if \\(X\\sim B(n,p)\\) and \\(Y\\sim B(m,p)\\) are independent, then \\(X+Y\\sim B(n+m,p)\\). Application examples: Number of loans in a portfolio that are not being serviced (assumption: loans default independently of each other.) Number of patients cured by therapy; Number of products in a delivery that are defective. Example 3.3 (Number of app users) If we select \\(n=10\\) customers randomly (with replacement) from the customer pool and state whether they are app users or not, then we repeat the Bernoulli process: ‚ÄòDrawing a random customer and determining the \\(App\\)-property‚Äô independently ten times. The probability of success, here the probability of selecting an app user, corresponds to \\(\\mathbb P(App)=0.6\\) the proportion of app users in the entire customer pool. So \\(X=\\)‚ÄôThe number of app users among the randomly selected ten customers‚Äô is binomially distributed: \\(X\\sim B(\\color{green}{n=10}; \\color{blue}{p=0.6})\\) We can now calculate the probabilities üòé: \\[\\begin{align} \\mathbb P(X=\\color{red}{5}) &amp;=\\binom{\\color{green}{10}}{\\color{red}{5}} \\color{blue}{0.6}^\\color{red}{5}\\,(1-\\color{blue}{0.6})^{\\color{green}{10}-\\color{red}{5}} = 0.2007,\\\\ \\mathbb P(X&lt;2) &amp;=\\mathbb P(X=0) + \\mathbb P(X=1)=\\binom{\\color{green}{10}}{0} \\color{blue}{0.6}^0\\cdot(1-\\color{blue}{0.6}) ^{{\\color{green}{10}}-0} + \\binom{\\color{green}{10}}{1} \\color{blue}{0.6}^1\\,(1-\\color{blue}{0.6})^{\\color{green}{10}-1} \\\\ &amp;= 0.0017,\\\\ \\mathbb P(X=8) &amp;=\\binom{\\color{green}{10}}{8} \\color{blue}{0.6}^8\\,(1-\\color{blue}{0.6})^{\\color{green}{10}-8} = 0.1209. \\end{align} \\] Parameters: \\[\\begin{align} \\mathbb E(X) &amp;= n\\cdot \\color{blue}{p} = 10\\cdot \\color{blue}{0.6} = 6\\\\ \\text{Var}(X)&amp;= n\\cdot \\color{blue}{p}\\cdot(1-\\color{blue}{p}) = 10\\cdot \\color{blue}{0.6}\\cdot (1-\\color{blue}{0.6}) = 2.4. \\end{align}\\] Figure 3.4: Binomial distribution with n=10 and p=0.6 Example 3.4 (Loan portfolio) Consider a homogeneous loan portfolio with \\(500\\) loans and default probability of \\(1\\)%. Random variable \\(A = \\text{&#39;default of a loan&#39;}\\) with values \\(a\\in \\{0 (\\text{no default}),1 (\\text{default})\\}\\) is Bernoulli distributed with \\(p=0.01\\). Random variable \\(P = \\text{&#39;Number of defaults in the loan portfolio&#39;}\\) (assumptions: defaults occur independently of one another). Under the assumptions, \\(P\\) is \\(\\text{binomially distributed}\\) with \\(\\color{blue}{p=0.01}\\) and \\(\\color{green}{n=500}\\): \\(P\\sim B(500,0.01)\\). \\[\\begin{align*} \\mathbb E(P) &amp;= \\color{green}{n}\\color{blue}{p} = \\color{green}{500}\\cdot \\color{blue}{0.01} = 5,\\\\ \\text{Var}(P) &amp;=\\color{green}{n}\\color{blue}{p}(1-\\color{blue}{p})=\\color{green}{500}\\cdot \\color{blue}{0.01}\\cdot (1-\\color{blue}{0.01}) = 4.95,\\\\ \\mathbb P(P&gt;1) &amp;=1-\\mathbb P(P\\leq 1) =1-\\sum_{k=0}^1\\binom{n}{k}p^k(1-p)^{n-k} \\\\ &amp;=1- \\Big(\\frac{500!}{0!500!}(\\color{blue}{0,01})^0(1-\\color{blue}{0.01})^{500} + \\frac{500!}{1!499!}(\\color{blue}{0.01})^1(1-\\color{blue}{0.01})^{499}\\Big)=0,9602. \\end{align*}\\] Figure 3.5: Binomial distribution with n=500 and p=0.01 Submit Submit \\()\\) Submit Submit Submit Submit \\()\\) Submit Submit 3.1.4 Geometric distribution If a Bernoulli experiment is repeated with the same probability \\(p=\\mathbb P(A)\\) until the first one The random variable is the time the event of interest \\(A\\) occurs \\[\\begin{equation*} X=\\text{&quot;Number of attempts until $A$ occurs for the first time&#39;&#39;} \\end{equation*}\\] is geometrically distributed with parameter \\(p\\). Application examples: Duration (‚Äúdiscrete waiting time‚Äù in days) until the first failure of one device, Duration in days until an insurance customer makes one for the first time reports damage, Number of customers interviewed (waiting time in customers üòÑ), until the first time a customer with the desired characteristic is interviewed. A random variable \\(X\\) is said to be geometrically distributed (\\(X\\sim G(p)\\)) with parameter \\(p\\) if: \\[\\begin{equation*} \\mathbb P(X=k)=f(k) = (1-p)^{k-1}\\cdot p, \\quad \\text{ for } k=1,\\ldots, \\end{equation*}\\] and \\(f(k)=0\\) else. Parameters: \\[\\begin{align*} \\mathbb E(X) &amp;= \\sum _{k=1}^\\infty k\\, p\\, (1-p)^{k-1} = \\frac{1}{p}\\\\ \\text{Var}(X) &amp;= \\sum_{k=1}^\\infty k^2 \\, p\\, (1-p)^{k-1} - \\frac{1}{p^2} = \\frac{1-p}{p^2} \\end{align*}\\] Example 3.5 (App user in a customer Survey) A survey of \\(n=50\\) customers is conducted. Customers are randomly selected in the survey and can take part multiple times. The proportion of customers who use the app is 60%. Random variable: \\(M = \\text{&#39;Number of app users in the customer survey&#39;}\\), \\(M\\sim B(n=50,p=0.6)\\). Random variable \\(V = \\text{&#39;Number of respondents until an app user is interviewed&#39;}\\) is geometrically distributed \\(V\\sim G(p=0.6)\\). \\[\\begin{align*} \\mathbb E(V) &amp;= \\frac 1p = 1.6667,\\\\ \\text{Var}(V) &amp;=\\frac{1-p}{p^2}=\\frac{0.4}{0.36} = 1.1111,\\\\ \\mathbb P(V&lt; 5)&amp; = \\mathbb P(V\\leq 4) =f_{G_{0.6}}(1) + f_{G_{0.6}}(2) + \\ldots + f_ {G_{0,6}}(4),\\\\ &amp;=(1-0.6)^0\\cdot 0.6 + (1-0.6)^1\\cdot 0.6 + \\ldots + (1-0.6)^3\\cdot 0.6 = 0,9898. \\end{align*}\\] Figure 3.6: Geometric distribution with p=0.6 3.1.5 Poisson distribution Sometimes we also need a distribution that indicates the number of ‚Äúsuccesses‚Äù per unit of time. For example, if the random process ‚Äúcontinues‚Äù, such as an online shop is always available and the number of customers who log in per hour is of interest. We may be interested in the following random variable: \\[\\begin{equation*} X=\\text{&quot;Number of events that occur within a time interval $[0,1]$&#39;&#39;}. \\end{equation*}\\] Under the following assumptions, \\(X\\) is Poisson distributed: Two events cannot occur at the same time. The probability that an event occurs in a small time interval of length \\(\\Delta t\\) is approximately \\(\\lambda\\Delta t\\). The probability of occurrence depends only on the length of the time interval, not on the location. The numbers of events in disjoint subintervals are independent. A random variable \\(X\\) is Poisson distributed (\\(X\\sim Poi(\\lambda)\\)) with parameter \\(\\lambda\\) if: \\[\\begin{equation*} \\mathbb P(X=k) = f(k) = \\frac{\\lambda^k}{k!}\\, \\text{e}^{-\\lambda}, \\quad k=0,1,2,\\ldots, \\end{equation*}\\] and \\(f(k)=0\\) else. The parameter \\(\\lambda\\) is also called intensity rate. Parameters: \\[\\begin{equation*} \\mathbb E(X)=\\lambda\\quad \\text{ and } \\quad \\text{Var}(X)=\\lambda. \\end{equation*}\\] Addition property: If \\(X\\sim Poi(\\lambda)\\) and \\(Y\\sim Poi(\\mu)\\) are independent, then: \\[\\begin{equation*} X+Y\\sim Poi(\\lambda+\\mu). \\end{equation*}\\] In particular, it follows that the number of events in an interval of length \\(t\\) is Poisson distributed with parameters \\(\\lambda t\\). Application examples: the number of loan defaults by a bank in a given accounting period; the number of customers arriving at a counter during a certain period of time; Example 3.6 (Advertisement) An advertising banner will be displayed on your website between 8 a.m. and 10 a.m. or 7 p.m. and 8 p.m. The visitors to your website act independently and the click rate (\\(\\lambda\\)) in the two time periods is constant and is 5 thousand clicks per hour. \\(B_{morning} = \\text{&#39;Number of clicks between 8 and 10 a.m.&#39;}\\), \\(B_M\\sim Poi(\\lambda_{morning}=2\\cdot\\lambda=10)\\). \\(B_{evening} = \\text{&#39;Number of clicks between 7 p.m. and 8 p.m.&#39;}\\), \\(B_A\\sim Poi(\\lambda_{evening}=\\lambda=5)\\). Because of the addition property of the Poisson distribution: \\(B_{total} = \\text{&#39;number of clicks on your advertising banner in the two time periods&#39;}\\) is also Poisson distributed \\(B_{total}\\sim Poi(\\lambda_{total} =\\lambda _{morning}+\\lambda _{evening}=15)\\). \\[\\begin{align} \\mathbb E(B_{total}) &amp;= \\lambda_{total} = 15,\\\\ \\text{Var}(B_{total}) &amp;=\\lambda_{total}=15,\\\\ \\mathbb P(B_{total}&gt;10)&amp;=1-\\mathbb P(B_{total}\\leq 10) = 1-(f_{poi_{15}}(0) + f_{poi_{15}}( 1) + \\ldots+ f_{poi_{15}}(10))\\\\ &amp;=1-\\exp(-15)\\cdot\\Big(\\frac{15^0}{0!}+\\frac{15^ 1}{1!}+\\ldots+\\frac{15^{10}}{10!}\\Big)= 0.8815. \\end{align}\\] Figure 3.7: Poisson distribution with \\(\\lambda=15\\) \\(Ber(0.25)\\) \\(B(n=100,p=0.25)\\) \\(G(p=0.25)\\) \\(Poi(\\lambda=0.25)\\) Submit Submit \\(Ber(0.5)\\) \\(B(n=4,p=0.5)\\) \\(G(p=0.5)\\) \\(Poi(\\lambda=0.5)\\) Submit Submit 3.2 Continuous distributions There are also continuous random variables distribution models that are used in several contexts. We will learn about a pair of continuous distributions here. Figure 3.8: Distributional models for continuous random variables 3.2.1 Normal distribution The normal distribution is the best known and most important distribution. In many applications, the empirical distribution of a feature can be approximated sufficiently well using a normal distribution. In particular, the normal distribution is a good model if the underlying variable results from the additive overlap of a large number of independent, random individual effects. The theoretical justification for this empirically observable phenomenon is provided by the Central Limit Theorem (\\(\\leadsto\\) next chapter) The normal distribution is often called the Gauss distribution, named after the mathematician Carl Friedrich Gau√ü (1777‚Äì1855). A random variable \\(X\\) is said to be normally distributed with parameters \\(\\mu\\in \\mathbb R\\) and \\(\\sigma&gt;0\\), for short \\(X\\sim N(\\mu,\\sigma^2)\\), if it has the has the following density: \\[\\begin{equation*} \\varphi(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right), \\quad x\\in \\mathbb R. \\end{equation*}\\] Key parameters: \\[\\begin{equation*} \\mathbb E(X)=\\mu\\quad\\text{ and }\\quad \\text{Var}(X)=\\sigma^2. \\end{equation*}\\] With \\(\\mu=0\\) and \\(\\sigma^2=1\\) you get the standard normal distribution \\(N(0,1)\\) with density \\[\\begin{equation*} \\varphi(x)=\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right), \\quad x\\in \\mathbb R. \\end{equation*}\\] The density of the standard normal distribution is denoted as \\(\\varphi(x)\\) and the distribution function as \\(N(x)\\). Properties of normal distribution The density is symmetrical about \\(\\mu\\), i.e. \\[\\begin{equation*} \\varphi(\\mu-x) = \\varphi(\\mu+x), \\quad x\\in \\mathbb R. \\end{equation*}\\] In particular in the case \\(\\mu=0\\): \\(\\varphi(-x)=\\varphi(x)\\). The following applies to the distribution function of the standard normal distribution: \\(N(-x) = 1- N(x)\\) Standardize: \\(X\\) is \\(N(\\mu,\\sigma^2)\\)-distributed \\(\\iff\\) \\(\\displaystyle\\frac{X-\\mu}{\\sigma}\\) is \\(N(0,1)\\)-distributed . \\(Y\\) is \\(N(0,1)\\)-distributed \\(\\iff\\) \\(\\mu + \\sigma Y\\) is \\(N(\\mu,\\sigma^2)\\)-distributed. Probabilities within \\(\\sigma\\) bounds \\[\\begin{align*} \\mathbb P(|X-\\mu|\\leq \\sigma) &amp;= 0.6827\\\\ \\mathbb P(|X-\\mu|\\leq 2\\sigma) &amp;= 0.9545\\\\ \\mathbb P(|X-\\mu|\\leq 3\\sigma) &amp;= 0.9973 \\end{align*}\\] Addition property: If \\(X\\sim N(\\mu_X,\\sigma_X^2)\\) and \\(Y\\sim N(\\mu_Y,\\sigma_Y^2)\\) are independent, then \\[\\begin{equation*} X+Y\\sim N(\\mu_X+\\mu_Y, \\sigma_X^2+\\sigma_Y^2). \\end{equation*}\\] Probability with which an \\(N(\\mu,\\sigma^2)\\)-distributed random variable lies between the boundaries \\(a\\) and \\(b\\): \\[\\begin{equation*} \\mathbb P(a\\leq X\\leq b) = \\mathbb P\\left(\\frac{a-\\mu}{\\sigma}\\leq \\frac{X-\\mu}{\\sigma} \\leq \\frac{b-\\mu}{\\sigma}\\right) = N\\left(\\frac{b-\\mu}{\\sigma}\\right) - N\\left(\\frac{a-\\mu}{\\sigma}\\right) \\end{equation*}\\] Example 3.7 (Average temperature) We model the average temperature in Berlin in July as a random variable \\(T: \\text{&#39;Average temperature in Berlin in July&#39;}\\) with values \\(t\\in \\mathbb R\\). \\(T\\stackrel{}{\\sim}{} N(\\mu_T,\\sigma_T)\\) with \\(\\mu_T=\\mathbb E(T)=20\\), \\(\\text{Var}(T)=16\\) and \\(\\sigma_T = 4\\). Probability: \\[\\begin{align*} \\mathbb P(|T-20|\\leq 4) &amp;\\approx 0.68,\\\\ \\mathbb P(|T-20|\\leq 8) &amp;\\approx 0.95,\\\\ \\mathbb P(15\\leq T\\leq 25)&amp; = \\mathbb P(T\\leq 25)-\\mathbb P(T&lt;15) =N\\Big(\\frac{25-20}{4}\\Big) - N\\Big(\\frac{15-20}{4}\\Big)\\\\ &amp; = 0.8944 - 0.1056 = 0.7887. \\end{align*}\\] Figure 3.9: Distributional models for continuous random variables Example 3.8 (Sample mean of normally distributed random variables) If we make a random selection (with replacement) from a normally distributed population and write down a possible result as \\(n\\) random variable ‚Äúplaceholders‚Äù as \\(X_1, X_2, \\ldots, X_n\\), or calculate the mean \\(\\bar X\\) , then: \\(X_i\\sim N(\\mu_X,\\sigma_X^2)\\) by assumption, since we are selecting from a normally distributed population. \\(X_i\\) independent, as you move with replacement \\(\\bar X = \\frac 1n\\sum_{i=1}^n\\) using the additive property of the normal distribution, \\(\\bar X\\sim N(\\mu_X,\\frac{\\sigma_X^2}n)\\), since \\(\\mathbb E(\\bar n X_i) = \\frac 1n\\cdot n\\cdot \\mu_X\\) and \\(\\text{Var}(\\bar \\frac 1{n^2}\\cdot n\\cdot \\sigma_X^2\\). Or \\(\\frac{\\bar X-\\mu_X}{\\frac{\\sigma_X}{\\sqrt n}}\\sim N(0;1)\\). Example 3.9 (Linear regression under the normal distribution assumption) Standard Single Linear Regression Model: \\[\\begin{equation*} Y_i = \\alpha + \\beta\\, x_i + \\varepsilon_i, \\quad i=1,\\ldots, n. \\end{equation*}\\] These include: \\(Y_1,\\ldots, Y_n\\): observable metric random variable; \\(x_1,\\ldots, x_n\\): given deterministic values (or Realizations of a rv \\(X\\)); \\(\\varepsilon_1, \\ldots, \\varepsilon_n\\): unobservables rv‚Äôs that are independent and identically distributed with \\(\\mathbb E(\\varepsilon_i)=0\\), \\(\\text{Var}(\\varepsilon_i)=\\sigma^2\\). Under the normal distribution assumption \\(\\varepsilon_i\\sim N(0,\\sigma^2)\\quad i=1,\\ldots,n\\) independent of \\(X_i\\sim N(\\mu_X,\\sigma_X^2)\\) (in case \\(X\\) random) applies: \\[\\begin{equation*} Y_i\\sim N(\\alpha+ \\beta x_i,\\sigma^2), \\quad i=1,\\ldots,n. \\end{equation*}\\] and \\[\\begin{align*} \\hat\\beta &amp;= \\frac{\\sum _{i=1}^n (x_i-\\overline x) (Y_i-\\overline Y)} {\\sum _{i=1}^n (x_i-\\overline x) ^2}\\\\ &amp;\\sim N(\\beta, \\sigma_{\\hat\\beta}^2),\\\\ \\hat\\alpha &amp;=\\overline Y-\\hat\\beta \\overline x\\\\ &amp;\\sim N(\\alpha, \\sigma_{\\hat\\alpha}^2). \\end{align*}\\] since these random variables represent a linear combination of independent, normally distributed random variables. Submit Submit Submit Submit \\(B(26,0.0228)\\) \\(B(52;0.0228)\\) \\(N(26,25)\\) \\(N(52,5)\\) \\(N(52,25)\\) Submit 3.2.2 Exponential distribution The exponential distribution is often used to model waiting times under the assumption that the remaining waiting time does not depend on the lifespan that has already elapsed (‚Äúmemorylessness‚Äù). Application examples: Lifespan of products or technical systems Processing time of customer orders Waiting time until the next call arrives in the call center A continuous random variable \\(X\\) is called exponentially distributed with parameter \\(\\lambda&gt;0\\), for short \\(X\\sim E(\\lambda)\\), if it has the density: \\[\\begin{equation*} f(x) = \\lambda\\, \\text e^{-\\lambda x}, \\quad \\text{ for } x\\geq 0, \\end{equation*}\\] and \\(f(x)=0\\) else. The distribution function is \\(F(x)=1-\\text e^{-\\lambda x}\\), for \\(x&gt;0\\). Key figures: \\[\\begin{equation*} \\mathbb E(X) = \\frac{1}{\\lambda} \\quad\\text{ and }\\quad \\text{Var}(X)=\\frac{1}{\\lambda^2}. \\end{equation*}\\] Example 3.10 (Transaction duration in payment service) As part of fraud prevention, you monitor transaction times (i.e.¬†the waiting times until a payment transaction is completed) in seconds. You model the random variable \\(X =\\)‚Äútransaction duration in seconds‚Äù as exponentially distributed. The average transaction duration is \\(5\\) seconds. \\(\\mathbb E(X) = 10\\) \\(\\leadsto\\) \\(\\lambda = \\frac 1{\\mathbb E(X)}=\\frac 1{10}=0,1\\) \\(X\\sim E(0,1)\\) and the probability of transaction times less than 5 seconds: \\[ \\mathbb P(X&lt;5) = F_{E}(5) = 1-\\text{e}^{-0,1\\cdot 5} = 0,3935 \\] Figure 3.10: Distributional models for continuous random variables Transactions with long transaction times are suspicious of fraud and may need to be checked manually. You now want to set the threshold for the duration of the transaction: if the threshold is exceeded, the accuracy of the transaction is checked again manually by an employee. However, a maximum of $10% of transactions should be checked. How do you choose an appropriate threshold? The approach (\\(x_0\\) is the threshold): \\[\\begin{equation*} \\mathbb P(X &gt; x_0) = 1-F_E(x_0)= 1-(1-\\text e^{-0,1 x_0}) = 0.1, \\end{equation*}\\] Solving for \\(x_0\\) gives: \\[\\begin{equation*} x_0 =-\\ln (0.1)/0,1 = 1,0536\\text{ Sekunden}. \\end{equation*}\\] The threshold value should therefore be approximately 1,05 seconds. 3.2.3 Uniform distribution The continuous random variable \\(X\\) is uniformly distributed with the parameters \\(a\\in \\mathbb R\\) and \\(b\\in \\mathbb R\\), in short \\(X\\sim U(a,b)\\), if it has the density \\[\\begin{equation*} f(x)=\\frac{1}{b-a}, \\quad x\\in [a,b], \\end{equation*}\\] (and \\(f(x)=0\\) otherwise). The distribution function is \\(F(x) = \\frac{x-a}{b-a} 1_{[a,b]}(x) +1_{(b,\\infty)}(x)\\). Key figures: \\[\\begin{align*} \\mathbb E(X)&amp;= \\int_a^b x\\cdot \\frac{1}{b-a}\\, d x = \\frac{a+b}{2},\\\\ \\text{Var}(X)&amp;=\\int_a^b x^2\\cdot \\frac{1}{b-a}\\, d x - \\frac{(a+b)^2}{4} = \\frac{(b-a)^2}{12}. \\end{align*}\\] Example 3.11 (Update banner ads) In the last example for advertising banners 3.6 we modeled clicks. Well, advertising banners change and then have to be updated on the SHop website. You‚Äôve created a new banner ad template and want to update your ads. A transport script fetches all advertising updates every \\(20\\) minutes. After the transport script picks up your new template, it is immediately online. Random variable \\(Z: \\text{&#39;Waiting time in minutes until the web display is updated&#39;}\\) follows a continuous uniform distribution on the time interval \\([0,20]\\): \\(Z\\sim U([0,20])\\). Expected value and variance: \\[\\begin{align*} \\mathbb E(Z)&amp;=\\frac{a+b}2=\\frac{20}2=10,\\\\ \\text{Var}(Z)&amp;=\\frac{(b-a)^2}{12} = \\frac{400}{12}=33\\frac13. \\end{align*}\\] Figure 3.11: Distributional models for continuous random variables Submit Submit Submit Submit 3.3 Further distributions 3.3.1 \\(\\chi^2\\)-distribution Some distributions are related to each other (such as Bernoulli and binomial distributions). They come about by transforming a random variable with a specific distribution. The \\(\\chi^2\\) distribution also represents the distribution of the sum of squared independent standard normally distributed random variables. This distribution is important for many estimation and test functions such as bpw. \\(\\hat \\sigma^2\\). Under suitable conditions they have at least an approximate \\(\\chi^2\\) distribution. Let \\(X_1, \\ldots, X_n\\) be independent and identically \\(N(0,1)\\)-distributed random variables. Then the distribution of the random variables is called \\[\\begin{equation*} Z=X_1^2 + \\cdots + X_n^2 \\end{equation*}\\] Chi-square distribution with \\(n\\) degrees of freedom, short \\(\\chi^2(n)\\)-distribution, and \\(Z\\) is called \\(\\chi^2\\)-distributed, short \\(Z\\sim \\chi ^2(n)\\). Parameters: \\[\\begin{equation*} \\mathbb E(Z)=n\\quad\\text{ and }\\quad \\text{Var}(Z)=2n. \\end{equation*}\\] Some \\(\\chi^2\\) distributions: Figure 3.12: Einige Chi-Quadrat-Verteilungen For the estimation we will need quantiles of this distribution. These are calculated using software or taken from the distribution table, where various degrees of freedom can be found in the left column and some probabilities in the top row. Example 3.12 (Stichprobenvarianz von normalverteilten Zufallsvariablen) Example 3.8 cont. If we also estimate the variance from the sample using the sample variance: \\[ S^2=\\frac 1{n-1}\\Big((X_1 - \\bar X)^2 + \\ldots + (X_n - \\bar X)^2 \\Big) \\] Then the random variable \\(\\frac{(n-1)S^2}{\\sigma^2}\\) is \\(\\chi^2\\)-distributed with \\(n-1\\) degrees of freedom. 3.3.2 t-distribution The \\(t\\) distribution is particularly used in parametric tests and confidence intervals. It comes about by dividing a standard normally distributed quantity by the root of a suitably transformed \\(\\chi^2\\)-distributed random variable. Let \\(X\\sim N(0,1)\\) and \\(Z\\sim\\chi^2(n)\\) be independent. Then the distribution of the random variables is called \\[\\begin{equation*} T=\\frac{X}{\\sqrt{Z/n}} \\end{equation*}\\] \\(t\\)-distributed with \\(n\\) degrees of freedom, short \\(T\\sim t(n)\\). Key figures: \\[\\begin{equation*} \\mathbb E(T)=0 \\quad (n&gt;1)\\quad\\text{ and }\\quad \\text{Var}(T)=\\frac{n}{n-2} \\quad (n&gt;2). \\end{equation*}\\] Some \\(t\\) distributions: Figure 3.13: Einige t-Verteilungen The \\(t\\) distribution has more distribution mass in the tails (‚Äúheavy tails‚Äù) compared to the normal distribution. For \\(n\\rightarrow\\infty\\), the density function of the \\(t\\) distribution converges to the density of the standard normal distribution. For the estimation we will need quantiles of this distribution. These are calculated using software or taken from the distribution table, where various degrees of freedom can be found in the left column and some probabilities in the top row. Example 3.13 (Sample mean of normally distributed random variables) Continuation of examples 3.8 and 3.12. In the example above we saw that under the normal distribution assumption: \\(\\frac{\\bar X-\\mu_X}{\\frac{\\sigma_X}{\\sqrt n}}\\sim N(0;1)\\). But if \\(\\sigma_X^2\\) is unknown and is estimated, one replaces the true standard deviation in standardization with the estimate \\(S\\) (\\(S_X=\\sqrt{S_X^2}\\), \\(S_X^2\\) is the sample variance) , then: \\(\\frac{\\bar X-\\mu_X}{\\frac{S_X}{\\sqrt n}}\\sim t(n-1)\\). Calculations\\(^\\ast\\): \\[\\begin{align} \\frac{\\bar X-\\mu_X}{\\frac{S_X}{\\sqrt n}} &amp;= \\frac{\\overbrace{\\frac{\\bar X-\\mu_X}{\\sigma_X/\\sqrt n}}^{N(0,1)}}{\\underbrace{\\sqrt\\frac{(n-1)S^2_X}{\\sigma^2_X}}_{()^2\\sim \\chi^2(n-1)}/\\sqrt{n-1}}\\sim t(n-1). \\end{align}\\] So a standard normally distributed rv \\(\\frac{\\bar /\\sigma_X/(n-1)\\) (divided by the degrees of freedom of the latter) and by definition of the \\(t\\) distribution \\(\\frac{\\bar X-\\mu_X}{\\frac{S_X}{\\sqrt n} So }\\) is \\(t\\)-distributed with \\(n-1\\) degrees of freedom. Example 3.14 (Linear regression under normal distribution assumption) Example 3.9 cont. Since \\(\\sigma^2\\) - the variance of the unobservable error terms \\(\\varepsilon_i\\) is always estimated by \\(\\hat\\sigma^2 = \\frac 1{n-2}\\sum_{i=1}^n\\big (Y_i - (\\hat \\alpha + \\hat \\beta x_i)\\big)^2\\) (\\(n-2\\) comes from the fact that we now estimate two parameters \\(\\alpha\\) and \\(\\beta\\) and \\(2\\) degrees of freedom as a result lose) and The variances of the estimators \\(\\sigma_{\\hat\\alpha}^2\\) and \\(\\sigma_{\\hat\\beta}^2\\) are based on this and must therefore also be determined by the estimates \\(\\hat\\sigma_{\\hat\\alpha }^2\\) and \\(\\hat\\sigma_{\\hat\\beta}^2\\) are replaced, the following applies: \\[ \\frac{\\hat\\beta - \\beta}{\\hat\\sigma_{\\hat\\beta}}\\sim t(n-2)\\,\\ \\frac{\\hat\\alpha - \\alpha}{\\hat\\sigma_{\\hat\\alpha}}\\sim t(n-2). \\] These derived distributions will be useful to us later when we quantify the uncertainty associated with an estimate. "],["limit-theorems.html", "Chapter 4 Limit theorems 4.1 Strong law of large numbers üí™ 4.2 Central limit theorem", " Chapter 4 Limit theorems We‚Äôve already seen that empirical\\(\\not=\\)theoretical, at least not entirely. How then are we supposed to understand anything about the population based on a sample? Well, fortunately there are mathematical guarantees that at least in the limit, i.e.¬†as our sample size gets larger and larger, we get closer and closer to the truth. In this chapter we will learn: What two important limit theorems there are and what implications arise for our analyses. 4.1 Strong law of large numbers üí™ Let \\(X_1,X_2,\\ldots\\) be a sequence of independent, identically distributed random variables with \\(\\mathbb E(X_i)=\\mu\\) and \\(\\text{Var}(X_i)=\\sigma^2&lt;\\infty\\). Then the following applies: \\[\\begin{equation*} \\lim_{n\\rightarrow\\infty} \\frac{1}{n}\\sum _{i=1}^n X_i =\\mathbb E(X)=\\mu\\quad\\text{ almost sure}. \\end{equation*}\\] ‚Äúalmost sure‚Äù is equivalent to ‚Äúwith probability \\(1\\)‚Äù. Example 4.1 (empirical frequencies) Using independent repetitions of a stochastic experiment, estimate probability \\(\\mathbb P(A)\\) for the occurrence of an event \\(A,\\) given the sample \\(x_1, x_2, \\ldots, x_k,\\ldots\\) Let \\(\\displaystyle X_k =\\begin{cases}1,&amp;\\text{ for } x_k\\in A\\\\0 &amp;\\text{ for } x_k\\not\\in A\\end{cases}\\) independent and identically distributed - iid - random variables. \\(\\displaystyle\\frac{1}{n} \\sum_{k=1}^n X_k =h_n(A)=\\) relative frequency of occurrence of \\(A\\) \\(\\mathbb E(X_k)=\\mathbb P(A)\\) for all \\(k\\) According to the law of large numbers: \\[h_n(A) \\rightarrow \\mathbb P(A)\\text{ almost sure}.\\] With the strong Law of Large Numbers (LLN) we have a guarantee that our sample based estimators eventually converge to the population values when \\(n\\rightarrow\\infty\\) (asymptotically). However, as we will see, the values of our estimators (the estimates) depend on the sample. So, what about the distribution of the estimators? The following Central limit theorem helps us to determine the approximative distribution. 4.2 Central limit theorem The Central Limit Theorem (ZGS) states that this sum of independent and identically distributed (iid) random variables is approximately normally distributed. On this basis we can draw statistical conclusions, e.g.¬†about the distribution of the sample mean over all possible sample realizations! Let \\(X_1,X_2, \\ldots\\) be independent identically distributed random variables with \\[\\begin{equation*} \\mathbb E(X_i)=\\mu\\quad\\text{ and }\\quad \\text{Var}(X_i)=\\sigma^2\\text{ with } 0&lt;\\sigma^2&lt;\\infty. \\end{equation*}\\] The following applies to the sum of the random variables: \\[\\begin{equation*} \\color{blue}{\\mathbb E\\left(\\sum _{i=1}^n X_i\\right)}=\\color{blue}{n\\mu}\\quad\\text{ and }\\quad \\color{red}{\\text{Var}\\left(\\sum_{i=1}^n X_i\\right)}=\\color{red}{n\\sigma^2}. \\end{equation*}\\] Then the distribution function \\(F_n(z)=\\mathbb P(Z_n\\leq z)\\) of the standardized sum: \\[ Z_n = \\frac{\\sum_{i=1}^n X_i - \\color{blue}{n\\cdot \\mu}}{\\sqrt{\\color{red}{n\\cdot\\sigma^2}}} %= \\sum_{i=1}^n \\frac{X_i-\\mu}{\\sigma/{\\sqrt{n}}} \\] converges for \\(n\\rightarrow\\infty\\) to \\(N(z)\\) of the standard normal distribution: \\[\\begin{equation*} F_n(z)\\rightarrow N(z) \\quad \\text{ for }n\\rightarrow\\infty. \\end{equation*}\\] That is: \\[Z_n\\stackrel{a}{\\sim} N(0,1).\\] It follows that for sufficiently large \\(n\\) the mean is approximately normally distributed: \\[\\begin{equation*} \\frac{1}{n}\\sum_{i=1}^n X_i\\stackrel{a}{\\sim} N(\\mu,\\sigma^2/n). \\end{equation*}\\] How to compute the mean and the variance for the sample mean Consider a random sample \\(X_1,X_2,\\ldots,X_n\\) of iid random variables with \\(\\mathbb E(X_i)=\\mu\\) and \\(\\text{Var}(X_i)=\\sigma^2\\) for all \\(i=1,\\ldots, n.\\) The sample mean is: \\[\\overline X = \\frac 1n\\sum_{i=1}^nX_i.\\] Using the properties of the expected value: \\[\\begin{align} \\mathbb E(\\overline X) &amp;= \\mathbb E\\left(\\frac 1n\\sum_{i=1}^nX_i\\right) = \\frac 1n\\mathbb E\\left(\\sum_{i=1}^nX_i\\right)\\\\ &amp;= \\frac 1n\\sum_{i=1}^n\\underbrace{\\mathbb E\\left(X_i\\right)}_{=\\mu}=\\frac 1n \\sum_{i=1}^n\\mu\\\\ &amp;= \\frac 1n \\cdot n\\cdot \\mu = \\mu. \\end{align}\\] \\[\\begin{align} \\text{Var}(\\overline X) &amp;= \\text{Var}\\left(\\frac 1n\\sum_{i=1}^nX_i\\right) = \\left(\\frac 1n\\right)^\\color{red}{2}\\cdot\\text{Var}\\left(\\sum_{i=1}^nX_i\\right)\\\\ &amp;\\stackrel{X_i iid}{=} \\frac {1^2}{n^2}\\cdot \\sum_{i=1}^n\\underbrace{\\text{Var}\\left(X_i\\right)}_{=\\sigma^2}=\\frac 1{n^{2}} \\sum_{i=1}^n\\sigma^2\\\\ &amp;= \\frac 1{n^2} \\cdot n\\cdot \\sigma^2 = \\frac{\\sigma^2}{n}. \\end{align}\\] Example 4.2 (Average time in the online shop) In examples 2.6 and 2.7 we have modelled \\(X=\\)‚Äútime in the online shop‚Äù as a continuous random variable with the density: \\[ f(x) = \\begin{cases}0,001\\cdot x , &amp;\\text{ f√ºr } 0\\leq x\\leq 20,\\\\ 0,025 -0,00025\\cdot x, &amp;\\text{ f√ºr } 20&lt; x\\leq 100.\\end{cases} \\] Figure 4.1: Density function of X=time in the online shop and computed: \\[\\begin{align} \\mathbb E(X) &amp;= 40,\\\\ \\text{Var}(X) &amp;= 466,6667. \\end{align}\\] Now we draw a pair of samples of size \\(n\\) from this distribution and in each sample realization we take the average of the values. The ZGS states that (although the individual times are obviously not normally distributed) the distribution of the (standardized) average will get closer and closer to the (standard) normal distribution as \\(n\\) increases. That is, \\(\\bar X=\\frac 1n(X_1 + \\ldots + X_n)\\) mit \\(\\mathbb E(\\bar X) = n\\cdot \\frac 1n\\cdot \\mathbb E(X) = 40\\) und \\(\\text{Var}(\\bar X)=n\\cdot\\frac1{n^2}\\cdot \\text{Var}(X) = \\frac{466,6667}{n}.\\) For \\(n=50\\) holds: \\[ \\bar X\\stackrel{a}{\\sim} N(40;9,3333) \\] and \\(\\mathbb P(35\\leq\\bar X\\leq 45)\\stackrel{\\sim}{=}N(\\frac{45-40}{\\sqrt{9,3333}}) - N(\\frac{35-40}{\\sqrt{9,3333}}) =0,8983\\) and Submit . Submit Submit "],["iii-inductive-statistics.html", "III Inductive Statistics", " III Inductive Statistics In the introductory Statistics course, we analyzed existing data given in a particular sample. Probability modelling helped us to understand how the random sampling process can be described in terms of probabilities and what happens during random sampling. We have seen that the distribution of estimators that are enable conclusions about the characteristics of the population (such as the sample mean \\(\\bar X\\) \\(\\leadsto\\) \\(\\mathbb E(X)\\)) over all possible sample realizations can be approximated by the normal distribution based on a CLT. Now let‚Äôs move on to our key question: ‚ÄúHow and under what conditions can we draw valid conclusions about the population based on estimates obtained from a single sample?‚Äù. "],["parameter-estimation.html", "Chapter 5 Parameter estimation 5.1 Basic ideas of inductive statistics 5.2 Point estimation 5.3 Construction of estimators 5.4 Interval estimation", " Chapter 5 Parameter estimation To answer the question about inferences about the population regarding parameter estimation, let‚Äôs take a close look at distributions of estimators like \\(\\bar X\\) to find some criteria of ‚Äúgood‚Äù estimators to define and quantify the uncertainty of the estimate. In this chapter we will learn: What properties should we require of the estimators we use, which point estimators are suitable for which parameters and how to construct tailor-made estimation functions or how to take the uncertainty of the estimate into account in the estimation procedure \\(\\leadsto\\) interval estimation. 5.1 Basic ideas of inductive statistics In inductive statistics, we want to generalize our findings, which are based on a single sample, on the whole population of interest. With the help of random sampling and appropriate estimators, we conduct inference for parameters of interest in the population. Exercise 5.1 (Basic Concepts) 5.1.1 Parameter estimation theoretically Before we conduct any data analysis, we think of a model for a real-world problem. Example 5.1 (New customers) You want to expand your customer base to a specific population group and are interested in: to what proportion will the new customers use the app (\\(Y\\)), specifically: what would be the proportion of app users in your new customer base (\\(p\\)), what average sales/invoice amount can you expect from new customers (\\(X\\)), specifically: what would be the average invoice amount of your new customers (\\(\\mathbb E(X)=\\mathbb \\mu_X\\)). \\(X\\) and \\(Y\\) are random variables. We are interested in the distribution parameters of these random variables. We will build upon the following concept of a random sample‚Ä¶ Modeling the quantities of interest App usage is the property of interest, so: \\[ Y=\\begin{cases}1,&amp;\\text{if app usage},\\\\0,&amp;\\text{no app usage}.\\end{cases} \\] \\(Y\\sim Ber(p)\\). Of interest is the share of app users (\\(p\\)). Invoice amount is size \\(X\\) and the average invoice amount is of interest: \\(\\mathbb E(X)=\\mu_x\\). Random sample We have often talked about a sample and have seen how sampling can be modeled (See ‚ÄúRandom events and Probability‚Äù). Now we formally define what we mean by a random sample as a random process. Let \\(X_1,\\ldots, X_n\\) be random variables. \\((X_1,\\ldots, X_n)\\) is called a random sample of size \\(n\\) (sample variables) to \\(X\\) if the distributions of \\(X\\) and \\(X_i\\), \\(i=1,2,\\ldots, n\\), match and \\(X_1,\\ldots, X_n\\) are independent. The observed realization \\((x_1,\\ldots, x_n)\\) of random sample \\((X_1,\\ldots, X_n)\\) is called a sample realization. The set \\(\\mathcal X\\) of all possible sample realizations is called the sample space: \\(\\mathcal X\\subseteq\\mathbb R^n\\). Example 5.2 (New customers) Example 5.1 cont. The plan is to randomly select \\(n\\) customers and survey them. App Usage: The sample variables for \\(Y\\) are: \\(\\color{blue}{Y_1, Y_2,\\ldots,Y_n},\\) where \\(Y_i\\) takes the value 1 or 0, depending on whether the \\(i\\)th selected person uses the app or not. So the value of \\(Y_i\\) is initially unknown because we don‚Äôt know which person will be interviewed in the \\(i\\)th position. \\(\\leadsto\\) \\(Y_1,\\ldots, Y_n\\) are random variables! Invoice amount: The sample variables for \\(X\\) are: \\({\\color{red}{X_1, X_2,\\ldots,X_n}},\\) where \\(X_i\\) represents the estimate of the bill amount of the \\(i\\)th selected person. So the value of \\(X_i\\) is initially unknown because we don‚Äôt know which person will be interviewed in the \\(i\\)th position. \\(\\leadsto\\) \\(X_1,\\ldots, X_n\\) are random variables! Estimation In descriptive statistics we have used the formulas for computing the empirical mean and the empirical frequencies in order to summarize the sample given. Now we look at those formulas from the probabilistic point of view, realize that there are many possible samples from a population and the formulas (which correspond to estimators) return different values depending on a particular sample realization. Example 5.3 (New customers) Example 5.2 cont. Calculation formulas (estimators) for the parameters of interest: the app user share \\(p\\): \\[\\begin{align} \\color{blue}{\\overline{Y}}&amp;\\color{blue}{=\\frac 1n (Y_1 + Y_2 + \\ldots Y_n)} \\leadsto\\color{blue}{\\frac{\\text{Number of 1s (=app usage)}}{\\text{Number of values}}}\\\\ &amp;\\color{blue}{= \\text{Sample frequency of ones (=app usage)}} \\end{align}\\] the average invoice amount (\\(\\mathbb E(X)\\)): \\[\\color{red}{\\overline{X}=\\frac 1n (X_1 + X_2 + \\ldots X_n) \\leadsto\\text{sample mean}}.\\] \\(\\overline{Y}\\) and \\(\\overline{X}\\) are functions of the sample variables, so they are themselves random variables. 5.1.2 Parameter estimation practically Let us return to our estimation problem. Assume the proportion of app users is \\(0.6\\) and the mean invoice amount in the population of all potential new customers is \\(25\\) Euro. In the reality we do not know those values and we have to estimate them. So imagine, we conduct a survey among the potential new customers and ask the survey participants to indicate whether they would use the app or not, and how high would be their willingness to pay. Example 5.4 (New customers) Example 5.3 cont. We need to set the sample size (\\(n\\)) and then conduct a survey to get concrete sample values (\\(x_i\\) and \\(y_i\\) for \\(i=1,\\ldots,n\\)). In the animation you can see what happens when a sample is taken: Sample size \\(n\\): 10 20 50 100 Sample realizations: The survey results for App usage (yes/no) as realized sample values: \\(\\color{blue}{\\{1,2,\\ldots, n\\}=} ~\\) As realizations of the corresponding 0-1 Bernoulli random variable: \\(\\color{blue}{\\{y_1,y_2,\\ldots,y_n\\}=}~\\) Willingness to pay in form of self-estimated invoice amount with sample values: \\(\\color{red}{\\{x_1,x_2,\\ldots,x_n\\}=}~\\) Calculation of the resulting estimates: The estimate for the proportion of app users: \\(\\color{blue}{\\bar y=\\frac 1n(y_1+y_2 + \\ldots + y_n)=}\\) The estimate for the average invoice amount: \\(\\color{red}{\\bar x=\\frac 1n(x_1+x_2 + \\ldots + x_n)= }\\) draw a new sample The estimates change the sample. The estimated values change from sample to sample. The estimation result deviates from the true parameter. The estimation result perfectly agrees with the population parameter to be estimated. Submit Nothing changes. Precision increases as the sample size increases. The estimates vary more from sample to sample. The estimates vary less from sample to sample. Submit Submit Submit Submit 5.2 Point estimation The aim of a point estimate is to provide the most accurate possible approximation value for an unknown population parameter. Parameters come in two forms: Characteristics of an arbitrary, unknown distribution (\\(\\leadsto \\mathbb E(\\cdot), \\text{Var}(\\cdot), \\rho, q_p,\\ldots\\)) Parameters of an assumed distribution model (\\(\\leadsto p \\text{ of the Bernoulli distribution }, \\lambda \\text{ of the Poisson distribution },\\ldots\\)). For both types of parameters there exist estimation functions or estimators (formulas from the descriptive statistics) that provide a point estimate for a given sample. In the case of a distribution model, such estimators can be constructed. 5.2.1 Estimation functions Assume that a parameter \\(\\theta\\) is to be deduced from the realizations in the sample (e.g.¬†distribution parameter). A function: \\[\\begin{equation*} T=g(X_1,\\ldots, X_n) \\end{equation*}\\] of the random sample \\((X_1,\\ldots, X_n)\\) an estimation function for \\(\\theta\\). As a function of random variables, \\(T\\) is itself a random variable. There two types of estimation functions, that we will be talking about: estimator if it is an estimator of a distribution parameter; test statistics in case of hypotheses testing. Example 5.5 (Examples of estimators) We already know the following estimation functions quite well: \\(\\displaystyle\\overline X = \\frac{1}{n} \\sum_{i=1}^n\\) \\(\\displaystyle \\overline X = \\frac{1}{n} \\sum_{i=1}^n\\) Probability of occurrence \\(\\mathbb P(X=1)\\). \\(\\hat\\sigma_X^2=\\displaystyle S_X^2 = g(X_1,\\ldots, X_n) = \\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\overline X )^2\\) is estimator for the variance \\(\\sigma_X^2=\\text{Var}(X)\\). A point estimate for \\(\\theta\\) is a function of realizations of the form \\[t=g(x_1,\\ldots, x_n).\\] The resulting numerical value \\(g(x_1,\\ldots, x_n)\\) is called estimate. For example, \\(g(x_1,\\ldots, x_n)=\\displaystyle \\frac{1}{n}\\sum_i x_i\\) is an estimate for expected value \\(\\theta=\\mathbb E X\\). 5.2.2 Properties of estimators What characterizes a ‚Äúgood‚Äù estimator? For example, which properties justify the estimator \\(\\overline X\\) as an estimator of the expected value? The following properties provide clear criteria for the quality of an estimator: Unbiasedness Consistency Efficiency 5.2.2.1 Unbiasedness An estimate should tend to deliver the correct value, i.e.¬†neither systematically overestimate nor underestimate. An estimator \\(T=g(X_1,\\ldots, X_n)\\) for the parameter \\(\\theta\\) is called unbiased if holds \\[\\begin{equation*} \\mathbb E(T) = \\theta. \\end{equation*}\\] The bias refers to the systematic overestimation or underestimation of an estimator: \\[\\begin{equation*} \\text{Bias}(T) = \\mathbb E(T) - \\theta. \\end{equation*}\\] Example 5.6 (Unbiasedness of the sample mean) We check whether the sample mean \\(\\bar X\\) is unbiased for \\(\\mathbb E(X)\\) and the relative frequency (\\(h_{Y=1}=\\bar Y\\), \\(Y\\in\\{0;1\\}\\) ) are unbiased for \\(\\mathbb P(Y=1)\\). \\(\\overline X=\\displaystyle \\frac{1}{n} \\sum_{i=1}^n\\) because \\[\\begin{equation*} \\mathbb E(\\overline X) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb E(X_i) = \\frac{1}{n} n\\cdot \\mu = \\mu. \\end{equation*}\\] The relative frequency \\(H_{Y=1}=\\displaystyle \\overline Y=\\displaystyle\\frac{1}{n}\\sum_{i=1}^n Y_i\\), with \\(Y_i\\in \\{0,1\\}\\), is unbiased estimator for the share value/ the probability of occurrence \\(\\mathbb P(Y=1)\\). \\[\\begin{equation*} \\mathbb E(\\overline Y) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb E(Y_i) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb P(Y_i=1)\\frac{1}{n} n\\cdot \\mathbb P(Y=1) = \\mathbb P(Y=1). \\end{equation*}\\] If we now take several random samples (e.g.¬†\\(100\\)) and calculate the sample mean for each drawing with a certain size \\(n\\), then we observe the following: Example 5.7 (Sample variance and mean square deviation) What about the variance estimators? Sample variance: \\[\\begin{equation*} S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\overline X)^2 \\end{equation*}\\] is an unbiased estimator for the variance \\(\\sigma^2=\\text{Var}(X)\\), i.e.¬†\\(\\mathbb E(S^2) = \\sigma^2\\). For the mean square deviation \\(\\displaystyle \\tilde S^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i-\\overline X)^2\\) applies: \\[\\begin{equation*} \\mathbb E(\\tilde S^2) = \\frac{n-1}{n} \\sigma^2. \\end{equation*}\\] So: \\(\\tilde S^2\\) is biased (it is asymptotically unbiased since the bias disappears for increasing \\(n\\)). The distortion is \\[\\begin{equation*} \\text{Bias}(\\tilde S^2) = \\mathbb E(\\tilde S^2) - \\sigma^2 = -\\frac{1}{n}\\sigma^2. \\end{equation*}\\] Proof: Mean square deviation is biased üëΩ The following applies: \\(\\displaystyle \\mathbb E((X_i-\\mu)(X_j-\\mu))=0\\) if \\(i\\not=j\\). Furthermore: \\[\\begin{equation*} \\mathbb E((\\overline X-\\mu)^2) = \\text{Var}(\\overline X) = \\frac{1}{n^2} \\text{Var}(X_1+\\cdots+X_n) = \\frac{1}{n}\\sigma^2. \\end{equation*}\\] This results in: \\[\\begin{align*} \\mathbb E((X_i-\\overline X)^2) &amp;= \\mathbb E(((X_i-\\mu) - (\\overline X-\\mu))^2) \\\\ &amp;= \\sigma^2 -2 \\mathbb E\\left((X_i-\\mu) \\left(\\frac{1}{n} \\sum_{j=1}^n X_j-\\mu\\right)\\right) + \\frac{\\sigma^2}{n} \\\\ &amp;= \\sigma^2 - \\frac{2}{n} \\sum_{i=1}^n \\mathbb E((X_i-\\mu)(X_j-\\mu)) + \\frac{\\sigma^2}{n} \\\\ &amp;= \\sigma^2 - \\frac{2\\sigma^2}{n} + \\frac{\\sigma^2}{n} = \\sigma^2 - \\frac{\\sigma^2}{n}. \\end{align*}\\] If we now carry out several samplings (e.g.¬†\\(100\\)) and calculate the sample variance and the mean square deviation for each drawing with a certain size \\(n\\), then we observe the following: 5.2.2.2 Consistency Another important property of an estimator is consistency, which states that the estimator converges to the quantity being estimated. This definition takes into account the variance and the bias of the estimator; the two sizes should approach zero as the sample size \\(n\\) increases. Formally, consistency is defined by the convergence of the expected mean square deviation. The expected mean square error is determined by \\[\\begin{equation*} \\text{MSE}(T)= \\mathbb E((T-\\theta)^2). \\end{equation*}\\] It can be expressed as \\[\\begin{equation*} \\text{MSE}(T) = \\text{Var}(T) + \\text{Bias}(T)^2. \\end{equation*}\\] An estimator is said to be consistent if: \\[\\begin{equation*} \\text{MSE} \\rightarrow 0\\text{ for } n\\rightarrow\\infty. \\end{equation*}\\] Example 5.8 (Consistency of the sample mean) From independent, identically distributed observations \\(X_1,\\ldots, X_n\\) with \\(\\mathbb E(X_1)=\\mu\\) and \\(\\text{Var}(X_1)=\\sigma^2\\) the expected value is estimated by \\(\\overline X=\\displaystyle\\frac{1}{n} \\sum_{i=1}^n X_i\\). \\(\\mathbb E(\\overline X)=\\mu\\) (See unbiasedness of \\(\\bar X\\) \\(\\leadsto \\text{Bias}(\\overline X)=0\\)) \\(\\text{MSE}(\\overline X) = \\text{Var}(\\overline X) + \\text{Bias}^2(\\overline X)=\\frac{\\sigma^2}{n} + 0^2 {\\rightarrow}0\\) for \\({n\\rightarrow\\infty}\\) (\\(\\leadsto\\) consistency) 5.2.2.3 Efficiency An unbiased estimator for \\(\\theta\\) is more efficient than another unbiased estimator if it has the smaller variance. Efficiency means that of two unbiased estimators \\(T_1\\) and \\(T_2\\) for \\(\\theta\\), one (here \\(T_1\\)) is more efficient if: \\[\\begin{equation*} \\text{Var}(T_1)\\leq \\text{Var}(T_2) \\end{equation*}\\] The most efficient or best estimator is the unbiased estimator that has the smallest variance. \\(\\overline X\\) is efficient for the expected value; \\(\\overline X\\), with \\(X_i\\in\\{0,1\\}\\) is efficient for \\(\\mathbb P(X=1)\\); \\(\\overline X\\) is efficient for the parameter \\(\\lambda\\) of the Poisson distribution. 5.3 Construction of estimators We already have good estimators for the most important parameters of distributions (expectation and variance). But what do we do if we want to estimate unknown parameters of a distribution model? It‚Äôs not always possible to ‚Äúgoogle‚Äù a suitable estimator. There are two useful methods to construct the estimators for the unknown parameters of a distribution model. 5.3.1 Method of moments Basic idea: Estimate parameters of the distribution such that the corresponding moments of the distribution \\(\\mathbb E(X), \\mathbb E(X^2), \\ldots\\) match with the corresponding empirical moments of the sample \\(\\bar X, \\overline{X^2}, \\ldots\\): \\[ \\begin{array}{lp{.5cm}l} \\hline \\text{Distribution} &amp; &amp; \\text{Sample}\\\\\\hline \\mathbb E(X)=\\mu &amp;&amp; \\displaystyle \\overline x=\\frac{1}{n}\\sum_{i=1}^n x_i\\\\ \\mathbb E(X^2) = \\mu^2 + \\sigma^2 &amp; &amp; \\displaystyle \\overline{x^2}=\\frac{1}{n}\\sum_{i=1}^n x_i^2\\\\ \\vdots &amp; &amp; \\vdots\\\\ \\mathbb E(X^m) &amp; &amp; \\displaystyle \\overline{x^m}=\\frac{1}{n} \\sum_{i=1}^n x_i^m\\\\\\hline \\end{array} \\] Starting with the first moment, just enough moments are included so that all parameters to be estimated can be clearly estimated. Example 5.9 (Bernoulli distribution) Estimation of the parameter \\(p\\) of a Bernoulli distribution. Theoretically: Distribution assumption: \\(\\mathbb P\\in\\{B(1,p) | p\\in \\Theta=[0,1]\\}\\) First moment of the distribution: \\(\\mathbb E(X) = p\\) First moment of the sample: \\(\\bar X = \\frac 1n \\sum_{i=1}^n X_i\\) Estimator: \\(\\hat p = \\overline X\\) (proportion of ‚Äúsuccesses‚Äù in the sample) Practical: Estimation of the app user share based on a customer survey. From the sample: \\(\\bar x = 0.55\\) Estimate: \\(\\hat p=0.55\\) Example 5.10 (Exponential distribution) Estimation of the parameter \\(\\lambda\\) of an exponential distribution. Theoretically: Distribution assumption: \\(\\mathbb P\\in\\{E(\\lambda)| \\lambda&gt;0\\}\\) First moment of the distribution: \\(\\mathbb E(X) = \\displaystyle\\frac{1}{\\lambda}\\) First moment of the sample: \\(\\bar X = \\frac 1n \\sum_{i=1}^n X_i\\) Estimator: \\(\\hat\\lambda = \\displaystyle\\frac{1}{\\overline X}\\) (because then: \\(\\overline X = \\displaystyle \\mathbb E(X) = \\frac{1}{\\lambda}\\) ) Practical: estimate the \\(\\lambda\\) for the transaction duration. From the sample: \\(\\bar x = 9.5\\) Estimate: \\(\\hat \\lambda=\\frac1{9.5}=0.1053.\\) Example 5.11 (Normal distribution) Estimation of the parameters \\((\\mu,\\sigma^2)\\) of a normal distribution. Theoretically: Distribution assumption: \\(\\mathbb P\\in\\{N(\\mu,\\sigma^2)|\\mu\\in \\mathbb R, \\sigma^2&gt;0\\}\\) First moment of the distribution: \\(\\mathbb E(X) = \\mu\\) First moment of the sample: \\(\\bar X = \\frac 1n \\sum_{i=1}^n X_i\\) Second moment of the distribution: \\(\\mathbb E(X^2) = \\mu^2 + \\sigma^2\\) Second moment of the sample: \\(\\overline{X^2} = \\frac 1n \\sum_{i=1}^n X^2_i\\) Inserting the empirical moments gives: \\(\\hat\\mu=\\overline X\\) and \\(\\hat\\sigma^2 = \\tilde S^2\\) Practical: distribution parameters for the average temperature in July. From the sample: \\(\\bar t = 20.5\\); \\(\\tilde s_T^2 = 12\\) Estimated value: \\(\\hat \\mu_T=20.5\\); \\(\\hat\\sigma_T^2=12\\). Exercise 5.2 (The method of moments) 5.3.2 Maximum likelihood method Let \\(X_1,\\ldots, X_n\\) be independent, identically distributed random variables. The distribution of \\(X_i\\), \\(i=1,\\ldots, n\\) is unknown, but we assume that the distribution belongs to a distribution model that characterizes \\(\\theta\\) by an appropriate parameterization is. That is, \\(X_i\\) has probability function or density \\(f(x|\\theta)\\), \\(\\theta\\in\\Theta\\), where \\(\\theta\\) is a vector of parameters from the set \\(\\Theta\\). The maximum likelihood principle states: For the sample result \\((x_1,\\ldots, x_n)\\), choose the value \\(\\hat\\theta\\) as the estimated value for \\(\\theta\\), below which the probability of the sample result occurring is greatest. The sentence ‚Äúunder which the probability of the outcome occurring is greatest‚Äù can be replaced by: ‚Äúunder which the corresponding probability function or density is maximum‚Äù. The latter is quantified using the likelihood function: If a realization \\((x_1,\\ldots, x_n)\\) is given, the density can be determined as a function of the unknown parameters \\(\\theta\\) comprehend \\[\\begin{equation*} L(\\theta)= f(x_1,\\ldots, x_n|\\theta). \\end{equation*}\\] Because of the independence of the random sample, the following applies to the joint probability function / density: \\[\\begin{equation*} L(\\theta)=f(x_1,\\ldots, x_n|\\theta) = f(x_1|\\theta)\\cdot f(x_2|\\theta)\\cdot \\dots \\cdot f(x_n|\\theta). \\end{equation*}\\] The Maximum Likelihood Principle says: for \\(x_1,\\ldots, x_n\\), choose as parameter estimate the parameter \\(\\hat\\theta\\) for which the likelihood is maximum, i.e. \\[\\begin{equation*} L(\\hat\\theta)=\\max_{\\theta} L(\\theta), \\end{equation*}\\] or \\[\\begin{equation*} f(x_1,\\ldots, x_n|\\hat\\theta) = \\max_{\\theta} f(x_1,\\ldots, x_n|\\theta). \\end{equation*}\\] For the given realizations \\(x_1,\\ldots, x_n\\), one chooses the parameter \\(\\hat\\theta\\) for which the probability or density that these values ‚Äã‚Äãoccur is maximum. These are the parameters \\(\\theta\\) that provide the most plausible explanation for how these values ‚Äã‚Äãcame about. The estimate depends in particular on the sample: \\(\\hat\\theta=\\hat\\theta(x_1,\\ldots, x_n)\\). The estimating function \\(g(x_1,\\ldots, x_n)=\\hat\\theta(x_1,\\ldots, x_n)\\) is called maximum likelihood estimator (ML estimator). To practically determine the maximum, one often chooses the logarithm of the likelihood function, the log-likelihood, as this converts the product of the densities into a sum: \\[\\begin{equation*} \\mathcal L(\\theta) = \\ln L(\\theta) = \\sum_{i=1}^n \\ln f(x_i|\\theta). \\end{equation*}\\] Because of the strict monotonicity of the logarithm, maximizing \\(L(\\theta)\\) and \\(\\ln L(\\theta)\\) yield the same value \\(\\hat\\theta\\). Example 5.12 (ML: Bernoulli distribution) Randomly select \\(n=4\\) customers from a customer pool with \\(a\\) app users and \\(w\\) web users (with replacement); \\(p=\\displaystyle\\frac{a}{a+w}\\) unknown. Construction: So: \\(4\\) independent repetitions of the Bernoulli random process. The sample is \\(x_1=1\\), \\(x_2=1\\), \\(x_3=0\\), \\(x_4=1\\): Likelihood function: \\[\\begin{equation*} L(p) = f(x_1|p) \\cdots f(x_4|p) = \\prod_{i=1}^4 p^{x_i}(1-p)^{1-x_i} = p\\cdot p\\cdot (1-p)\\cdot p = p^3\\cdot (1-p). \\end{equation*}\\] Deriving and setting to zero results in: \\[\\begin{equation*} L&#39;(p) = 3p^2 (1-p) - p^3 = (3-4p)p^2 = 0 \\iff p\\in \\left\\{0,\\frac{3}{4}\\right\\}. \\end{equation*}\\] Maximum at \\(p^\\ast=3/4\\) (negative second derivative). Representation of the likelihood function: Interpretation of the likelihood function: \\(L(p)\\) is the probability of the observations \\(x_1=1,x_2=1, x_3=0\\) and \\(x_4=1\\) occurring together under the assumption that \\(\\mathbb P(X =1)=p\\). Example 5.13 (ML: Exponential distribution) Let \\(X_1,\\ldots, X_4\\) be independent repetitions of an exponentially distributed quantity with estimated value \\(\\lambda\\). Construction: Sample: \\(x_1=0.95\\), \\(x_2=1.81\\), \\(x_3=1\\), \\(x_4=0.67\\) Likelihood function: \\[\\begin{equation*} L(\\lambda) = f(x_1|\\lambda)\\cdots f(x_4|\\lambda) = \\prod_{i=1}^4 \\lambda \\text e^{-\\lambda x_i} = \\lambda^4 \\text e^{-\\lambda (x_1+\\cdots + x_4)} = \\lambda^4 \\text e^{-4.43\\lambda} \\end{equation*}\\] Log likelihood function: \\(\\displaystyle \\mathcal{L}(\\lambda) = \\ln L(\\lambda) = -4.43\\lambda + 4 \\ln(\\lambda)\\) Derive and zero: \\[\\begin{equation*} \\mathcal{L}^\\prime(\\lambda) = -4.43 + \\frac{4}{\\lambda} = 0 \\quad \\iff \\quad \\lambda=4/4.43 = 0.9029 \\end{equation*}\\] Second derivative: \\(\\displaystyle -\\frac{4}{\\lambda^2}&lt;0\\), i.e.¬†maximum. \\(\\hat\\lambda=1/\\overline X = 1/1.1075=0.902935\\). Representation of the likelihood function and log-likelihood function: Interpretation of the likelihood function: \\(L(\\lambda)\\) is the density for the co-occurrence of the observations \\(x_1=0.95\\), \\(x_2=1.81\\), \\(x_3=1\\) and \\(x_4=0, 67\\) under the assumption that there is an exponential distribution with parameter \\(\\lambda\\). A high density value \\(f(x)\\) at location \\(x\\) implies a high probability of occurrence in a small interval around \\(x\\). Exercise 5.3 (The maximum likelihood method) 5.4 Interval estimation The point estimate provides a parameter estimate \\(\\hat\\theta\\), which is typically not the same as the true value \\(\\theta\\). In addition to the estimate \\(\\hat\\theta\\) itself, one can specify the precision of the estimator by taking into account the variability of the estimator. The Interval Estimation is a way to directly incorporate the accuracy of the estimation process. The result of the estimation is an interval. However, it is not really possible to hit the true parameter with certainty. The error probability \\(\\alpha\\), with which the method returns an interval that does not contain the true value \\(\\theta\\), is given. Common values \\(\\alpha\\) are \\(\\alpha=0.1\\), \\(\\alpha=0.05\\) and \\(\\alpha=0.01\\). The coverage probability/confidence level \\(1-\\alpha\\) is the probability that the procedure returns an interval that contains the value \\(\\theta\\). Since the width of such an interval can also be understood as the accuracy of the estimate, the sample size, which ensures a certain level of accuracy at the given level of certainty, can be derived from it. *** Example 5.14 (Estimate the average invoice amount (Rechnungsbetrag)) Our estimator - the sample mean \\(\\bar{X}=\\frac 1n \\sum_{i=1}^n\\). The values fluctuate less as the sample size \\(n\\) increases. But how exactly is the relationship to the sample size? The answer is provided by the central limit theorem (CLT). If the sample variables \\(X_1,X_2,\\ldots, X_n\\) are identical and independently distributed with a finite mean \\(\\mu_X\\) and a finite variance \\(\\sigma_X^2\\), the approximate distribution of sample mean is: \\[\\bar{X} \\stackrel{a}{\\sim} N(\\mu_X, \\frac{\\sigma_X^2}{n})\\] or in its standardized version: \\[\\frac{\\bar X-\\mu_X}{\\sigma_X/\\sqrt{n}}\\stackrel{a}{\\sim}N(0;1).\\] The sample size depends on the unknown variance in the population. In practice, an estimate of the variance is used. You should assume a rather higher value. Assuming \\(\\sigma_X=100\\) is known or determined by experience and \\(n=50\\), therefore \\(\\frac{\\sigma_X^2}{n}=\\frac{100}{50}=2\\), then according to CLT : \\[\\bar{X}\\stackrel{a}{\\sim} N(\\mu_X; 2)\\text{ or }\\frac{\\bar X-\\mu_X}{\\sqrt 2}\\stackrel{a}{\\sim} N(0;1)\\] I.e. \\(\\mathbb P\\left(-1.96\\leq\\frac{\\overline X-\\mu_X}{\\sqrt 2}\\leq1.96\\right)=0.95\\leftrightarrow\\) \\(\\mathbb P\\left(-1 .96\\cdot \\sqrt 2\\leq \\overline X-\\mu_X\\leq1.96\\cdot \\sqrt 2\\right)=0.95\\) and with 95% probability the values of \\(\\bar x\\) away from \\(\\mu_X\\). Rearranging the inequalities within the probability according to \\(\\mu_X\\) gives: \\[\\begin{align} \\mathbb P\\left(-1.96\\cdot \\sqrt 2\\leq \\bar X-\\mu_X\\leq 1.96\\cdot \\sqrt 2\\right)&amp;=0.95~~ |\\leadsto -\\bar X\\\\ \\mathbb P\\left(-\\bar X-1.96\\cdot \\sqrt 2\\leq-\\mu_X\\leq -\\bar X+1.96\\cdot \\sqrt2\\right)&amp;=0.95~~|\\leadsto :(-1)\\\\ \\mathbb P\\left(\\bar X+1.96\\cdot \\sqrt 2\\geq\\mu_X\\geq \\bar X-1.96\\cdot \\sqrt 2\\right)&amp;=0.95~~|\\leadsto \\text{swipping the sides}\\\\ \\mathbb P\\left(\\bar X-1.96\\cdot \\sqrt 2\\leq\\mu_X\\leq \\bar X+1.96\\cdot \\sqrt 2\\right)&amp;=0.95 \\end{align}\\] With 95% probability, the above term guarantees us that the unknown parameter \\(\\mu_X\\) is somewhere between \\(\\bar X-1.96\\cdot \\sqrt 2\\) and \\(\\bar X+1.96\\cdot \\sqrt 2\\). Now \\(\\bar X-1.96\\cdot \\sqrt 2\\) and \\(\\bar X+1.96\\cdot \\sqrt 2\\) are random variables and the values vary with each new sampling. For a concrete sampling, you can calculate the values \\(\\overline x-1.96\\cdot \\sqrt 2\\) and \\(\\overline x+1.96\\cdot \\sqrt 2\\) and get an estimation interval for \\(\\mu_X\\) as a result: \\[ [\\overline x-1,96\\cdot \\sqrt 2; \\overline x+1,96\\cdot \\sqrt 2]. \\] or for a general sample size (\\(\\text{Var}(\\overline X) = \\sigma^2/n\\) \\(\\leadsto \\sqrt{\\text{Var}(\\overline X)} = \\sigma/\\sqrt n\\)): \\[ \\left[\\overline x-1,96\\cdot \\frac{10}{\\sqrt n}; \\overline x+1,96\\cdot \\frac{10}{\\sqrt n}\\right]. \\] Sample size \\(n\\): 10 20 50 100 Sample realizations: Willingness to pay in form of self-estimated invoice amount with sample values: \\(\\color{red}{\\{x_1,x_2,\\ldots,x_n\\}=}~\\) Calculation of the resulting estimates: The estimate for the average invoice amount: \\(\\color{red}{\\bar x=\\frac 1n(x_1+x_2 + \\ldots + x_n)= }\\) The interval estimate for the average invoice amount: \\(\\color{blue}{\\left[\\overline x-1,96\\cdot \\frac{10}{\\sqrt n}; \\overline x+1,96\\cdot \\frac{10}{\\sqrt n}\\right]}=[\\)\\(\\color{blue}{]}\\) draw a new sample Repeated sampling and interval calculation for the given coverage probability \\(1‚àí\\alpha\\) gives on average approximately a \\((1‚àí\\alpha)\\) proportion of the interval estimates that cover the true parameter. For our example 5.14 we want to estimate \\(\\mu_X\\) using a sample of size \\(n=50\\) and assuming \\(\\sigma_X=10.\\) Then, a collection of \\(100\\) repeated interval estimates for \\(\\alpha=0.05\\) could look like this: 5.4.1 \\((1-\\alpha)\\) confidence interval for \\(\\mu\\) Type A: Confidence interval for \\(\\mu\\) with known variance \\(\\sigma^2\\) (\\(N(\\mu,\\sigma^2)\\)-distributed random variables) The starting point is the point estimator \\(\\overline X\\), which is \\(N(\\mu,\\sigma^2/n)\\)-distributed. The standardized sample mean \\(\\displaystyle\\frac{\\overline X-\\mu}{\\sigma/\\sqrt{n}}\\) is \\(N(0,1)\\)-distributed; therefore: \\[\\begin{equation*} \\mathbb P\\left(-N_{1-\\alpha/2} \\leq \\frac{\\overline X-\\mu}{\\sigma/\\sqrt{n}}\\leq N_{1-\\alpha/2}\\right)=1-\\alpha, \\end{equation*}\\] with \\(N_{1-\\alpha/2}\\) the \\(1-\\alpha/2\\) quantile of the standard normal distribution. Forming results in: \\[\\begin{equation*} 1-\\alpha = \\mathbb P\\left(\\overline X - N_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\overline X + N_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right), \\end{equation*}\\] and a \\((1-\\alpha)\\) confidence interval is given by \\[\\begin{equation*} \\left[\\overline X - N_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\overline X + N_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right]. \\end{equation*}\\] Example 5.15 (Monthly returns) Given: monthly returns of a stock \\(A\\) random monthly return \\(X\\sim N(\\mu,\\sigma^2)\\) with \\(\\sigma^2 = 0.25\\) Data: \\(12\\) monthly returns in \\(\\%\\) \\[\\begin{array}[t]{|*{4}{cc|}} \\hline n &amp; x_n &amp; n &amp; x_n &amp; n &amp; x_n &amp; n &amp; x_n \\\\\\hline 1 &amp; 0.35 &amp; 4 &amp; \\phantom{-}0.52 &amp; 7 &amp; -0.48 &amp; 10 &amp; 0.29\\\\ 2 &amp; 0.89 &amp; 5 &amp; \\phantom{-}0.12 &amp; 8 &amp; -0.37 &amp; 11 &amp; 0.41\\\\ 3 &amp; 1.02 &amp; 6 &amp; -0.22 &amp; 9 &amp; \\phantom{-}0.07 &amp; 12 &amp; 0.72\\\\\\hline \\end{array}\\] Error probability \\(\\alpha=5\\%\\) \\(\\Rightarrow\\) Confidence level \\(1-\\alpha=0.95\\) Sample mean: \\(\\displaystyle \\overline x=\\frac{3.32}{12}=0.2767\\%\\) \\((1-\\alpha/2)\\)-quantile \\(N_{1-\\alpha/2} = N_{0.975} =1.96\\) Realized confidence interval: \\[\\begin{align*} &amp;\\left[\\overline x - N_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\overline x + N_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right] = \\\\ &amp;\\left[0.2767-1.96\\cdot \\frac{\\sqrt{0.25}}{\\sqrt{12}}; 0.2767+ 1.96\\cdot\\frac{\\sqrt{0.25}} {\\sqrt{12}}\\right] = [-0.0062\\%; 0.5596\\%] \\end{align*}\\] Type B: Confidence interval for \\(\\mu\\) with unknown variance \\(\\sigma^2\\) (\\(N(\\mu,\\sigma^2)\\)-distributed random variables) The starting point is the point estimator \\(\\displaystyle\\frac{\\overline X-\\mu}{S/\\sqrt{n}}\\), where \\(S^2=\\displaystyle\\frac{1}{n-1}\\sum_{i =1}^n (X_i-\\overline X)^2\\) is an estimate for \\(\\sigma^2\\). The following applies: \\(\\displaystyle \\frac{\\overline X-\\mu}{S/\\sqrt{n}}\\sim t(n-1)\\), where \\(t(n-1)\\) is a Student $t $ distribution with parameters denotes \\(n-1\\), and thus: \\[\\begin{equation*} \\mathbb P\\left(-t_{1-\\alpha/2,(n-1) }\\leq \\frac{\\overline X-\\mu}{S/\\sqrt{n}}\\leq t_{1-\\alpha/2,(n-1)}\\right) = 1-\\alpha, \\end{equation*}\\] with \\(t_{1-\\alpha/2}(n-1)\\) the corresponding quantile of the \\(t\\) distribution. Reshaping gives the confidence interval \\[\\begin{equation*} \\left[\\overline X-t_{1-\\alpha/2,(n-1)} \\frac{S}{\\sqrt{n}}, \\overline X + t_{1-\\alpha/2,(n-1)} \\frac{S}{\\sqrt{n}}\\right]. \\end{equation*}\\] Example 5.16 (Monthly returns) Example 5.15 cont. Sample mean: \\(\\overline x=\\displaystyle\\frac{3.32}{12}=0.2767\\%\\) Sample variance: \\(s^2 = \\displaystyle\\frac{2.5124}{11} =0.2284\\) \\((1-\\alpha/2)\\)-quantile: \\(t_{1-\\alpha/2}(n-1) = t_{0.975}(11) =2.2010\\) Realized confidence interval: \\[\\begin{align*} &amp;\\left[\\overline x-t_{1-\\alpha/2}(n-1) \\frac{s}{\\sqrt{n}}, \\overline x + t_{1-\\alpha/2}(n-1) \\frac{s}{\\sqrt{n}}\\right]= \\\\ &amp; \\left[0.2767-2.201\\cdot\\frac{\\sqrt{0.2284}}{\\sqrt{12}} ; 0.2767 + 2.201\\cdot \\frac{\\sqrt{0.2284}} {\\sqrt{12}}\\right] = [-0.0270\\%; 0.5803\\%] \\end{align*}\\] 5.4.1.1 Type C: Approximate confidence interval for \\(\\mu\\) for any distribution For any distribution but a large sample size (\\(n&gt;30\\)), the confidence interval results from the approximate normal distribution. \\((1-\\alpha)\\) confidence interval for \\(\\mu\\) with any distribution (\\(n&gt;30\\)) - If \\(\\sigma^2\\) is known, represents \\[\\begin{equation*} \\left[\\overline X - N_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\overline X + N_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right]. \\end{equation*}\\] represents an approximate confidence interval. Analogously: If \\(\\sigma^2\\) is unknown, then \\[\\begin{equation*} \\left[\\overline X-N_{1-\\alpha/2} \\frac{S}{\\sqrt{n}}, \\overline X + N_{1-\\alpha/2} \\frac{S}{\\sqrt{n}}\\right]. \\end{equation*}\\] Example 5.17 (Estimating the average invoice amount) Example 5.14 cont. In this example, we have already derived the \\(95\\%\\) confidence interval for the mean invoice amount and \\(\\sigma\\) known \\((n=100)\\): \\[ \\overline X\\pm 1.96\\cdot \\frac{\\sigma}{\\sqrt n} \\] Now if \\(\\sigma\\) is unknown, we replace it with the estimate \\(S\\): \\[ \\overline X\\pm 1.96\\cdot \\frac{S}{\\sqrt n} \\] We are allowed to keep the quantiles of the standard normal distribution, since \\(t\\) quantiles for \\(n\\) large approach these. Submit \\(\\left[\\overline X \\pm N_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right].\\) \\(\\left[\\overline X \\pm t_{1-\\alpha/2, n-1}\\frac{S}{\\sqrt{n}}\\right].\\) none of the above Submit Submit Submit Submit Submit 5.4.2 \\((1-\\alpha)\\) confidence interval for \\(\\sigma^2\\) Now it‚Äôs about interval estimation for the unknown variance \\(\\sigma^2\\). \\((1-\\alpha)\\) confidence interval for \\(\\sigma^2\\) for a normally distributed characteristic The two-sided confidence interval is determined by the limits \\[\\begin{equation*} \\left[\\frac{(n-1)S^2}{q_{1-\\alpha/2,(n-1)}} , \\frac{(n-1)S^2}{q_{\\alpha/2,(n-1)}}\\right]. \\end{equation*}\\] Note that - in contrast to the normal or \\(t\\) distribution - the \\(\\chi^2\\) distribution is not symmetric. Example 5.18 (Monthly returns) Example 5.16 cont. Sample variance: \\(s^2 = \\displaystyle\\frac{2.5124}{11} =0.2284\\) Quantiles: \\[ \\begin{array}[t]{rcl} q_{\\alpha/2,(n-1)} &amp;=&amp; q_{0.025,(11)} = 3.8157\\\\ q_{1-\\alpha/2,(n-1) }&amp;=&amp; q_{0.975,(11)} = 21.9200 \\end{array} \\] Confidence interval: \\[\\begin{align*} &amp;\\left[\\frac{(n-1)S^2}{q_{1-\\alpha/2(n-1)}} , \\frac{(n-1)S^2}{q_{\\alpha/2(n-1)}}\\right]=\\\\\\\\ &amp; \\left[\\frac{11\\cdot 0.2284}{21.9200}, \\frac{11\\cdot 0.2284}{3.8157}\\right] = \\left[0.1146; 0.6584\\right] \\end{align*}\\] Submit Submit Submit 5.4.3 \\((1-\\alpha)\\) confidence interval for probability \\(p\\) Let \\(X_1,\\ldots, X_n\\), with \\(X_i\\in \\{0,1\\}\\), be independent Bernoulli experiments. The sum of successes \\(X_i=1\\), \\(i=1,\\ldots, n\\), is binomially distributed: \\(\\sum_{i=1}^n X_i\\sim B(n,p)\\) with unknown parameter \\(p\\) . The ZGS states that \\(\\overline X\\) is approximately normally distributed with \\(\\mathbb E(\\overline X)=p\\) and \\(\\text{Var}(\\overline X) = p(1-p)/n\\). For a sufficiently large sample size (\\(n\\cdot p&gt;5\\) and \\(n\\cdot (1-p)&gt;5\\)), an approximate confidence interval is given by \\[\\begin{equation*} \\left[\\hat p - N_{1-\\alpha/2} \\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}, \\hat p + N_{1-\\alpha/2} \\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\\right], \\end{equation*}\\] where \\(\\hat p=\\overline X\\) denotes the relative frequency. Example 5.19 (App usage) Of the \\(100\\) customers surveyed, \\(55\\) were app users. If you use a confidence level of \\(1-\\alpha=0.95\\), you get \\(N_{1-\\alpha/2}=1.96\\). This results in \\(\\hat p= 55/100=0.55\\): \\[\\begin{equation*} \\left[0.55 - 1.96 \\cdot \\sqrt{\\frac{0.55\\cdot 0.45}{100}}; 0.55 + 1.96\\cdot \\sqrt{\\frac{0.55\\cdot 0.45}{100}}\\right] = [0.4525; 0.6475]. \\end{equation*}\\] Submit Submit ; Submit 5.4.4 Determining \\(n\\) based on the length of confidence intervals If a certain precision (e.g.¬†\\(\\pm 1\\%\\) or \\(\\pm 0.01\\)) with \\(1-\\alpha\\) confidence is to be achieved in an estimation problem, one can use the * *Length** of the associated confidence interval to determine the required sample size. The length of the \\(1-\\alpha\\) confidence interval for \\(p\\) is: \\[\\begin{align} \\text{upper limit} - \\text{lower limit} &amp;= p + N_{1-\\alpha/2} \\sqrt{\\frac{\\hat p(1-\\hat p)}{n}} - \\Big( p - N_{1-\\alpha/2} \\sqrt{ \\frac{\\hat p(1-\\hat p)}{n}}\\Big)\\\\ &amp;=2\\cdot N_{1-\\alpha/2} \\frac{\\sqrt{\\hat p(1-\\hat p)}}{\\sqrt{n}} \\end{align}\\] Before a sample is drawn, \\(\\hat p\\) is a random variable, so so is the term \\(\\sqrt{\\hat p(1-\\hat p)}\\). However, you can limit it upwards, because for \\(p=\\frac 12\\) it takes on the maximum value. Figure 5.1: f(p)=p(1-p) I.e. the maximum length of the confidence interval in this case is (\\(p=\\frac 12\\) inserted): \\[\\text{max. length} = \\frac{N_{1-\\alpha/2}}{\\sqrt{n}}\\] In order to maintain the needed accuracy, we set: \\[ \\frac{N_{1-\\alpha/2}}{\\sqrt{n}}\\leq 2\\cdot\\text{accuracy} = 2g \\] and solve for \\(n\\): \\[ \\sqrt n\\geq \\frac 12\\cdot\\frac{N_{1-\\alpha/2}}{g}\\rightarrow n\\geq \\left(\\frac 12\\cdot\\frac{N_{1-\\alpha/2}}{g}\\right)^2 \\] For example, if the accuracy of \\(0.01\\) is required at the confidence level of \\(99\\%\\), then the minimum sample size is \\(n\\geq 66348,966\\). Submit "],["hypothesis-testing.html", "Chapter 6 Hypothesis testing 6.1 The basic idea of statistical testing 6.2 Hypothesis tests for \\(p\\) 6.3 Hypothesis tests for \\(\\mu\\) 6.4 Type 1 and Type 2 errors 6.5 Exceeding probability: \\(p\\) value", " Chapter 6 Hypothesis testing So far we have made statistical inferences based on numerical estimators (point estimators and interval estimators). Well, there are problems where you want to test a certain assumption. It‚Äôs less about getting a number and more about supporting a hypothesis based on the data (e.g.¬†was a marketing campaign effective?). Test procedures answer the questions regarding these possible conclusions of a random process (hypotheses) and bring about a decision for one of the possibilities using tools of probability theory. In this chapter we will learn: How to formulate the hypotheses, which test procedures should be used and when? how to make a test decision. 6.1 The basic idea of statistical testing Example 6.1 (Discount campaign) You want to check whether a discount campaign has contributed to increasing sales. In a pilot study, \\(n=30\\) randomly selected customers were offered the discount. The remaining customers did not receive a discount and have an average invoice amount of \\(25\\) EUR with a known standard deviation of \\(\\sigma=2.4\\). The discount group has an average invoice amount of \\(\\overline y=26.3\\) EUR. There are two possibilities: The campaign is actually effective: it increases sales sustainably and we would be making a wrong decision if we assume that the increase in sales is random \\(\\Rightarrow\\) Company may abandon an effective campaign. The campaign is actually ineffective: sales in the pilot group increased due to random fluctuations and we would be making a wrong decision if we assumed that the increase in sales was due to the promotion \\(\\Rightarrow\\) cost for a pointless discount campaign. In the case of two possible, mutually exclusive conclusions, a distinction is made: null hypothesis \\(H_0\\) (status quo \\(\\leadsto\\) action does not change behavior, sales, ‚Ä¶) is not rejected as long as there is not enough evidence to refute it. Alternative hypothesis \\(H_1\\) (\\(\\leadsto\\) action is effective, favorably changes behavior, sales, ‚Ä¶) is accepted if ‚Äúsufficient‚Äù evidence speaks against the null hypothesis. Possible conclusions and consequences: \\[ \\begin{array}{c|cc} \\hline &amp;H_0\\text{ not rejected }&amp; H_0\\text{ rejected }\\\\\\hline H_0\\text{ is correct }&amp;\\text{decision correct }&amp; \\text{decision incorrect}\\\\ &amp;&amp;\\text{(Type 1 error)}\\\\ H_0\\text{ is wrong }&amp;\\text{decision incorrect}&amp;\\text{decision correct}\\\\ &amp;\\text{(Type 2 error)}&amp;\\\\\\hline \\end{array} \\] Wrong decisions are always possible and it is not possible to judge whether a given test decision is right or wrong. However, we construct statistical test in a way, that incorrect decisions have low probability. Two types of incorrect decisions can occur: incorrectly rejecting or incorrectly maintaining the null hypothesis: Type 1 error when \\(H_0\\) is rejected even though \\(H_0\\) is true, Type 2 error when \\(H_0\\) is retained even though \\(H_1\\) is true. The aim is to define a decision-making strategy so that Type 1 and Type 2 errors are kept as small as possible. Example 6.2 (Discount campaign) Example 6.1 cont. Probabilistic formulation of the problem: Sample \\(Y_1,Y_2,\\ldots,Y_{30}\\): Sales from pilot customers. Assumption: \\((Y_i)_{i=1,\\ldots,30}\\) are independent repetitions of a normally distributed random variable \\(Y\\) with unknown expected value \\(\\mu\\) and known standard deviation \\(\\sigma=2.4\\). Then, sample mean \\(\\overline Y\\) of the \\(30\\) realizations is a normally distributed random variable with expected value \\(\\mu\\) and standard deviation \\(\\sigma/\\sqrt{30}\\). Setting up the hypotheses: \\(H_0\\): \\(\\mu=25.0\\) (action not effective,‚ÄúStatus Quo‚Äù) \\(H_1\\): \\(\\mu&gt;25.0\\) (action effective) Basis for decision: makes sense to keep \\(H_0\\) if \\(\\overline y\\) is only slightly larger than \\(25\\). makes sense to reject \\(H_0\\) if \\(\\overline y\\) is significantly larger, e.g.¬†\\(35.0\\). \\(\\leadsto\\) Find critical \\(\\overline y^\\ast\\) from which on \\(H_0\\) is rejected. Choosing \\(\\overline y^\\ast\\) using probabilistic considerations: 1st attempt: \\(\\overline y^\\ast=25.25\\) Then \\(H_0\\) is rejected with a probability of \\(28\\%\\), assuming that \\(H_0\\) is true (Error Type 1): \\[\\begin{align*} \\mathbb P(H_0 \\text{ reject}|H_0 \\text{ is true})&amp;= \\mathbb P(\\overline Y\\geq 25.25|\\mu=25.0)\\\\ &amp;= \\mathbb P\\left(\\frac{\\overline Y-25,0}{2.4/\\sqrt{30}} \\geq \\frac{25.25-25.0}{2.4/\\sqrt{30}}\\right)\\\\ &amp;= 1-N(0.57) = 0.2843. \\end{align*}\\] \\(\\leadsto\\) Probability of type I error too high. 2nd attempt: \\(\\overline y^\\ast=26.5\\) \\(H_0\\) is ‚Äúonly‚Äù rejected incorrectly with probability \\(0.03\\%\\): \\[\\begin{equation*} \\mathbb P(H_0 \\text{ reject}| H_0 \\text{ is true}) = \\mathbb P(\\overline Y\\geq 26.50|\\mu=25.0) = 0.0003. \\end{equation*}\\] \\(\\leadsto\\) probability of error 1.¬†type very small (good!) But: Probability of type 2 error may be high (a distribution other than \\(H_0\\) might be more plausible in the range of values lying very far from the mean). Idea: Choose \\(\\overline y^\\ast\\) so that \\(H_0\\) is (falsely) rejected with a given small probability \\(\\alpha\\), e.g.¬†\\(\\alpha=0.05\\): \\[\\begin{align*} \\mathbb P(H_0 \\text{ reject}|H_0 \\text{ is true})&amp; = \\mathbb P(\\overline Y \\geq \\overline y^{\\ast}|\\mu=25.0) =0.05. \\end{align*}\\] This gives \\(\\overline y^\\ast=25.718\\). Our decision would be: Reject the hypothesis that action has no effect because \\(\\overline Y=26.3\\geq 25.718\\). This is not proof that \\(H_0\\) is false. High evidence against \\(H_0\\) leads to rejection. Because: If the true expected value is \\(25.0\\), samples with \\(\\overline Y\\geq 25.718\\) will only occur with a probability of \\(5\\%\\). Decision rule: Reject \\(H_0\\) if \\(\\overline Y\\geq 25.718\\). The following key figure is also interesting: Assuming that \\(H_0\\) holds, what is the probability - if the experiment is repeated - of obtaining a result that is at least as extreme as the one observed? This key figure is called the \\(p\\) value and is calculated as follows: \\[\\begin{equation*} \\mathbb P(\\overline Y_{\\text{Rep.}}\\geq \\overline Y|H_0) = \\mathbb P(\\overline Y_{\\text{Rep.}} \\geq 26.3|\\mu=25.0) = 0.0015 = 0.15%. \\end{equation*}\\] 6.2 Hypothesis tests for \\(p\\) Now we formally define hypothesis testing starting with tests for proportion \\(p\\). Consider the following example: Example 6.3 (Quality inspection) Before a batch of goods is delivered, the quality should be checked on the basis of a random sample. An attribute check is carried out. This means that for each piece produced, it is recorded whether it meets the quality standards (‚Äúgood‚Äù) or not (‚Äúbad‚Äù). The bad pieces are counted - the number can be assumed to be binomial distributed with parameter \\(p\\) (proportion of the defect pieces). A customer accepts delivery of goods if percentage of defect is less than \\(10\\%\\). For cost reasons, not the entire delivery can be checked. A sample of \\(n=1000\\) pieces results in \\(102\\) defect pieces. Is the difference between \\(10.2\\%\\) and \\(10\\%\\) small enough to conclude that the actual share is less than \\(10\\%\\)? 6.2.1 The exact binomial test General: Let \\(X_1, \\ldots, X_n\\) be sample variables with \\[\\begin{equation*} X_i= \\begin{cases} 1, &amp;\\text{ if } A \\text{ occurs},\\\\ 0, &amp;\\text{ if } \\overline A \\text{ occurs}. \\end{cases} \\end{equation*}\\] Let the probability of the event \\(\\{X_i=1\\}\\) be \\(p\\) (unknown!). Statistical testing problem: The hypothesis of interest is formulated as alternative hypothesis \\(H_1\\). It must prevail against the null hypothesis \\(H_0\\). A statistical test is about deciding whether the observed behavior also applies to the population. Test statistic is the quantity for which we know the distribution under \\(H_0\\): \\[\\begin{equation*} \\text{Number of &quot;successes&quot;: } X=\\sum_{i=1}^n X_i \\end{equation*}\\] If the assumption of independent draws is justified, then \\(X=\\sum_{i=1}^n X_i\\) is binomially distributed with parameters \\(n\\) and \\(p\\). Rejection region: How large does \\(X\\) have to be so that it is extremely unlikely that \\(X\\) follows the distribution under \\(H_0\\)? First, specify ‚Äúextremely unlikely‚Äù; usual values \\(0.01\\); \\(0.05\\); \\(0.1\\), denoted as significance level/ confidence level with symbol \\(\\alpha\\). This results in the rejection region, i.e.¬†those values of the random experiment, which point in the direction of the alternative, and whose probability under \\(H_0\\) is less than \\(\\alpha\\). Example 6.4 (Quality inspection) Example 6.3 cont. \\[ \\begin{array}{cccccccc}\\hline x_i&amp;101&amp;102&amp; 105 &amp;110 &amp;115&amp; 120&amp; 125\\\\ \\mathbb P(X&gt;x_i)&amp;0,432 &amp;0,391&amp; 0,278&amp; 0,135&amp; 0,053&amp; 0,017&amp; 0,005\\\\\\hline \\end{array} \\] \\(\\mathbb P(X&gt;102) = 0.391\\) i.e.¬†not small enough (this should be smaller than the error probability \\(\\alpha=0.01/0.05/0.1\\)). What is the critical value? At what point would we decline? (\\(\\mathbb P(X&gt;x_i)\\leq \\alpha\\)) \\[ \\begin{array}{ccccc}\\hline x_i&amp;115&amp; \\color{red}{116}&amp; 120&amp; 125\\\\ \\mathbb P(X&gt;x_i)&amp; 0,053&amp;\\color{red}{0,04334} &amp;0,017&amp; 0,005\\\\\\hline \\end{array} \\] 6.2.2 The approximate binomial test For large \\(n\\), the binomial distribution can be approximated by the normal distribution \\(\\leadsto\\) approximative binomial test. Let \\(X=\\sum_{i=1}^n X_i\\sim B(n,p)\\) be sufficiently large for \\(np\\) and \\(n(1-p)\\). Then the following applies: \\[\\begin{align*} X&amp;\\stackrel{\\text{approx.}}{\\sim} N(np, np(1-p))\\quad \\text{ or. }\\\\ Z=\\frac{X-n\\, p}{\\sqrt{n\\, p(1-p)}} &amp;\\stackrel{\\text{approx.}}\\sim N(0,1). \\end{align*}\\] Rule of thumb for ‚Äúsufficiently large‚Äù: \\(np&gt;5\\) and \\(n(1-p)&gt;5\\). Example 6.5 (Quality inspection) Example 6.4 cont. Formulation of the test problem (status quo corresponds to the defect pieces proportion of \\(0.1\\)): \\[\\begin{equation*} H_0: p = 0.1\\quad\\text{ versus }\\quad H_1: p&gt;0.1. \\end{equation*}\\] In a sample size of \\(n=1000\\) \\(x=102\\) defect pieces are observed. Approximation: \\(\\displaystyle Z=\\frac{X-1000\\cdot 0,1}{\\sqrt{1000\\cdot 0,1\\cdot 0.9}}=\\frac{X-100}{\\sqrt{90}} \\stackrel{\\text{approx.}}{\\sim}N(0;1).\\) Rejection region \\(C=\\{z:z&gt;N_{1-\\alpha}\\}\\), where \\(N_{1-\\alpha}\\) is the \\((1-\\alpha)\\)-quantile of the standard normal distribution (also called critical value). Let \\(\\alpha=0.05\\), then the critical value is \\(N_{1-\\alpha}=N_{0.95} = 1.64\\). Test statistic: \\(\\displaystyle z=\\frac{102-100}{9.5} = 0.21\\). Test decision: because \\(0.21&lt;1.64\\), \\(H_0\\) cannot be rejected. In words: The number of \\(102\\) defective pieces in the sample does not allow us to conclude that the proportion of defect pieces in the entire batch deliverred lies over \\(10\\%.\\) Alternatively, one could ask how many pieces \\(x^\\ast\\) would result in a rejection of \\(H_0\\)? Solving \\[\\begin{equation*} \\frac{x^\\ast-n\\, p}{\\sqrt{n\\, p (1-p)}} &gt;N_{1-\\alpha}, \\end{equation*}\\] for \\(x^\\ast\\) results \\[\\begin{equation*} x^\\ast&gt;\\sqrt{n\\, p(1-p)} N_{1-\\alpha} + n\\, p = 9.5\\cdot 1.64 + 100 = 115.58. \\end{equation*}\\] Approximate binomial test (summary) Given the following test problems over the parameter \\(p\\) one \\(B(n,p)\\) distribution: \\[\\begin{align*} (a) \\quad&amp; H_0:p=p_0\\quad\\text{ versus } \\quad H_1:p\\not=p_0 \\text{(two-sided)}\\\\ (b) \\quad &amp; H_0:p=p_0\\quad\\text{ versus } \\quad H_1:p&lt;p_0 \\text{(left-sided)}\\\\ (c) \\quad &amp; H_0:p=p_0\\quad\\text{ versus } \\quad H_1:p&gt;p_0 \\text{ (right-hand side)}. \\end{align*}\\] Based on the test size \\(\\displaystyle Z = \\frac{X-n\\, p_0}{\\sqrt{n\\, p_0 (1-p_0)}}\\stackrel{\\text{approx.}}\\sim N(0,1)\\) and the given level \\(\\alpha\\) the decision is made for \\(H_1\\) in the test problem \\[\\begin{align*} (a),\\quad&amp; \\text{ if } |z|&gt;N_{1-\\alpha/2}\\\\ (b),\\quad&amp; \\text{ if } z&lt;-N_{1-\\alpha}\\\\ (c),\\quad&amp;\\text{ if } z&gt;N_{1-\\alpha}. \\end{align*}\\] 6.3 Hypothesis tests for \\(\\mu\\) We have already seen the example with the discount campaign, where we actually care about mean sales \\(\\rightarrow \\mu\\)! The population mean \\(\\mu\\) is another distribution parameter, that builds frequently a basis for hypothesis testing. Below we specify the test concerning hypothesis testing about \\(\\mu.\\) 6.3.1 The Gauss test Assume that independent, identically distributed random variables are given \\(X_1, \\ldots, X_n\\) with \\(X_i\\sim N(\\mu,\\sigma^2)\\), or \\(X_i\\) with any continuous distribution, \\(\\mathbb E(X_i)=\\mu\\), \\(\\text{Var}(X_i)=\\sigma^2\\), \\(n\\) big enough (rule of thumb \\(n\\geq 30\\) ). \\(\\sigma^2\\) is known. Consider the following test problems: \\[\\begin{align*} (a) \\quad&amp; H_0:\\mu=\\mu_0\\quad\\text{ versus } \\quad H_1:\\mu\\not=\\mu_0\\\\ (b) \\quad &amp; H_0:\\mu=\\mu_0\\quad\\text{ versus } \\quad H_1:\\mu&lt;\\mu_0\\\\ (c) \\quad &amp; H_0:\\mu=\\mu_0\\quad\\text{ versus } \\quad H_1:\\mu&gt;\\mu_0. \\end{align*}\\] and test statistics: \\[\\displaystyle Z= \\frac{\\overline X-\\mu_0}{\\sigma/ \\sqrt{n}}\\stackrel{}{\\sim} N(0,1).\\] Based on the test statistic \\(Z\\), the decision in favor of \\(H_1\\) in the test problem: \\[\\begin{align*} (a),\\quad&amp; \\text{ if } |z|&gt;N_{1-\\alpha/2}\\\\ (b),\\quad&amp; \\text{ if } z&lt;-N_{1-\\alpha}\\\\ (c),\\quad&amp;\\text{ if } z&gt;N_{1-\\alpha}. \\end{align*}\\] Example 6.6 (Quality inspection of pencils) Pencils are produced with a should be length of \\(17\\) cm. Test problem: \\(\\displaystyle H_0: \\mu=17\\text{ versus } H_1: \\mu\\not=17\\). \\(\\alpha=0.01\\) is chosen as the significance level. The length \\(X\\) of the pencils is approx. normally distributed with \\(\\mathbb E(X)=\\mu\\) and \\(\\text{Var}(X)=\\sigma^2=2.25\\). A sample (\\(n=5\\)) is taken from current production; The following lengths are measured: \\[19.2\\mathrm{cm};\\ 17.4\\mathrm{cm};\\ 18.5\\mathrm{cm};\\ 16.5\\mathrm{cm};\\ 18.9\\mathrm{cm}\\] This results in \\(\\overline x=18.1\\text{cm}\\) and as a test variable \\[\\begin{equation*} z=\\frac{\\overline x-\\mu_0}{\\sigma/sqrt n} = \\frac{18.1-17}{\\sqrt{2.25}/\\sqrt{5}} = 1.64. \\end{equation*}\\] The rejection region is given as the \\((1-\\alpha/2)=(1-0.005) = 0.995\\)-quantile of the standard normal distribution, so \\(H_0\\) is rejected if \\(|z| &gt;2.5758.\\) Because \\(|z|=1.64\\), \\(H_0\\) is not rejected. 6.3.2 The \\(t\\) test As in the case of the Gauss test, a hypothetical expected value \\(\\mu_0\\) should be compared with an actual, unknown expected value. In contrast to the assumption in the Gauss test, the variance \\(\\sigma^2\\) is assumed to be unknown here. This leads to a different normalization in the denominator than in the Gaussian test, namely the unknown variance \\(\\sigma^2\\) is replaced by sample variance \\(S^2=\\sum_i (X_i-\\overline X)^2/(n-1)\\). In the following, let \\(t(n-1)\\) denote the Student \\(t\\) distribution with parameter \\(n-1\\) (degrees of freedom). Assumptions: \\(X_1,\\ldots, X_n\\) are independent and identically distributed with \\(X\\sim N(\\mu,\\sigma^2)\\) or arbitrarily distributed and \\(n&gt;30\\). Hypotheses: \\[\\begin{align*} \\text{(a)} \\quad &amp; H_0:\\mu=\\mu_0\\quad\\text{ versus } \\quad H_1:\\mu\\not=\\mu_0\\\\ \\text{(b)} \\quad &amp; H_0:\\mu=\\mu_0\\quad\\text{ versus } \\quad H_1:\\mu&lt;\\mu_0\\\\ \\text{(c)} \\quad &amp; H_0:\\mu=\\mu_0\\quad\\text{ versus } \\quad H_1:\\mu&gt;\\mu_0. \\end{align*}\\] Test statistics: \\[\\displaystyle T=\\frac{\\overline X-\\mu_0}{S/\\sqrt{n}}\\sim t(n-1)\\] under \\(H_0.\\) Rejection region: \\[\\begin{align*} \\text{(a)} \\quad &amp; |t|&gt;t_{1-\\alpha/2(n-1)}\\\\ \\text{(b)} \\quad &amp; t&lt;t_{\\alpha(n-1)}=-t_{1-\\alpha(n-1)}\\\\ \\text{(c)} \\quad &amp; t&gt;t_{1-\\alpha(n-1)} \\end{align*}\\] For \\(n &gt; 30\\) quantiles of \\(t(n-1)\\) can be replaced by the quantiles of \\(N(0,1)\\) because for more than \\(30\\) degrees of freedom, \\(t\\)-distribution can be well-approximated by the normal distribution. Example 6.7 (Quality inspection of pencils) Example 6.6 cont. We consider the same problem without the assumption of a known variance. Test problem: \\(\\displaystyle H_0: \\mu=17\\quad\\text{ versus } \\quad H_1: \\mu\\not=17\\). \\(\\alpha=0.01\\) is chosen as the significance level. The following pencil lengths are available as data: \\[\\begin{equation} 19.2\\mathrm{cm};\\ 17.4\\mathrm{cm};\\ 18.5\\mathrm{cm};\\ 16.5\\mathrm{cm};\\ 18.9\\text{cm}. \\end{equation}\\] \\end{equation} This results in \\(\\overline x=18.1cm\\) and \\(s^2=1.265\\). The test statistic is \\(\\displaystyle t = \\frac{\\overline x-\\mu_0}{s/\\sqrt{n}} = \\frac{18.1-17}{1.125/\\sqrt{5}} = 2.186\\). The critical value is the \\((1-\\alpha/2)\\)-quantile of \\(t(n-1)\\) distribution, so \\(t_{0.995,4} = 4.604\\). Since for the present realization of \\(T\\) holds \\(|2.186|&lt;4.604\\), \\(H_0\\) is not rejected. Frage: The claim should be tested as part of a one-sided test. The Gauss test is a suitable approach in this context. The exact binomial test represents a suitable approach in this context. The claim should be tested using a two-sided test. The t-test represents a suitable approach in this context. Submit Frage: The claim should be tested as part of a one-sided test. The Gauss test is a suitable approach in this context. The exact or approximative binomial test represents a suitable approach in this context. The claim should be tested using a two-sided test. The t-test represents a suitable approach in this context. Submit Frage: The claim should be tested as part of a one-sided test. The Gauss test is a suitable approach in this context. The exact or approximative binomial test represents a suitable approach in this context. The claim should be tested using a two-sided test. The t-test represents a suitable approach in this context. Submit 6.4 Type 1 and Type 2 errors The probability of a type I error (wrongly rejecting \\(H_0\\)): \\[\\mathbb P(H_0\\text{ reject}|H_0\\text{ true})\\leq \\alpha\\] is controlled by the error probability \\(\\alpha\\), also referred to as the significance level. A statistical test is called significance test at level \\(\\alpha\\), \\(0&lt;\\alpha&lt;1\\), if \\[\\begin{equation*} \\mathbb P(H_0\\text{ reject}|H_0\\text{ true}) \\leq \\alpha, \\end{equation*}\\] i.e. \\[\\begin{equation*} \\mathbb P(\\text{Type 1 error})\\leq \\alpha. \\end{equation*}\\] In the case of rejecting the null hypothesis one can also says: ‚Äúthe result is statistically significant at level \\(\\alpha\\)‚Äù. Interpretation: if \\(100\\) diefferent samples are drawn from the distribution under the null hypothesis, then the proportion of false rejections in a statistical test at the level \\(\\alpha=5\\%\\) will be (approximately/in the expectation) equal to \\(\\frac 5{100}.\\) The probability of a type II error, denoted as \\(\\beta\\), is not controlled: \\[\\mathbb P(H_0\\text{ not rejected}|H_1\\text{ true}) = \\beta\\] Statisticians look for tests that keep type 2 error low. 6.5 Exceeding probability: \\(p\\) value The \\(p\\)-value is defined as the probability to obtain under \\(H_0\\) a more extreme value for the test statistics in the direction of \\(H_1\\) than the value observed in the sample. If the \\(p\\)-value is smaller than the specified significance level \\(\\alpha\\), then \\(H_0\\) is rejected. Otherwise (\\(p\\)-value larger than \\(\\alpha\\)), \\(H_0\\) is not rejected. If the \\(p\\) value is very small, it means that it is very unlikely to observe the present test value under \\(H_0.\\) The \\(p\\)-value is commonly used by software packages to facilitate decisions in statistical tests. Example 6.8 (Quality inspection of pencils) Example 6.6 cont. In the quality inspection example, the test variable value is \\(z=0.21\\). The \\(p\\) value is given as: \\[\\begin{equation*} p = \\mathbb P(Z&gt; 0.21|H_0) = 1-\\mathbb P(Z\\leq 0.21|H_0) = 1-N(0.21) = 1-0.5832 = 0.4168. \\end{equation*}\\] Since the \\(p\\)-value is greater than the specified Significance level \\(\\alpha=0.05\\), the null hypothesis cannot be discarded. Calculating the \\(p\\)-value (\\(Z\\) is the test statistic): two-sided test: \\[\\begin{align} p-\\text{value} &amp;= \\mathbb P(Z&gt;|z|\\text{ under } H_0) + \\mathbb P(Z&lt;-|z|\\text{ under } H_0)\\\\ &amp;= 2\\cdot \\mathbb P(Z&gt;|z|\\text{ under } H_0) \\text{ with a symmetrical distribution of }Z. \\end{align}\\] left-sided test: \\[ p-\\text{value} = \\mathbb P(Z&lt; z\\text{ under } H_0). \\] right-sided test: \\[ p-\\text{value} = \\mathbb P(Z&gt; z\\text{ under } H_0). \\] Submit Submit Submit "],["regression-analysis.html", "Chapter 7 Regression analysis 7.1 Linear regression model from descriptive Statistics 7.2 Simple linear regression from a probabilistic perspective 7.3 Estimating, testing and forecasting 7.4 Forecast in linear regression models 7.5 Multivariate linear regression", " Chapter 7 Regression analysis Now we can apply both interval estimation techniques and hypothesis testing in the context of linear regression. In this chapter we will learn: How to analyze the linear regression model from a probabilistic perspective, how to carry out interval estimation or hypothesis testing for the coefficients and how to make a prediction.. 7.1 Linear regression model from descriptive Statistics For two metric features, \\(Y\\) and \\(X\\), the following relationship postulates linear regression model: \\[\\begin{equation*} Y=\\alpha + \\beta X. \\end{equation*}\\] Given \\(n\\) observations \\((y_i,x_i)\\), \\(i=1,\\ldots, n\\), (of two features \\(Y\\) and \\(X\\)) the following system of equations is set up: \\[\\begin{equation*} y_i = \\alpha + \\beta\\, x_i + \\varepsilon_i,\\quad i=1,\\ldots, n, \\end{equation*}\\] where \\(\\varepsilon_1, \\ldots, \\varepsilon_n\\) are error terms, which are chosen so that equality applies. The coefficients \\(\\alpha\\) and \\(\\beta\\) are calculated using the method of least squares, which is determined by: \\[\\begin{equation*} \\sum_{i=1}^n \\varepsilon_i^2 = \\sum_{i=1}^n (y_i - (\\alpha+\\beta\\, x_i))^2 \\quad\\rightarrow\\quad \\text{minimum}. \\end{equation*}\\] Example 7.1 (Forecasting the number of orders) We have already seen, that app usage and number of orders are positively associated to each other (see example 2.5) Plus we have a strong feeling, that time spent in shop could be a driver for the number of orders. So now we would like to forecast the average number of orders in the next six months using the information on app usage and time spend in shop via a linear regression model: \\[Y\\approx \\alpha + \\beta X,\\] where \\(Y\\) is the average number of orders, \\(\\alpha\\) and \\(\\beta\\) are some numbers, and \\(X\\) contains a binary variable for the app usage (with values eighter \\(0\\) (no app) or \\(1\\) (app)) and the time spent in shop. If we can estimate \\(\\alpha\\) and \\(\\beta\\) from the data, we would be able to compute \\(Y\\) from knowing just \\(X.\\) So before a customer makes some product orders, we can forecast their number, if we know whether this customer uses the app or not and what is his/her average time spent in shop. üòé 7.2 Simple linear regression from a probabilistic perspective In the following, we extend this purely data-based view to a probabilistic setting with dependent variable \\(Y\\) and error \\(\\varepsilon\\) being random variables with certain properties. We start with a simple (or univariate) linear regression model, where the following relationship between a dependent random variable \\(Y\\) and a single independent (random) variable \\(X\\) applies: \\[\\begin{equation*} Y_i = \\alpha + \\beta\\, x_i + \\varepsilon_i, \\quad i=1,\\ldots, n, \\end{equation*}\\] where \\(Y_1,\\ldots, Y_n\\): observable metric random variable, aka dependent variable; \\(x_1,\\ldots, x_n\\): given deterministic values (or Realizations of a RV \\(X\\)), aka regressor or independent variable; \\(\\varepsilon_1, \\ldots, \\varepsilon_n\\): error terms, unobservable RV‚Äôs that are independent and identically distributed with \\(\\mathbb E(\\varepsilon_i)=0\\), \\(\\text{Var}(\\varepsilon_i)=\\sigma^2\\). (In the case of a stochastic regressor, the assumptions apply over \\(\\varepsilon_i\\) then only conditionally, so \\(\\mathbb E(\\varepsilon_i|X_i=x_i)=0\\), etc.) The regression coefficients \\(\\alpha,\\beta\\) and the variance of the error terms \\(\\sigma^2\\) are unknown parameters which have to be estimated from the data \\((y_i,x_i)\\), \\(i=1,\\ldots, n.\\) It holds: \\[\\begin{align} \\mathbb E(Y_i) &amp;= \\mathbb E(\\alpha + \\beta x_i + \\varepsilon_i) = \\alpha + \\beta x_i\\\\ \\text{Var}(Y_i) &amp;= \\text{Var}(\\alpha + \\beta x_i + \\varepsilon) = \\sigma^2. \\end{align}\\] Statements about distributions of estimators and test statistics, also for small \\(n\\), is obtained by the following - stronger - assumption. Normal distribution assumption: \\[\\begin{equation*} \\varepsilon_i \\sim N(0,\\sigma^2),\\quad i=1,\\ldots,n, \\end{equation*}\\] and \\[\\begin{equation*} Y_i\\sim N(\\alpha+ \\beta x_i,\\sigma^2), \\quad i=1,\\ldots,n. \\end{equation*}\\] 7.3 Estimating, testing and forecasting In the following we will deal with the following aspects of linear regression model: Point and interval estimation of the unknown parameters \\(\\alpha\\), \\(\\beta\\), \\(\\sigma^2\\) Testing hypotheses about the regression coefficients \\(\\alpha\\), \\(\\beta\\) Prediction of target variable \\(Y\\) for a new value of \\(x\\) 7.3.1 Estimating the parameters of a linear regression model As for the distributional parameters in the previous chapters, we can conduct point and interval estimation in the linear regression setup. 7.3.1.1 Points estimators Parameter estimation is conducted via the ordinary Least Squares method (OLS) The idea of the method is: Determine \\(\\hat\\alpha\\) and \\(\\hat\\beta\\) such that \\[\\begin{equation*} \\sum_{i=1}^n (Y_i-\\alpha-\\beta x_i)^2 \\quad\\rightarrow\\quad \\min_{\\alpha,\\beta} \\end{equation*}\\] Minimizing the objective above, results in: \\[\\begin{align*} \\hat\\beta &amp;= \\frac{\\sum _{i=1}^n (x_i-\\overline x) (Y_i-\\overline Y)} {\\sum_{i=1}^n (x_i-\\overline x)^2} = \\hat\\rho _{XY} \\frac{\\hat\\sigma_Y}{\\hat\\sigma_X},\\\\[5pt] \\hat\\alpha &amp;= \\overline Y-\\hat\\beta \\overline x,\\\\[5pt] \\hat\\sigma^2&amp;= \\frac{1}{n-2} \\sum_{i=1}^n \\hat\\varepsilon_i^2 = \\frac{1}{n-2} \\sum_{i=1}^n (Y_i-\\hat\\alpha - \\hat\\beta x_i)^2, \\end{align*}\\] with residuals \\(\\hat\\varepsilon_i=Y_i-\\hat Y_i\\) and fitted values \\(\\hat Y_i=\\hat\\alpha + \\hat\\beta\\, x_i\\), \\(i=1,\\ldots, n\\). Note that \\(\\hat\\alpha\\), \\(\\hat\\beta\\) and \\(\\hat\\sigma^2\\) expression depend on random variables \\(Y_1,\\ldots, Y_n\\), therefore, they are themselves random estimation functions. These are unbiased: \\[\\begin{equation*} \\mathbb E(\\hat\\alpha)=\\alpha, \\quad \\mathbb E(\\hat\\beta)=\\beta, \\quad \\mathbb E(\\hat\\sigma^2)=\\sigma^2, \\end{equation*}\\] For the variance estimation the following applies: \\[\\begin{align*} \\text{Var}(\\hat\\alpha) &amp;= \\sigma_{\\hat\\alpha}^2 = \\sigma^2 \\frac{\\sum x_i^2} {n\\left(\\sum x_i^2 - n\\overline x^2\\right)}\\\\ \\text{Var}(\\hat\\beta) &amp;= \\sigma_{\\hat\\beta}^2 = \\frac{\\sigma^2} {\\sum x_i^2 - n\\overline x^2} \\end{align*}\\] Since \\(\\sum (x_i-\\overline x)^2\\rightarrow\\infty\\) for \\(n\\rightarrow\\infty\\), the estimators are also consistent. Under the normal distribution assumption: \\[\\begin{equation*} \\varepsilon_i \\sim N(0,\\sigma^2), \\quad Y_i\\sim N(\\alpha+\\beta x_i, \\sigma^2), \\quad \\hat\\alpha \\sim N(\\alpha, \\sigma_{\\hat\\alpha}^2), \\quad \\hat\\beta\\sim N(\\beta, \\sigma_{\\hat\\beta}^2). \\end{equation*}\\] Returning to our example 7.1: Example 7.2 (Forecasting number of orders) Now we use historical data on the amount of orders and the information on app usage to estimate the parameters of the linear relationship assumed. The output of running a linear regression in gretl is: We see, that the estimates are (\\(\\color{green}{n=100}\\)): \\[\\hat\\alpha=2.85 \\leadsto \\text{const}\\] and \\[\\hat\\beta=0.8108\\leadsto \\text{app}.\\] The estimated residual standard deviation can be deduced from the sum of squared residuals (\\(\\sum_{i=1}^n\\hat\\varepsilon_i^2\\)), contained in the output, as: \\[\\hat \\sigma_\\varepsilon = \\sqrt{\\frac{\\color{blue}{\\sum_{i=1}^n\\hat\\varepsilon_i^2}}{\\color{green}{n}-2}} = \\sqrt{\\frac{\\color{blue}{20.14512}}{98}}=0.4534.\\] 7.3.1.2 Interval estimators Under the normal distribution assumption, we have: \\[\\begin{equation*} \\frac{\\hat\\alpha-\\alpha}{\\hat\\sigma_{\\hat\\alpha}} \\sim t(n-2), \\quad \\frac{\\hat\\beta-\\beta}{\\hat\\sigma_{\\hat\\beta}} \\sim t(n-2). \\end{equation*}\\] \\(\\leadsto\\) Confidence intervals for \\(\\alpha\\) and \\(\\beta\\) \\[\\begin{equation*} \\hat\\alpha \\pm \\hat\\sigma_{\\hat\\alpha} \\, t_{1-\\alpha/2}(n-2),\\quad \\hat\\beta \\pm \\hat\\sigma_{\\hat\\beta}\\, t_{1-\\alpha/2}(n-2). \\end{equation*}\\] For \\(n&gt;30\\) the \\(t\\) quantiles can be replaced by \\(N(0,1)\\) quantiles be replaced. If the normal distribution assumption is not met, the above results apply approximately for sufficiently large \\(n\\). Example 7.3 (Forecasting number of orders) For our linear model in example 7.2, we can confidence intervall for the coefficients based on the results returned. \\(95\\%\\CI for \\(\\hat\\alpha\\) (\\(\\color{green}{n=100}\\)): )-CI for \\(\\hat\\alpha\\) (\\(n=100\\)): \\[[2.85 - t_{0.975, 98}\\cdot 0.0745; 2.85 + t_{0.975, 98}\\cdot 0.0745] = [2.7021; 2.9979]\\] \\(95\\%\\)-CI for \\(\\hat\\beta\\) (\\(n=100\\)): \\[[0.8108 - t_{0.975, 99}\\cdot 0.0939; 0.8108 + t_{0.975, 99}\\cdot 0.0939] = [0.6244; 0.9971]\\] 7.3.2 Testing in linear regression model Note that in our model, we assume that there is a linear relationship between the dependent and the independent variables. However, this assumption may not be fulfilled. If there is no linear relationship, then \\(\\beta=0.\\) However, the resulting estimate of \\(\\beta\\) may be different from zero, just because of random fluctuations. So, our first task is to check, whether the coefficients of the linear regression model are significant in the sense, that they are significantly different from zero. The so-called significance tests allow us to do that. The pair of hypotheses is of particular importance \\[\\begin{equation*} H_0: \\beta=0, \\quad H_1:\\beta\\not=0. \\end{equation*}\\] Here, \\(H_0:\\beta=0\\) means that \\(Y_i=\\alpha +\\varepsilon_i\\), \\(i=1,\\ldots, n\\), actually applies, and therefore \\(X\\) none has explanatory value for \\(Y\\). The relevant test type is the \\(t\\)-test.The \\(t\\)-test should be used to test the Hypotheses regarding¬†\\(\\alpha\\) and \\(\\alpha_0\\), or¬†\\(\\beta\\) and \\(\\beta_0\\). The test statistics are: \\[\\begin{equation*} T_{\\alpha_0} = \\frac{\\hat\\alpha - \\alpha_0}{\\hat\\sigma_{\\hat\\alpha}},\\quad T_{\\beta_0} = \\frac{\\hat\\beta- \\beta_0} {\\hat\\sigma_{\\hat\\beta}}. \\end{equation*}\\] Statistical program packages typically provide the \\(t\\) values and \\(p\\) values to the hypotheses \\(H_0:\\alpha=\\alpha_0\\) or¬†\\(H_0:\\beta=\\beta_0\\) with \\(\\alpha_0=0\\) and \\(\\beta_0=0\\). This shows whether the inclusion of \\(\\alpha\\), or¬†\\(\\beta\\) makes sense in the regression equation. Example 7.4 (Forecasting number of orders) For our linear model in example 7.2, we can conduct significance testing based on the results returned. Since both \\(p\\)-values are very small (scientific notation), we reject the Null hypothesis of zero coefficients. Of course, we can also use the \\(t\\)-values (the \\(t\\)-ratio) and check whether they lie in the rejection region instead. Besides the significance tests, also other hypothesis can be tested using the results. For example, we could test, whether the obtained \\(\\beta\\) is significantly smaller than \\(1.\\) Example 7.5 (Forecasting number of orders) For our linear model in example 7.2, we can test the hypothesis whether the obtained \\(\\beta\\) is significantly smaller than \\(1.\\) based on the results returned. Hypothesis: \\(H_0: \\beta=1\\) vs \\(H_1: \\beta&lt;1\\) Test statistics value: \\[t=\\frac{0.8108 - 1}{0.0939} = -2.0152\\] \\(p\\)-value: \\[\\mathbb P(T&gt;|t|) = 0.0233\\] So we reject \\(H_0\\) on the singificance level of \\(5\\%.\\) 7.4 Forecast in linear regression models Forecasting aims at finding a point estimate to a new value \\(x_0\\): \\[\\begin{equation*} \\hat y_0 = \\hat\\alpha + \\hat\\beta x_0. \\end{equation*}\\] The quality of the model (and its forecast) can be assessed by \\(R^2\\). The so-called coefficient of determination \\(R^2\\) gives the relative proportion the variance of the sample \\((y_1,\\ldots, y_n)\\), which is explained by the regression. In the case of linear regression, it is defined as \\[\\begin{equation*} R^2 = 1-\\frac{\\sum_{i=1}^n (Y_i - \\hat \\alpha -\\hat \\beta x_i)^2} {\\sum _{i=1}^n(Y_i-\\overline Y)^2}. \\end{equation*}\\] One can show that \\[\\begin{equation*} R^2 = \\frac{\\hat\\beta^2 \\hat\\sigma_x^2}{\\hat\\sigma_y^2} = \\frac{\\hat\\rho_{xy}^2 \\, \\hat\\sigma_y^2}{\\hat\\sigma_y^2} = \\hat\\rho _{xy}^2. \\end{equation*}\\] It holds \\(0\\leq R^2\\leq 1\\), where \\(R^2=0\\) the uncorrelated case, and \\(R^2=1\\) the case of a perfect linear relationship between the features \\(X\\) and \\(Y\\). Example 7.5 (Forecasting number of orders) Let us take a look on the output of running a linear regression is from example 7.1 again. Now we have a formula for forecasting the average amount of orders using the information on app usage: \\[norders \\approx 2.85 + 0.8108\\cdot app.\\] Using the result above, we can now forecast the average amount of orders depending on whether a customer uses the app or not. for an app user, we have: \\[norders \\approx 2.85 + 0.8108\\cdot 1 = 3.6607.\\] for a web user, we have: \\[norders \\approx 2.85 + 0.8108\\cdot 0 = 2.85.\\] With our model, we explained a fairly large proportion \\({43.2\\%}\\) of the variation in the number of orders data as indicated by \\(\\color{orange}{R^2}\\). the regressor the error term the dependent variable the independent variable the regressand the residuum Submit the regressor the error term the dependent variable the independent variable the regressand the residuum Submit the proportion of variance in the regression. the proportion of variance that was not explained by the regression. the correlation. the proportion of variance explained by the regression. the proportion of variance in the error terms. Submit Submit 7.5 Multivariate linear regression Using Univariate linear regression (only one explanatory variable) we found a functional relationship between the sales \\(U\\) and the average temperature \\(T\\). Can we take other influences into account (time spend in online shop)? Multivariate linear regression: Approach: \\(y=b_0 + b_1\\, x_1 + b_2\\, x_2 + \\cdots + b_L\\, x_L,\\) with the regressand or dependent variable \\(y\\) and \\(L\\) regressors \\(x_1, \\ldots, x_L\\) (multivariate regression); Sample: \\[ ((x_{11}, \\ldots, x_{L1}), y_1), \\ldots, ((x_{1n}, \\ldots, x_{Ln}), y_n) \\] System of linear equations: \\[\\begin{align*} y_1 &amp;= b_0 + b_1\\, x_{11} + b_2\\, x_{21} + \\cdots + b_L\\, x_{L1} + \\varepsilon_1\\\\ \\vdots\\\\ y_n &amp;= b_0+ b_1\\, x_{1n} + b_2\\, x_{2n} + \\cdots + b_L\\, x_{Ln} + \\varepsilon_n \\end{align*}\\] Representation in matrix form: \\[ \\bf{y} = \\bf{X}\\bf{b} + \\bf{\\varepsilon}, \\] with the design matrix \\[ \\bf{X}= \\begin{pmatrix} 1 &amp; x_{11} &amp; x_{21} &amp; \\cdots &amp; x_{L1}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\ 1 &amp; x_{1n} &amp; x_{2n} &amp; \\cdots &amp; x_{Ln} \\end{pmatrix} \\] OLS-Solution: \\[ \\bf{b} = (\\bf{X}^T \\bf{X})^{-1}\\, \\bf{X}^T\\, \\bf{y} \\] Example 7.6 (Forecasting number of orders) example 7.1 cont. Design matrix \\(\\bf X\\) with columns: constant (vector of ones), app indicator, time spend in shop \\[\\bf{X} = \\begin{pmatrix} 1&amp;1 &amp; 41.80\\\\ 1&amp;1 &amp; 21.16\\\\ 1&amp;1 &amp; 26.89\\\\ 1&amp;0 &amp; 51.89\\\\ 1&amp;0 &amp; 61.99\\\\ \\ldots &amp; \\ldots &amp;\\ldots\\\\ \\\\ \\end{pmatrix}\\] OLS-Solution: \\[\\begin{align*} \\beta&amp;=(\\bf{X}^\\top \\bf{X})^{-1}\\, \\bf{X}^\\top\\, \\bf{Y} \\end{align*}\\] Function to forecast the number of orders: \\[norders\\approx 2.0121 + 0.7941\\cdot app + 0.0198\\cdot times.\\] Which number of order would you forecast for an app user with \\(41.8\\) minutes spend in shop? Submit Submit \\(\\%\\) Submit "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
