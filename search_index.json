[["random-variables.html", "Chapter 2 Random variables 2.1 Discrete random variable 2.2 Continuous random variables", " Chapter 2 Random variables Nice to be able to calculate probabilities for various events üòÑ! As business analysts, we mostly work with numbers. This means that we usually transfer the results of a random process to numerical values. For example, we summarize a purchase decision (the interaction of unknown customer preferences and complex consequences of customer interactions with their environment) into a few numerical values, such as the number of products purchased, orders placed or the average invoice amount. We are primarily interested in the resulting numbers, while we can‚Äôt or don‚Äôt want to model the customer‚Äôs complex decision-making process. However, these numbers that we are looking at are not fixed but vary from customer to customer, month to month, etc. If we had to indicate future sales today, we would hardly be able to give a concrete value. We know approximately what values the sales volume can attain, but these are associated with uncertainty and therefore depend on complex events (random processes) that are partly controlled by chance. Probabilistically speaking, this future sales volume is a random variable. In this chapter we will learn: How to define the random variables and what kind of examples exist (\\(\\leadsto\\) discrete vs.¬†continuous random variables), how to derive distribution of random variables and how to calculate the key parameters of this distribution, as the expected value and the variance, but also the covariance and the quantiles. A random variable \\(X\\) is a variable whose values are derived from the result of a random operation. A number \\(x\\in \\mathbb R\\), which the random variable \\(X\\) assumes based on an outcome of an associated random process, is called the realization or the value of \\(X\\). Examples: Sum of two dice, Stock return in the next month, Number of app users in a survey. We differentiate between two types: discrete and continuous random variables, depending on what values they can assume. The distribution of a random variable \\(X\\) indicates which values in \\(\\mathbb R\\) are assumed with which probabilities. This distribution is derived from the original random process. Since random variables have numbers as values, we consider the following events: \\[\\begin{equation*} \\{X=x\\},\\quad \\{X\\not=x\\}, \\quad \\{X\\leq x\\},\\quad \\{X&gt;x\\}, \\quad \\{a\\leq X\\leq b\\}. \\end{equation*}\\] These events are assigned probabilities by the distribution of the random variable. 2.1 Discrete random variable A random variable \\(X\\) is called discrete if it assumes a finite or countably infinite number of values \\(x_1,x_2,\\ldots, x_n, \\ldots\\). Such as dice numbers, number of orders, number of customers per hour, indicator of a loan default or of a product purchase. The probability distribution of \\(X\\) is given by the probabilities \\[\\begin{equation*} \\mathbb P(X=x_i)=p_i, \\quad i=1,2,\\ldots. \\end{equation*}\\] We denote the set of values of \\(X\\) with \\(\\mathcal T=\\{x_1,x_2,\\ldots\\}\\). For the probabilities \\(p_1,p_2,\\ldots\\) applies (according to the axioms): \\[\\begin{gather*} \\phantom{\\quad i=1,2,\\ldots} 0\\leq p_i\\leq 1,\\quad i=1,2,\\ldots,\\\\ p_1+p_2+\\cdots = \\sum_{i\\geq 1} p_i=1. \\end{gather*}\\] The probability \\(\\mathbb P(X\\in A)\\) for \\(A\\subseteq \\mathcal T\\) is given as \\[\\displaystyle \\mathbb P(X\\in A) = \\sum_{i: x_i\\in A} p_i.\\] Example 2.1 (Win when rolling two dice) Example ?? cont. In the experiment ‚Äúsum of two fair dice with faces 1, 2 and 3‚Äù we found: \\(\\Omega = \\{2;3;4;5;6\\}\\) \\(\\omega\\in\\Omega,\\) \\(\\omega=\\) the sum of the dice, \\(\\mathcal F=\\mathcal P(\\Omega)\\), \\(\\mathbb P\\): \\[ \\begin{array}{c|ccccc}\\omega_i&amp;2&amp;3&amp;4&amp;5&amp;6\\\\\\hline \\mathbb P(\\{\\omega_i\\})&amp;\\frac 19&amp;\\frac 29&amp;\\frac 39&amp;\\frac 29&amp;\\frac 19 \\end{array} \\] Based on this, we now define a random variable \\(X=\\) The random win (in euros): \\[ X=\\begin{cases} 2\\cdot \\omega, &amp;\\text{ if } \\omega \\text{ odd},\\\\ - 1, &amp;\\text{ if } \\omega \\text{ even},\\\\ \\end{cases} \\] What are the values of \\(X\\) and what are their probabilities? Well, for each result \\(\\omega\\) we can determine the value of \\(X\\), specifically: \\[ \\begin{array}{c|ccccc}\\omega_i&amp;2&amp;3&amp;4&amp;5&amp;6\\\\\\hline x_i&amp;-1&amp;6&amp;-1&amp;10&amp;-1\\\\ \\mathbb P(\\{\\omega_i\\})&amp;\\frac 19&amp;\\frac 29&amp;\\frac 39&amp;\\frac 29&amp;\\frac 19 \\end{array} \\] The probabilities of the \\(X\\) values correspond to the probabilities of the respective outcomes. Only, some values in the above list are doubled. Therefore, to create a distribution of \\(X\\), we write down each value only once and sum the probabilities of the outcomes that give us that value to determine the final probability of that \\(X\\)-value: \\[ \\begin{array}{c|ccc} x_i&amp;-1&amp;6&amp;10\\\\\\hline \\mathbb P(x_i)=p_i&amp;\\frac 59&amp;\\frac 29&amp;\\frac 29 \\end{array} \\] This is the distribution of \\(X\\). Probability that \\(X\\) is positive is calculated as: \\[\\begin{equation*} \\mathbb P(X&gt;0)=\\mathbb P(X\\in \\{6;10\\}) = \\mathbb P(X=6) + \\mathbb P(X=10) = \\frac{4}{9 }. \\end{equation*}\\] 2.1.1 Probability and distribution functions Probability function If you specify the distribution of \\(X\\) like in Example 2.1, then you only have probabilities for numbers that are in the value set \\(\\mathcal T\\). That is, other real numbers \\(x\\not\\in \\mathcal T\\) have the probability of zero, i.e.¬†\\(\\mathbb P(X=x)=0\\) for \\(x\\not \\in \\mathcal T\\). The probability function assigns a value to every \\(x\\in \\mathbb R\\). The probability function \\(f(x)\\) of a discrete random variable \\(X\\) for \\(x\\in \\mathbb R\\) is defined by \\[\\begin{equation*} f(x) = \\begin{cases} \\mathbb P(X=x_i)=p_i, &amp;\\quad x=x_i\\in \\{x_1,x_2,\\ldots, \\},\\\\ 0,&amp;\\quad\\text{else}. \\end{cases} \\end{equation*}\\] Properties of the probability function: \\(f(x)\\geq 0\\), \\(x\\in \\mathbb R\\), \\(\\sum_{i\\geq 1} f(x_i)=1\\) Distribution function The (cumulative) distribution function (cdf) of a discrete random variable is given as \\[\\begin{equation*} F(x) = \\mathbb P(X\\leq x) = \\sum_{i: x_i\\leq x} f(x_i). \\end{equation*}\\] Properties of the distribution function of a discrete random variable: \\(F\\) is a right-sided continuous step function, \\(x\\mapsto F(x)\\) is monotonically increasing, \\(0\\leq F(x)\\leq 1\\), for all \\(x\\in \\mathbb R,\\) \\(\\lim_{x\\rightarrow-\\infty} F(x)=0\\) and \\(\\lim_{x\\rightarrow\\infty} F(x)=1\\). Example 2.2 (Win when rolling two dice) Example 2.1 cont. Probability function: \\[\\begin{align} f(x)&amp;=\\begin{cases}\\frac 59, \\text{if }x=-1,\\\\ \\frac 29, \\text{if }x\\in\\{6;10\\},\\\\ ~0, \\text{else.} \\end{cases} \\end{align}\\] Figure 2.1: Probability function for X Distribution function: \\[\\begin{align} F(x)=\\begin{cases} ~0, \\text{if } x&lt; -1,\\\\ \\frac 59, \\text{if }-1\\leq x&lt;6,\\\\ \\frac 79, \\text{if }~~6\\leq x&lt; 10,\\\\ ~1, \\text{if }~~x\\geq 10. \\end{cases} \\end{align}\\] Calculation example: if \\(6\\leq x&lt; 10\\) then \\[ F(x)=\\mathbb P(X\\leq x) = \\mathbb P(X=-1) + \\mathbb P(X=6) =\\frac 59+\\frac 29 = \\frac 79. \\] Figure 2.2: Distribution function for X Submit Submit Submit Submit Example 2.3 (Costs for customer orders) Example ?? cont. In the example with orders via app or web, we had the following common probabilities: \\[ \\begin{array}{c|cccccc} \\mathbb P((a_i,b_i))&amp;b_1=1&amp;b_2=2&amp;b_3=3&amp;b_4=4&amp;b_5=5&amp;b_6=6\\\\\\hline a_1=App&amp;0.02&amp;0.06&amp;0.12&amp;0.2&amp;0.12&amp;0.08 \\\\ a_2=Web&amp;0.08&amp;0.09&amp;0.13&amp;0.05&amp;0.03&amp;0.02 \\\\ \\end{array} \\] We also found that app users place more orders on average. Assume you process the orders placed in the online shop through a service partner. When customers places their orders in the shop, you have to pay a certain fee to the service partner. Furthermore, the fee depends on the number of orders and the interface: app or web. We define the costs that result from all ordering processes on a user account as a random variable \\(K\\). \\(K\\) is calculated as: \\[ K = \\begin{cases} 1.5\\cdot \\text{No. Orders}, &amp;\\text{ if via app and number. Orders }\\leq 4,\\\\ 1\\cdot \\text{No. Orders} + 1, &amp;\\text{ if via web and number. Orders }\\leq 4,\\\\ 6, &amp;\\text{ if via app and number. Orders }&gt;4,\\\\ 5, &amp;\\text{ if via web and display. Orders }&gt;4. \\end{cases} \\] How are the costs distributed? Values of \\(K\\): \\[ \\begin{array}{c|cccccc} (App,b_i)&amp;(App,1)&amp;(App,2)&amp;(App,3)&amp;(App,4)&amp;(App,5)&amp;(App,6)\\\\\\hline k_{1,i}&amp;1.5&amp;3&amp;4.5&amp;6&amp;6&amp;6\\\\ \\mathbb P(App,b_i)&amp;0.02&amp;0.06&amp;0.12&amp;0.2&amp;0.12&amp;0.08\\\\ \\end{array} \\] \\[ \\begin{array}{c|cccccc} (Web,b_i)&amp;(Web,1)&amp;(Web,2)&amp;(Web,3)&amp;(Web,4)&amp;(Web,5)&amp;(Web,6)\\\\\\hline k_{2,i}&amp;2&amp;3&amp;4&amp;5&amp;5&amp;5\\\\ \\mathbb P(Web,b_i)&amp;0.08&amp;0.09&amp;0.13&amp;0.05&amp;0.03&amp;0.02\\\\ \\end{array} \\] Distribution of \\(K\\): \\[ \\begin{array}{c|cccccccc} k_i&amp;1.5&amp;2&amp;3&amp;4&amp;4.5&amp;5&amp;6\\\\\\hline p_i&amp;0.02&amp;0.08&amp;0.15&amp;0.13&amp;0.12&amp;0.1&amp;0.4 \\end{array} \\] 2.1.2 Expected value and variance As in the descriptive statistics, it makes sense to define some meaningful parameters, that essentially describe the distribution of a random variable. For both types of random variables, we define the mean value \\(\\leadsto\\) expected value, a measure for the spread \\(\\leadsto\\) variance and standard deviation. For two discrete random variables, we also define covariance and correlation. Expected value and variance are the most important metrics, as many families of distributions (more on this later) are clearly defined by these two metrics. The two key metrics give us information about the mean position and variability of the values of random variables. Expected value Definition 2.1 (Expected value/ Expectation) The expected value \\(\\mathbb E(X)\\) of a discrete random variable \\(X\\) with value set \\(\\mathcal T=\\{x_1,x_2,\\ldots\\}\\) and probability function \\(f(x)\\) is: \\[\\begin{equation*} \\mathbb E(X) = x_1 f(x_1) + x_2 f(x_2) + \\cdots = \\sum_{i\\geq 1} x_i f(x_i). \\end{equation*}\\] Properties of the expected value \\(\\mathbb E(a X + b) = a \\mathbb E (X) + b\\) (linear transformation) \\(\\mathbb E(X+Y) = \\mathbb E(X) + \\mathbb E(Y)\\) (expected value of a sum = sum of expected values) Variance and standard deviation Definition 2.2 (Variance and standard Deviation) The variance of a random variable \\(X\\) is \\[\\begin{equation*} \\sigma^2 = \\text{Var}(X) = \\mathbb E((X-\\mathbb E(X))^2)\\\\ =\\sum_{i\\geq 1} (x_i-\\mathbb E(X))^2 f(x_i), \\end{equation*}\\] The standard deviation is \\(\\sigma = \\sqrt{\\text{Var}(X)}.\\) Best suited for calculating variance: \\[\\begin{equation*} \\text{Var}(X)=\\mathbb E(X^2) - (\\mathbb E(X))^2. \\end{equation*}\\] For discrete random variables this means: \\[\\begin{align} \\text{Var}(X)&amp;=\\sum_{i\\geq1} x_i^2f(x_i)- (\\mathbb E(X))^2. \\end{align}\\] Properties of Variance \\(\\text{Var}(aX+b)= a^2\\text{Var}(X)\\) (linear transformation) \\(\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y)\\) if \\(X\\) and \\(Y\\) are stochastically independent. This results in the calculation rules for linear transformations (\\(Y=a X + b\\)): \\[\\begin{align} \\mathbb E(Y) &amp;= \\mathbb E (a X + b)=a \\mathbb E(X) + b,\\\\ \\text{Var}(Y)&amp;=\\text{Var}(a X + b) = a^2\\text{Var}(X). \\end{align}\\] Example 2.4 (Costs for customer orders) Example 2.3 cont. In this example, we derived the following probability distribution for the cost \\(K\\): \\[ \\begin{array}{c|ccccccc} k_i&amp;1.5&amp;2&amp;3&amp;4&amp;4.5&amp;5&amp;6\\\\\\hline p_i&amp;0.02&amp;0.08&amp;0.15&amp;0.13&amp;0.12&amp;0.1&amp;0.4 \\end{array} \\] That is, \\(\\mathbb E(K) = \\sum_{i=1}^7 k_i\\cdot p_i = 1.5\\cdot 0.02 + \\ldots + 6\\cdot 0.4 = 4.6.\\) \\(\\text{Var}(K) = \\sum_{i=1}^7 k^2_i\\cdot p_i - \\mathbb E(K)^2= 1.5^2\\cdot 0.02 + \\ldots + 6^2\\cdot 0.4 -4,6^2 = 1.965.\\) Because of rising inflation the service cost is revised. The new cost includes a \\(20\\%\\) increase (an increase by the factor \\({\\color{red}{1.2}}\\)) and an additional fee of \\(\\color{blue}1\\) euro compared to the old costs, such that \\[K_{new} = {\\color{red}{1.2}}\\cdot K+{\\color{blue}{1}}.\\] The new parameters are: \\(\\mathbb E(K_{new}) = \\mathbb E ({\\color{red}{1.2}}\\cdot K+{\\color{blue}{1}})={\\color{red}{1.2}}\\cdot \\mathbb E(K)+{\\color{blue}{1}} = 1.2\\cdot 4.6+1=6.52.\\) \\(\\text{Var}(K_{new}) = \\text{Var}({\\color{red}{1.2}}\\cdot K+{\\color{blue}{1}})= ({\\color{red}{1.2}})^2\\cdot \\text{Var}(K) = 1.44\\cdot 1.965=2.8296.\\) Submit Submit Submit Submit 2.1.3 Covariance and correlation Whenever we are dealing with several random variables as in example ??, the question of association between the variables arises. If we can find a positive association pattern between the number of orders and the usage of app, then we could possibly boost the sales by promoting the mobile app usage. A measure of linear dependence between two random variables is their covariance and their associated correlation. Definition 2.3 (Covariance and Correlation) The covariance of two random variables \\(X\\) and \\(Y\\) is determined by \\[\\begin{equation*} \\sigma_{XY}=\\text{Cov}(X,Y) =\\mathbb E[(X-\\mathbb E(X))\\, (Y-\\mathbb E(Y))] = \\mathbb E[X\\cdot Y] - \\mathbb E(X)\\, \\mathbb E(Y). \\end{equation*}\\] Specifically for discrete random variables: \\[ \\text{Cov}(X,Y) =\\sum_{i,j}x_i\\cdot y_j\\cdot f(x_i,y_j) - \\left(\\sum_ix_i\\cdot f(x_i)\\right)\\cdot\\left( \\sum_jy_j\\cdot f(y_j)\\right). \\] The correlation coefficient is determined by \\[\\begin{equation*} \\rho_{XY}=\\frac{\\sigma_{XY}}{\\sigma_X\\, \\sigma_Y} = \\frac{\\text{Cov}(X,Y)} {\\sqrt{\\text{Var}(X)}\\, \\sqrt{\\text{Var}(Y)}}. \\end{equation*}\\] \\(X\\) and \\(Y\\) are called uncorrelated if \\(\\text{Cov}(X,Y)=0\\). \\(-1\\leq \\rho_{XY}\\leq 1\\) If \\(X, Y\\) are independent, then they are also uncorrelated. Example 2.5 (Orders and Usage) In the example ?? we looked at the joint distribution of the app usage and orders. Now we define two random variables: \\(A=\\begin{cases}1, &amp;\\text{ falls } App\\\\0, &amp;\\text{ falls } Web\\end{cases}\\) (an indicator of app usage). \\(B=b_i\\), \\(b_i\\in\\{1; 2; \\ldots 6\\}\\) the number of orders. The joint probabilities are: \\[ \\begin{array}{c|cccccc} \\mathbb P((a_i,b_i))&amp;B=1&amp;B=2&amp;B=3&amp;B=4&amp;B=5&amp;B=6\\\\\\hline A=1&amp;0.02&amp;0.06&amp;0.12&amp;0.2&amp;0.12&amp;0.08\\\\ A=0&amp;0.08&amp;0.09&amp;0.13&amp;0.05&amp;0.03&amp;0.02\\\\ \\end{array} \\] Are the two quantities correlated? Expected values \\[\\begin{align} \\mathbb E(A)&amp;=1\\cdot0,6 + 0\\cdot0,4 = 0,6,\\\\ \\mathbb E(B)&amp;=1\\cdot0,1 + 2\\cdot0,15 +\\ldots + 6\\cdot0,1=3,5. \\end{align}\\] Variances: \\[\\begin{align} \\text{Var}(A)&amp;=1^2\\cdot0,6 + 0^2\\cdot0,4 - 0,6^2 = 0,24,\\\\ \\text{Var}(B)&amp;=1^2\\cdot0,1 + 2^2\\cdot0,15 +\\ldots + 6^2\\cdot0,1 - 3,5^2=2,05. \\end{align}\\] Covariance: \\[\\begin{align} \\text{Cov}(A,B)&amp;=\\mathbb E(A\\cdot B) - \\mathbb E(A)\\, \\mathbb E(B)\\\\ &amp;=1\\cdot 1\\cdot0.02 + 1\\cdot 2\\cdot0.06 + 1\\cdot 3\\cdot0.12 + 1\\cdot 4\\cdot0.2 \\\\ &amp;+ 1\\cdot 5\\cdot0.12 + 1\\cdot 6\\cdot0.08 \\text{ (f√ºr A=1 und B=1;2;$\\ldots$ 6)} + \\\\ &amp;+ 0\\cdot 1\\cdot0.08 + \\dots + 0\\cdot 6\\cdot0.02\\text{ (f√ºr A=0 und B=1;2;$\\ldots$ 6)} - \\\\ &amp;- 0,6\\cdot3,5 = 0,28. \\end{align}\\] Correlation: \\[\\begin{align} \\rho_{A,B}&amp;=\\frac{\\text{Cov}(A,B)}{\\sqrt{\\text{Var}(A)}\\cdot\\sqrt{\\text{Var}(B)}}\\\\ &amp;= \\frac{0,28}{\\sqrt{0,24}\\cdot\\sqrt{2,05}} = 0,3992. \\end{align}\\] So the two random variables are positively correlated. Yuhu! We have a positive association between the app usage and the number of orders. It means, that by increasing the incentive for the app usage, we could increase our sales. 2.2 Continuous random variables We have seen how to model discrete random variables. However, there are certainly cases where the framework of discrete random variables is not sufficient. For example, when we look at sizes, lengths, volumes, but also when we examine sales, times in the online shop, waiting times or security returns. These variables take on many different values, which makes modeling difficult for us because we have to take probabilities into account for all values ü§î. In addition, due to measurement precision, we usually cannot distinguish between online shopping time of \\(12\\) minutes and \\(12,0001\\) minutes. Then why not just look at a range \\((12, 12.1)\\) minutes? So it makes sense for some variables, instead of looking at fixed values, to look at entire areas (\\(\\leadsto\\) continuous random variables) and the ‚Äúfullness‚Äù or, in other words, the density of these areas as the basis for the probability calculation, with the probabilities represented as areas under this density on the corresponding section. This implies that the probability of hitting a very specific value (the point probability) becomes zero. A random variable \\(X\\) is said to be continuous if there is a function (density function) \\(x\\mapsto f(x)\\geq 0\\), so that for every interval \\([a,b]\\) we have: \\[\\begin{equation*} \\mathbb P(a\\leq X\\leq b)=\\int_a^b f(x) dx. \\end{equation*}\\] The set of values \\(\\mathcal T\\subseteq\\mathbb R\\) of a continuous random variable is uncountable. It corresponds either to the whole \\(\\mathbb R\\) or to the subsets of \\(\\mathbb R\\) (intervals). 2.2.1 Density function Density properties: \\(f(x)\\geq 0\\) for all \\(x\\in \\mathbb R\\) \\(\\int_{\\mathbb R} f(x)\\, dx=1\\) The point probability, \\(\\mathbb P(X=x)=0\\) for all \\(x\\in \\mathbb R\\) if \\(X\\) represents a continuous random variable. Graphical representation: Figure 2.3: Example: Density function Probabilities are represented by areas under density \\(f(x)\\) set. Calculating the probability that \\(X\\) values ‚Äã‚Äãin \\(B\\) accepts: \\[\\begin{equation*} \\mathbb P(X\\in B) = \\int_B f(x)\\, dx = \\int_a^b f(x)dx \\end{equation*}\\] 2.2.2 Distribution function The (cumulative) distribution function of a continuous Random variable is given as \\[\\begin{equation*} F(x) = \\mathbb P(X\\leq x) = \\int_{-\\infty}^x f(t)\\, dt. \\end{equation*}\\] Properties of the distribution function for continuous random variables: \\(F\\) is a continuous function \\(x\\mapsto F(x)\\) is monotonically increasing \\(0\\leq F(x)\\leq 1\\), for all \\(x\\in \\mathbb R\\) \\(\\lim_{x\\rightarrow-\\infty} F(x)=0\\) and \\(\\lim_{x\\rightarrow\\infty} F(x)=1\\). Graphical representation: Figure 2.4: Example: Probability function of a continuous random variable If the distribution function \\(F\\) is given, this is what you get Density \\(f\\) by deriving: \\[\\begin{equation*} f(x) = F^\\prime(x),\\quad\\text{ differentiable for $F$ in $x$} \\end{equation*}\\] Calculating the probability that \\(X\\) values in \\(B\\) accepts: \\[\\begin{equation*} \\mathbb P(X\\in B) = \\mathbb P(a\\leq X \\leq b) = F(b) - F(a). \\end{equation*}\\] Example 2.6 (Time in the online shop as a continuous random variable) We model the random variable \\(X=\\) time in the online shop in minutes as a continuous random variable with the density: \\[ f(x) = \\begin{cases}0,001\\cdot x , &amp;\\text{ f√ºr } 0\\leq x\\leq 20,\\\\ 0,025 -0,00025\\cdot x, &amp;\\text{ f√ºr } 20&lt; x\\leq 100.\\end{cases} \\] That is, Figure 2.5: Probability density of X=Time in online shop We compute the event probabilities: \\(\\{X&lt;20\\}=\\text{&quot;Zeit im Shop weniger als 20 Minuten&quot;}\\): \\[\\begin{align} \\mathbb P(X&lt;20)&amp;= \\displaystyle \\int_0^{20} (0,001\\cdot x )dx = \\text{Area of the red triangle}\\\\ &amp;= \\frac{1}{2}\\cdot 20\\cdot 0,02 = 0,2. \\end{align}\\] \\(\\{X&lt;60\\}=\\text{&quot;Zeit im Shop weniger als 60 Minuten&quot;}\\): \\[\\begin{align} \\mathbb P(X&lt;60)&amp;= 1-\\mathbb P(X\\geq 60)=1-\\displaystyle \\int^{100}_{60} (0,025 -0,00025\\cdot x)dx \\\\ &amp;= 1- \\text{Area of the transparent triagle} = 1-\\frac{1}{2}\\cdot 40\\cdot 0,01 = 0,8. \\end{align}\\] Figure 2.6: Probability density of X=Time in online shop with probabilities as areas under the density fucntion. Computing the cumulative probability function from the density function: case: \\(x\\in [{0},{20}]\\): \\[\\begin{align} F(x) &amp;= \\int_{{0}}^x f(t)~ dt = \\int_{0}^x (0,001\\cdot t )~ dt=\\Big[0,001\\cdot \\frac{t^2}2 \\Big]_{0}^x \\\\ &amp; = 0,001\\cdot \\frac{x^2}2 - (0,001\\cdot \\frac{0^2}2 )\\\\ &amp;=0,0005x^2. \\end{align}\\] case: \\(x\\in ({20},{100}]\\): \\[\\begin{align} \\displaystyle F(x) &amp;= \\int_{0}^x f(t)~ dt\\\\ &amp;= \\int_{0}^{20} (0,001\\cdot t )~ dt + \\int_{20}^x (0,025 -0,00025\\cdot t)~ dt\\\\ &amp;=0,2 + \\big[0,025\\cdot t -0,00025\\cdot \\frac{t^2}2 \\big]_{20}^x\\\\ &amp;=0,2 + 0,025\\cdot x -0,00025\\cdot \\frac{x^2}2 - (0,025\\cdot 20 -0,00025\\cdot \\frac{20^2}2) \\\\ &amp;= -0.25 + 0.025x -0.000125x^2. \\end{align}\\] That is: \\[ F(x) = \\begin{cases} 0,&amp;x&lt;0,\\\\ 0,0005x^2,&amp;0\\leq x\\leq 20,\\\\ -0.25 + 0.025x -0.000125x^2,&amp;20&lt;x\\leq 100,\\\\ 1,&amp;x&gt;100. \\end{cases} \\] Figure 2.7: Cumulative distribution function of X=Time in online shop Using the cumulative distribution function, we can calculate the event probabilities: \\(\\{X&lt;20\\}=\\text{&quot;Time in shop less than 20 minutes&quot;}\\): \\[\\begin{align}\\mathbb P(X&lt;20)&amp;= \\mathbb P(X\\leq20)=F(20) = 0,0005\\cdot 20^2= 0.2.\\end{align}\\] \\(\\{X&lt;60\\}=\\text{&quot;Zeit im Shop weniger als 60 Minuten&quot;}\\):\\[\\begin{align}\\mathbb P(X&lt;60)&amp;=\\mathbb P(X\\leq60)=F(60) =-0.25 + 0.025\\cdot 60 -0.000125\\cdot 60^2= 0.8.\\end{align}\\] \\(\\{X &gt;30\\}=\\text{&quot;Time in shop less than 30 minutes&quot;}\\):\\[\\begin{align} \\mathbb P(X&gt;30)&amp;=1-\\mathbb P(X\\leq30)= 1-F(30) \\\\&amp;= 1-(-0.25 + 0.025\\cdot 30 -0.000125\\cdot 30^2)\\\\ &amp;= 0.6125.\\end{align}\\] Figure 2.8: Cumulative probability function of X=Time in online shop with probabilities \\(f(x)=\\begin{cases}2x,&amp;\\text{ if }0\\leq x \\leq 1,\\\\0,&amp;\\text{ if } x&lt;0,\\\\1,&amp;\\text{ if }x&gt;1.\\end{cases}\\) \\(f(x)=\\begin{cases}x^2,&amp;\\text{ if }0\\leq x \\leq 1,\\\\0,&amp;\\text{ if } x&lt;0,\\\\1,&amp;\\text{ if }x&gt;1.\\end{cases}\\) \\(f(x)=\\begin{cases}2x,&amp;\\text{ if }0\\leq x \\leq 1,\\\\0,&amp;\\text{ if } x&lt;0,x&gt;1.\\end{cases}\\) \\(f(x)=\\begin{cases}\\frac13 x^3,&amp;\\text{ if }0\\leq x \\leq 1,\\\\0,&amp;\\text{ if } x&lt;0,\\\\1,&amp;\\text{ if }x&gt;1.\\end{cases}\\) Submit Submit Submit Submit Submit 2.2.3 Expected value and variance Expected value and variance are the most important metrics, as many families of distributions (more on this later) are clearly defined by these two metrics. The two measures give us information about the mean position and variability of the values of random variables. Expected value Definition 2.4 (Expected Value/ Expectation (continuous rv)) The expected value \\(\\mathbb E(X)\\) of a continuous random variable \\(X\\) with density \\(f(x)\\) is \\[\\begin{equation*} \\mathbb E(X) = \\int_{-\\infty}^\\infty x\\, f(x)\\, dx. \\end{equation*}\\] Properties of the expected value \\(\\mathbb E(a X + b) = a \\mathbb E (X) + b\\) (linear transformation) \\(\\mathbb E(X+Y) = \\mathbb E(X) + \\mathbb E(Y)\\) (expected value of a sum = sum of expected values) \\(\\mathbb E(X\\cdot Y) = \\mathbb E(X)\\cdot \\mathbb E(Y)\\) if \\(X\\) and \\(Y\\) are stochastically independent. Variance and standard deviation Definition 2.5 (Variance and standard deviation (continuous rv)) The variance of a continuous random variable \\(X\\) is \\[\\begin{equation*} \\sigma^2 = \\text{Var}(X) = \\mathbb E((X-\\mathbb E(X))^2)= = \\int_{-\\infty}^\\infty (x-\\mathbb E(X))^2\\, f(x)\\, dx. \\end{equation*}\\] The standard deviation is \\(\\sigma = \\sqrt{\\text{Var}(X)}.\\) Best suited for calculating variance: \\[\\begin{equation*} \\text{Var}(X)=\\mathbb E(X^2) - (\\mathbb E(X))^2. \\end{equation*}\\] For continuous random variables this means: \\[\\begin{align} \\text{Var}(X)&amp;=\\int_{-\\infty}^{\\infty} x^2f(x)dx- (\\mathbb E(X))^2. \\end{align}\\] Properties of Variance \\(\\text{Var}(aX+b)= a^2\\text{Var}(X)\\) (linear transformation) \\(\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y)\\) if \\(X\\) and \\(Y\\) are stochastically independent. This results in the calculation rules for linear transformations (\\(Y=a X + b\\)): \\[\\begin{align} \\mathbb E(Y) &amp;= \\mathbb E (a X + b)=a \\mathbb E(X) + b,\\\\ \\text{Var}(Y)&amp;=\\text{Var}(a X + b) = a^2\\text{Var}(X). \\end{align}\\] Example 2.7 (Time in the online shop as a continuous random variable) The random variable \\(X=\\) time in the online shop in minutes has the density: \\[ f(x) = \\begin{cases}0,001\\cdot x , &amp;\\text{ f√ºr } 0\\leq x\\leq 20,\\\\ 0,025 -0,00025\\cdot x, &amp;\\text{ f√ºr } 20&lt; x\\leq 100.\\end{cases} \\] We have: Expected value: \\[\\begin{align} \\mathbb E(X) &amp;= \\int_{-\\infty}^{\\infty}x\\cdot f(x)dx = \\int_{0}^{20}x\\cdot f(x)dx + \\int_{20}^{100}x\\cdot f(x)dx\\\\ &amp;=\\int_{0}^{20}x\\cdot(0,001\\cdot x )dx + \\int_{20}^{100}x\\cdot (0,025 -0,00025\\cdot x)dx\\\\ &amp;=\\int_{0}^{20}0,001\\cdot x^2 dx + \\int_{20}^{100}0,025\\cdot x -0,00025\\cdot x^2dx\\\\ &amp;=\\Big[0,001\\cdot \\frac{x^3}3\\Big]_{0}^{20} + \\Big[0,025\\cdot \\frac{x^2}2 -0,00025\\cdot \\frac{x^3}3\\Big]_{20}^{100}\\\\ &amp;=2,6667 + 37,3333 = 40. \\end{align}\\] Variance: \\[\\begin{align} \\text{Var}(X) &amp;= \\mathbb E(X^2) - \\mathbb E(X) = \\int_{-\\infty}^{\\infty}x^2\\cdot f(x)dx - 40^2\\\\ &amp;\\ldots \\text{ (analog zu dem Erwartungswert) }\\\\ &amp;=\\int_{0}^{20}0,001\\cdot x^3 dx + \\int_{20}^{100}\\big(0,025\\cdot x^2 -0,00025\\cdot x^3\\big)dx - 40^2\\\\ &amp;=\\Big[0,001\\cdot \\frac{x^4}4\\Big]_{0}^{20} + \\Big[0,025\\cdot \\frac{x^3}3 -0,00025\\cdot \\frac{x^4}4\\Big]_{20}^{100}- 40^2\\\\ &amp;=40 + 2026,6667 - 40^2= 466,6667. \\end{align}\\] Submit Submit 2.2.4 Quantiles of continuous random variables In the first part of statistics, we determined empirical quantiles. They helped us to understand which value divides the ordered observations so that at least a proportion \\(p\\) of the observations are less than or equal and at least a proportion \\(1-p\\) - greater or equal. In other words, \\(\\hat q_p\\) has answered the question: which value is fallen below with frequency \\(p\\) or exceeded with frequency \\(1-p\\)? The same logic is used to define quantiles for random variables, except that it is now about the probability of falling below or exceeding this value. Definition 2.6 (Quantile (continuous rv)) The \\(p\\)-quantile of a continuous random variable \\(X\\) with density \\(f(x)&gt;0\\) on an interval in \\(\\mathbb R\\) is the number \\(q_p\\) with \\(0&lt;p&lt; 1\\), for which applies: \\[\\begin{equation*} \\mathbb P(X&lt;q_p)=\\mathbb P(X\\leq q_p)=F(q_p)=p. \\end{equation*}\\] The quantile is (for continuous random variables) clearly determined by the inverse of the distribution function, \\[\\begin{equation*} q_p=F^{(-1)}(p). \\end{equation*}\\] Figure 2.9: Quantil der Standardnormalverteilung f√ºr p=0,9 Special cases: \\(p=0.25\\), then \\(Q_p\\) is called lower quartile \\(p=0.50\\), then \\(Q_p\\) is called median \\(p=0.75\\), then \\(Q_p\\) is called upper quartile Example 2.8 (Time in the online shop as a continuous random variable) See empirical data on this 2.6. The random variable \\(X=\\) time in the online shop in minutes has the distribution function: \\[ F(x) = \\begin{cases} 0,&amp;x&lt;0,\\\\ 0,0005x^2,&amp;0\\leq x\\leq 20,\\\\ -0.25 + 0.025x -0.000125x^2,&amp;20&lt;x\\leq 100,\\\\ 1,&amp;x&gt;100. \\end{cases} \\] \\(0.1\\) quantile: The distribution function takes the value \\(0.1\\) on the intercept \\(0\\leq x \\leq 20\\), so we set \\[F(q_{0,1})=0.1\\rightarrow 0.0005q_{0.1}^ 2 = 0.1\\] and solve for \\(q_{0.1}\\): \\[q_{0.1} = \\pm\\sqrt{\\frac{0.1}{0.0005}} = \\pm 14, 1421,\\] so \\(q_{0,1} =14.1421\\), since \\(0\\leq q_{0,1} \\leq 20\\) must hold. With a probability of 10%, the time spent in the online shop is shorter than \\(14.1421\\) minutes. \\(0.9\\) quantile: The distribution function takes the value \\(0.9\\) on the intercept \\(20\\leq x \\leq 100\\), so we set \\[F(q_{0.9})=0.9\\rightarrow -0.25 + 0.025 q_{ 0.9} - 0.000125q_{0.1}^2 = 0.9.\\] Then (\\(abc\\) formula for quadratic equations \\(ax^2 + bx + c=0\\): \\(x_{1,2}=\\frac{-b\\pm\\sqrt{b^2 - 4ac}}{2a}\\)) we have \\[q_{0.9} = \\frac{-0.025\\pm\\sqrt{0.025^2 - 4\\cdot(-0.000125)\\cdot(-1.15)}}{2\\cdot ( -0.000125)}.\\] In addition, \\(q_{0.9}\\) should be between \\(20\\) and \\(100\\). So, let‚Äôs take the ‚Äú\\(+\\) version‚Äù and calculate \\(q_{0.9} = 71.7157\\). With a probability of 10%, time spent in the online shop is longer than \\(71.7157\\) minutes. 0.1111 0.25 0.3333 0.5 0.6667 Submit "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
